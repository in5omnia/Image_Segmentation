{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class DoubleConvReLU(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.doubleConvReLU = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dout, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.doubleConvReLU(x)\n",
    "    \n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.maxpool_doubleConv = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            DoubleConvReLU(din, dout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(din, dout, kernel_size=2, stride=2)\n",
    "        self.doubleConv = DoubleConvReLU(din, dout)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, self.upsample(x2)], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scale = 1 \n",
    "\n",
    "        self.down1 = DoubleConvReLU(3, self.scale * 64)\n",
    "        self.down2 = Down(self.scale * 64, self.scale * 128)\n",
    "        self.down3 = Down(self.scale * 128, self.scale * 256)\n",
    "        self.down4 = Down(self.scale * 256, self.scale * 512)\n",
    "        self.down5 = Down(self.scale * 512, self.scale * 1024)\n",
    "\n",
    "        self.up1 = Up(self.scale * 1024, self.scale * 512)\n",
    "        self.up2 = Up(self.scale * 512, self.scale * 256)\n",
    "        self.up3 = Up(self.scale * 256, self.scale * 128)\n",
    "        self.up4 = Up(self.scale * 128, self.scale * 64)\n",
    "\n",
    "        self.output = nn.Conv2d(self.scale * 64, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "        \n",
    "        x = self.up1(x4, x5)\n",
    "        x = self.up2(x3, x)\n",
    "        x = self.up3(x2, x)\n",
    "        x = self.up4(x1, x)\n",
    "\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from typing import Optional, List # Added List\n",
    "\n",
    "def calculate_class_weights_v3(\n",
    "    label_source,\n",
    "    num_classes: int,\n",
    "    ignore_index: Optional[int] = None,\n",
    "    source_type: str = 'files',\n",
    "    unimportant_class_indices: Optional[List[int]] = None, # Indices to down-weight\n",
    "    target_unimportant_weight: float = 1.0, # Target weight for unimportant classes\n",
    "    normalize_target_sum: float = -1.0 # Normalize weights sum (-1 means num_classes)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates class weights based on inverse frequency, then adjusts weights\n",
    "    for specified unimportant classes and re-normalizes.\n",
    "\n",
    "    Args:\n",
    "        # ... (Args same as v2 except for target_unimportant_weight) ...\n",
    "        target_unimportant_weight: The desired weight value for unimportant classes\n",
    "                                   BEFORE final re-normalization. 1.0 is often neutral.\n",
    "    Returns:\n",
    "        A torch.Tensor of shape [num_classes] containing weights.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating class weights for {num_classes} classes (v3)...\")\n",
    "    if unimportant_class_indices:\n",
    "        print(f\"  Adjusting weights for classes {unimportant_class_indices} post-calculation.\")\n",
    "\n",
    "    # 1. Count all classes (Same counting loop as v2)\n",
    "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
    "    total_valid_pixels = 0\n",
    "    # --- Start of loop ---\n",
    "    # (Identical loop structure as before to get raw 'counts')\n",
    "    iterator = None\n",
    "    num_labels = 0\n",
    "    if source_type == 'files': assert isinstance(label_source, (list, tuple)); iterator = label_source; num_labels = len(label_source)\n",
    "    elif source_type == 'dataset': assert hasattr(label_source, '__getitem__') and hasattr(label_source, '__len__'); iterator = range(len(label_source)); num_labels = len(label_source)\n",
    "    else: raise ValueError(\"source_type must be either 'files' or 'dataset'\")\n",
    "    print(f\"Processing {num_labels} labels...\")\n",
    "    pbar = tqdm(iterator, total=num_labels)\n",
    "\n",
    "    for idx_or_path in pbar:\n",
    "        try:\n",
    "            label_data = None\n",
    "            # ... (Identical label loading logic as before) ...\n",
    "            if source_type == 'files':\n",
    "                path = idx_or_path; img = Image.open(path); label_data = torch.from_numpy(np.array(img)) # Example\n",
    "            elif source_type == 'dataset':\n",
    "                 _, label_data = label_source[idx_or_path]; label_data = torch.tensor(label_data) if not isinstance(label_data, torch.Tensor) else label_data\n",
    "            if label_data is None: continue\n",
    "            label_long = label_data.long().view(-1)\n",
    "            if ignore_index is not None: valid_mask = (label_long != ignore_index); label_valid = label_long[valid_mask]\n",
    "            else: label_valid = label_long\n",
    "            label_valid = torch.clamp(label_valid, 0, num_classes - 1)\n",
    "            if label_valid.numel() > 0:\n",
    "                 counts += torch.bincount(label_valid, minlength=num_classes).double()\n",
    "                 total_valid_pixels += label_valid.numel()\n",
    "\n",
    "        except Exception as e:\n",
    "             item_id = idx_or_path if source_type=='files' else f\"index {idx_or_path}\"; print(f\"\\nError processing {item_id}: {e}. Skipping.\")\n",
    "             continue\n",
    "    # --- End of loop ---\n",
    "    print(\"\\nFinished counting.\")\n",
    "    print(f\"Raw pixel counts per class: {counts.long().tolist()}\")\n",
    "    print(f\"Total valid pixels counted: {total_valid_pixels}\")\n",
    "\n",
    "    if total_valid_pixels == 0:\n",
    "        print(\"Warning: No valid pixels found. Returning equal weights.\")\n",
    "        return torch.ones(num_classes, dtype=torch.float32)\n",
    "\n",
    "    # 2. Calculate initial inverse frequency weights for ALL classes\n",
    "    frequencies = counts / total_valid_pixels\n",
    "    epsilon = 1e-6\n",
    "    inverse_frequencies = 1.0 / (frequencies + epsilon)\n",
    "\n",
    "    # Intermediate weights (no normalization yet)\n",
    "    weights = inverse_frequencies\n",
    "\n",
    "    # 3. Adjust weights for unimportant classes\n",
    "    if unimportant_class_indices:\n",
    "        for idx in unimportant_class_indices:\n",
    "            if 0 <= idx < num_classes:\n",
    "                # Find the 'base' frequency corresponding to the target weight\n",
    "                # If target is 1.0, it corresponds to average frequency's inverse\n",
    "                # For simplicity, let's just scale relative to others,\n",
    "                # or simply set it directly. Setting directly is easier.\n",
    "                # Option A: Set directly relative to the *initial* mean inverse frequency\n",
    "                # mean_inv_freq = inverse_frequencies.mean()\n",
    "                # weights[idx] = mean_inv_freq * target_unimportant_weight\n",
    "                # Option B: Just set it towards 1 (average weight after normalization)\n",
    "                # This value might need tuning\n",
    "                 weights[idx] = min(weights) # Assign the target weight DIRECTLY.\n",
    "                 # Note: This works well if target_unimportant_weight=1.0 and normalization\n",
    "                 # makes the average weight 1.0 later.\n",
    "\n",
    "            else:\n",
    "                warnings.warn(f\"unimportant_class_index {idx} is out of bounds.\")\n",
    "\n",
    "    # 4. Normalize the *adjusted* weights\n",
    "    target_sum = normalize_target_sum if normalize_target_sum > 0 else float(num_classes)\n",
    "    final_weights = weights / weights.sum() * target_sum\n",
    "    final_weights_2 = weights / weights.sum()\n",
    "\n",
    "    print(f\"Calculated Final Class Weights: {final_weights.tolist()}\")\n",
    "    print(f\"Calculated Final Class Weights2: {final_weights_2.tolist()}\")\n",
    "    return final_weights.float(), final_weights_2.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_names = sorted([os.path.splitext(filename)[0] for filename in os.listdir(img_dir)])\n",
    "        self.len = len(self.img_names)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = decode_image(os.path.join(self.img_dir, self.img_names[idx] + \".jpg\")).float()/255\n",
    "        label = decode_image(os.path.join(self.label_dir, self.img_names[idx] + \".png\"))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "def display_img_label(data, idx):\n",
    "    img, label = data[idx]\n",
    "    figure = plt.figure(figsize=(10,20))\n",
    "    figure.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    figure.add_subplot(1, 2, 2)\n",
    "    plt.imshow(label.permute(1, 2, 0), cmap='grey')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class target_remap(object):\n",
    "    def __call__(self, img):\n",
    "        img[img == 255] = 3\n",
    "        return img\n",
    "\n",
    "def diff_size_collate(batch):\n",
    "    imgs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    return imgs, labels\n",
    "\n",
    "target_batch_size = 64\n",
    "batch_size = 4\n",
    "\n",
    "training_data = dataset(\"atrain/color\", \"atrain/label\", target_transform=target_remap())\n",
    "val_data = dataset(\"Val/color\", \"Val/label\", target_transform=target_remap())\n",
    "test_data = dataset(\"Test/color\", \"Test/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizers for Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "def resize_with_padding(image, target_size=512):\n",
    "    \"\"\"\n",
    "    Resize a single image (Tensor of shape (C, H, W)) so that the longer side\n",
    "    equals target_size, preserving aspect ratio; add black padding as needed.\n",
    "    Returns the resized and padded image, plus a metadata dictionary.\n",
    "    \"\"\"\n",
    "    _, orig_h, orig_w = image.shape\n",
    "    scale = min(target_size / orig_w, target_size / orig_h)\n",
    "    new_w = int(round(orig_w * scale))\n",
    "    new_h = int(round(orig_h * scale))\n",
    "    \n",
    "    # Resize the image\n",
    "    image_resized = TF.resize(image, size=(new_h, new_w))\n",
    "    \n",
    "    # Compute padding on each side\n",
    "    pad_w = target_size - new_w\n",
    "    pad_h = target_size - new_h\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "\n",
    "    # Pad the image (padding order: left, top, right, bottom)\n",
    "    image_padded = TF.pad(image_resized, padding=(pad_left, pad_top, pad_right, pad_bottom), fill=0)\n",
    "\n",
    "    meta = {\n",
    "        \"original_size\": (orig_h, orig_w),\n",
    "        \"new_size\": (new_h, new_w),\n",
    "        \"pad\": (pad_left, pad_top, pad_right, pad_bottom),\n",
    "        \"scale\": scale\n",
    "    }\n",
    "    return image_padded, meta\n",
    "\n",
    "def reverse_resize_and_padding(image, meta, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Remove the padding from image (Tensor of shape (C, target_size, target_size))\n",
    "    using metadata and then resize the cropped image back to the original size.\n",
    "    interpolation: \"bilinear\" for continuous outputs; use \"nearest\" for label maps.\n",
    "    \"\"\"\n",
    "    pad_left, pad_top, pad_right, pad_bottom = meta[\"pad\"]\n",
    "    new_h, new_w = meta[\"new_size\"]\n",
    "    \n",
    "    # Crop out the padding: from pad_top to pad_top+new_h and pad_left to pad_left+new_w.\n",
    "    image_cropped = image[..., pad_top: pad_top + new_h, pad_left: pad_left + new_w]\n",
    "    \n",
    "    # Resize the cropped image back to the original size.\n",
    "    orig_h, orig_w = meta[\"original_size\"]\n",
    "    # F.interpolate expects a 4D tensor.\n",
    "    image_original = F.interpolate(image_cropped.unsqueeze(0),\n",
    "                                   size=(orig_h, orig_w),\n",
    "                                   mode=interpolation,\n",
    "                                   align_corners=False if interpolation != \"nearest\" else None)\n",
    "    return image_original.squeeze(0)\n",
    "\n",
    "def process_batch_forward(batch_images, target_size=512):\n",
    "    \"\"\"\n",
    "    Process a batch (Tensor of shape (N, C, H, W)) by resizing each image to target_size\n",
    "    with aspect ratio preserved (adding black padding).\n",
    "    Returns the processed batch and a list of meta dictionaries.\n",
    "    \"\"\"\n",
    "    resized_batch = []\n",
    "    meta_list = []\n",
    "    for image in batch_images:\n",
    "        if image.ndim == 3 and image.shape[0] == 4:\n",
    "            image = image[:3, ...] # Slice to keep only the first 3 channels (R, G, B)\n",
    "        image_resized, meta = resize_with_padding(image, target_size)\n",
    "        resized_batch.append(image_resized)\n",
    "        meta_list.append(meta)\n",
    "    return torch.stack(resized_batch), meta_list\n",
    "\n",
    "def process_batch_reverse(batch_outputs, meta_list, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Given a batch of network outputs of shape (N, C, target_size, target_size) and the\n",
    "    corresponding meta info, reverse the transform for each one to obtain predictions at their\n",
    "    original sizes.\n",
    "    \"\"\"\n",
    "    original_outputs = []\n",
    "    for output, meta in zip(batch_outputs, meta_list):\n",
    "        restored = reverse_resize_and_padding(output, meta, interpolation=interpolation)\n",
    "        original_outputs.append(restored)\n",
    "    return original_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import PolynomialLR # Import PolynomialLR\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F # Needed for softmax in Dice Loss\n",
    "import warnings # Keep warnings\n",
    "from typing import Optional, List, Dict, Tuple, Callable # Keep typing\n",
    "\n",
    "# --- Configuration ---\n",
    "EPOCHS = 100\n",
    "MODEL_SAVE_DIR = \"unet\" # Changed path\n",
    "INITIAL_LR = 0.01 # Standard nnU-Net initial LR for SGD\n",
    "WEIGHT_DECAY = 3e-5 # A common weight decay value, adjust if needed\n",
    "SGD_MOMENTUM = 0.99 # nnU-Net standard momentum\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Modified Dice Loss with Class Weights ---\n",
    "class WeightedMemoryEfficientDiceLoss(nn.Module):\n",
    "    \"\"\" Version using ignore_index and supporting class weights \"\"\"\n",
    "    def __init__(self,\n",
    "                 apply_softmax: bool = True,\n",
    "                 ignore_index: Optional[int] = None,\n",
    "                 class_weights: Optional[torch.Tensor] = None, # New parameter\n",
    "                 smooth: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.apply_softmax = apply_softmax\n",
    "        self.ignore_index = ignore_index\n",
    "        self.smooth = smooth\n",
    "\n",
    "        # Store class weights, ensuring they are a Tensor if provided\n",
    "        if class_weights is not None:\n",
    "            assert isinstance(class_weights, torch.Tensor), \"class_weights must be a torch.Tensor\"\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        num_classes = x.shape[1]\n",
    "        shp_y = y.shape\n",
    "        if self.apply_softmax:\n",
    "            probs = F.softmax(x, dim=1)\n",
    "        else:\n",
    "            probs = x\n",
    "\n",
    "        # --- One-Hot Encoding and Masking ---\n",
    "        with torch.no_grad():\n",
    "            # Shape adjustments (same as before)\n",
    "            if len(shp_y) != len(probs.shape):\n",
    "                if len(shp_y) == len(probs.shape) - 1 and len(shp_y) >= 2 and shp_y == probs.shape[2:]:\n",
    "                     y = y.unsqueeze(1)\n",
    "                elif len(shp_y) == len(probs.shape) and shp_y[1] == 1: pass # ok\n",
    "                else: raise ValueError(f\"Shape mismatch: probs {probs.shape}, y {shp_y}\")\n",
    "            y_long = y.long()\n",
    "\n",
    "            # Spatial mask based on ignore_index\n",
    "            mask = None\n",
    "            if self.ignore_index is not None:\n",
    "                mask = (y_long != self.ignore_index)\n",
    "\n",
    "            # Create one-hot ground truth (potentially masked)\n",
    "            if probs.shape == y.shape: # Already one-hot\n",
    "                 y_onehot = y.float()\n",
    "                 if mask is not None:\n",
    "                      y_indices_for_mask = torch.argmax(y_onehot, dim=1, keepdim=True)\n",
    "                      mask = (y_indices_for_mask != self.ignore_index)\n",
    "                      y_onehot = y_onehot * mask\n",
    "            else: # Create from index map\n",
    "                y_onehot = torch.zeros_like(probs, device=probs.device)\n",
    "                y_onehot.scatter_(1, y_long, 1)\n",
    "                if mask is not None: y_onehot = y_onehot * mask\n",
    "\n",
    "            sum_gt = y_onehot.sum(dim=(2, 3)) # Pre-calculate GT sum needed later [N, C]\n",
    "        # --- End One-Hot Encoding ---\n",
    "\n",
    "        # Apply spatial mask to probabilities before summation\n",
    "        if mask is not None:\n",
    "             probs = probs * mask\n",
    "\n",
    "        # Calculate intersection and prediction sum (still per-sample)\n",
    "        intersect_persample = (probs * y_onehot).sum(dim=(2, 3)) # Shape [N, C]\n",
    "        sum_pred_persample = probs.sum(dim=(2, 3))              # Shape [N, C]\n",
    "        sum_gt_persample = sum_gt                               # Shape [N, C]\n",
    "\n",
    "        # --- Aggregate across batch ---\n",
    "        intersect = intersect_persample.sum(0) # Shape [C]\n",
    "        sum_pred = sum_pred_persample.sum(0)   # Shape [C]\n",
    "        sum_gt = sum_gt_persample.sum(0)     # Shape [C]\n",
    "\n",
    "        # --- Calculate per-class Dice ---\n",
    "        denominator = sum_pred + sum_gt\n",
    "        dc = (2. * intersect + self.smooth) / (torch.clip(denominator + self.smooth, 1e-8)) # Shape [C]\n",
    "\n",
    "        # --- Average Dice Logic (Weighted or Unweighted) ---\n",
    "        # Mask for valid (non-ignored) classes\n",
    "        valid_classes_mask = torch.ones_like(dc, dtype=torch.bool)\n",
    "        if self.ignore_index is not None and 0 <= self.ignore_index < num_classes:\n",
    "            valid_classes_mask[self.ignore_index] = False\n",
    "\n",
    "        dc_final = torch.tensor(0.0, device=dc.device) # Default loss if no valid classes\n",
    "\n",
    "        if valid_classes_mask.sum() > 0: # Proceed only if there are valid classes\n",
    "            dc_valid = dc[valid_classes_mask] # Dice scores for valid classes\n",
    "\n",
    "            if self.class_weights is not None:\n",
    "                # Use weighted average for valid classes\n",
    "                weights = self.class_weights.to(dc_valid.device) # Ensure weights are on correct device\n",
    "                weights_valid = weights[valid_classes_mask] # Select weights for valid classes\n",
    "\n",
    "                # Calculate weighted mean: sum(value*weight) / sum(weight)\n",
    "                weighted_sum = (dc_valid * weights_valid).sum()\n",
    "                weight_sum = weights_valid.sum()\n",
    "                dc_final = weighted_sum / weight_sum.clamp(min=1e-8) # Avoid division by zero\n",
    "\n",
    "            else:\n",
    "                # Use simple mean if no weights are provided\n",
    "                dc_final = dc_valid.mean()\n",
    "\n",
    "        return -dc_final # Return negative Dice score as loss\n",
    "\n",
    "\n",
    "# --- Modified Combined Loss to use Weighted Dice and accept CE weights ---\n",
    "class WeightedDiceCELoss(nn.Module): # Renamed for clarity\n",
    "    \"\"\"Combines WeightedMemoryEfficientDiceLoss and Cross Entropy Loss with class_weights support.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dice_weight: float = 1.0,\n",
    "                 ce_weight: float = 1.0,\n",
    "                 ignore_index: Optional[int] = None,\n",
    "                 class_weights: Optional[torch.Tensor] = None, # Pass weights here\n",
    "                 smooth_dice: float = 1e-5, # Adjust smooth value as needed (e.g., 1.0 for training)\n",
    "                 ce_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ignore_index = ignore_index # Store ignore_index\n",
    "\n",
    "        # Ensure class_weights is a Tensor if provided\n",
    "        if class_weights is not None:\n",
    "             assert isinstance(class_weights, torch.Tensor), \"class_weights must be a torch.Tensor\"\n",
    "\n",
    "        # Instantiate the modified Dice loss, passing weights\n",
    "        self.dice = WeightedMemoryEfficientDiceLoss(\n",
    "            apply_softmax=True, # Dice loss usually works on probabilities\n",
    "            ignore_index=ignore_index,\n",
    "            class_weights=class_weights, # Pass weights to Dice component\n",
    "            smooth=smooth_dice\n",
    "        )\n",
    "\n",
    "        # Prepare kwargs for standard CrossEntropyLoss\n",
    "        ce_final_kwargs = ce_kwargs.copy()\n",
    "        if ignore_index is not None:\n",
    "            ce_final_kwargs['ignore_index'] = ignore_index\n",
    "        if class_weights is not None:\n",
    "            # Pass the weights tensor to CE's 'weight' parameter\n",
    "            ce_final_kwargs['weight'] = class_weights\n",
    "            # Note: CE will handle moving the weights tensor to the correct device internally\n",
    "\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(**ce_final_kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # outputs are expected to be logits [N, C, H, W]\n",
    "        # targets are expected to be class indices [N, H, W] or [N, 1, H, W]\n",
    "\n",
    "        # --- Dice Loss ---\n",
    "        # WeightedMemoryEfficientDiceLoss handles softmax internally\n",
    "        dice_loss = self.dice(outputs, targets)\n",
    "\n",
    "        # --- Cross Entropy Loss ---\n",
    "        # Prepare targets for CE\n",
    "        if targets.ndim == 4 and targets.shape[1] == 1:\n",
    "             targets_ce = targets.squeeze(1).long()\n",
    "        elif targets.ndim == 3:\n",
    "             targets_ce = targets.long() # Assuming [N, H, W]\n",
    "        else:\n",
    "             # Added more specific error message for common cases\n",
    "             if targets.ndim == outputs.ndim and targets.shape[1] != 1:\n",
    "                 raise ValueError(f\"Target shape {targets.shape} has multiple channels but expected class indices [N, H, W] or [N, 1, H, W] for CE.\")\n",
    "             else:\n",
    "                 raise ValueError(f\"Unsupported target shape {targets.shape} for CE. Expected [N, H, W] or [N, 1, H, W].\")\n",
    "\n",
    "        # Weights are handled internally by nn.CrossEntropyLoss via its 'weight' parameter\n",
    "        ce_loss = self.cross_entropy(outputs, targets_ce)\n",
    "\n",
    "        print(f\"Dice loss: {dice_loss}\")\n",
    "        print(f\"CE loss: {ce_loss}\")\n",
    "        \n",
    "        # --- Combine ---\n",
    "        combined_loss = (self.dice_weight * dice_loss) + (self.ce_weight * ce_loss)\n",
    "        return combined_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Eval loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Training Loop (Adapted for nnU-Net style) ---\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, accumulation_steps, device):\n",
    "    \"\"\"Performs one epoch of training resembling nnU-Net practices.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0 # Tracks effective batches (after accumulation)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Determine total iterations for this epoch for tqdm progress bar\n",
    "    total_iters_in_epoch = len(dataloader)\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=total_iters_in_epoch, desc=\"Training\")\n",
    "    for batch_idx, (X, y) in pbar:\n",
    "        #### !!! CRITICAL FOR NNUNET STYLE !!! ####\n",
    "        # Apply extensive Data Augmentation HERE\n",
    "        # This is ideally done inside your Dataset __getitem__ or using\n",
    "        # a Pytorch augmentation library (Albumentations, batchgenerators)\n",
    "        # Examples: Random rotations, scaling, elastic deform, gamma, contrast...\n",
    "        # X, y = your_augmentation_function(X, y)\n",
    "        #### ------------------------------------ ####\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass (No AMP used by default in nnU-Net)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) # Combined Dice+CE loss\n",
    "\n",
    "        # Scale loss for gradient accumulation\n",
    "        scaled_loss = loss / accumulation_steps\n",
    "\n",
    "        # Backward pass\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        # Optimizer step after accumulation_steps batches\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == total_iters_in_epoch:\n",
    "            optimizer.step()\n",
    "            scheduler.step() # Step the scheduler after optimizer step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log loss and update progress bar\n",
    "            # Note: Logging unscaled loss from the *last* micro-batch in accumulation cycle\n",
    "            total_loss += loss.item()\n",
    "            processed_batches += 1\n",
    "            pbar.set_postfix({'loss': loss.item(), 'lr': optimizer.param_groups[0]['lr']})\n",
    "\n",
    "            # Optional memory check (keep if useful)\n",
    "            if processed_batches == 1:\n",
    "                try:\n",
    "                    # print(f\"Effective batch done. Memory allocated: {torch.cuda.memory_allocated(device)} bytes\", flush=True)\n",
    "                    pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    avg_loss = total_loss / processed_batches if processed_batches > 0 else 0\n",
    "    print(f\"Training Avg loss (per effective batch): {avg_loss:>8f}\")\n",
    "    print(f\"End of Epoch LR: {optimizer.param_groups[0]['lr']:>8f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# --- Evaluation Loop (Modified for aggregated IoU) ---\n",
    "def eval_loop(dataloader, model, loss_fn, device, target_size=512):\n",
    "    \"\"\"\n",
    "    Evaluation loop calculating loss, aggregated Dice, and aggregated IoU.\n",
    "\n",
    "    Args:\n",
    "        dataloader: yields batches of (list[Tensor(C,H,W)], list[Tensor(H,W)])\n",
    "        model: the neural network model (on device)\n",
    "        loss_fn: the combined loss function (e.g., DiceCELoss) used for training\n",
    "        device: the torch device (cuda or cpu)\n",
    "        target_size: the size the model expects for input\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_images_processed = 0\n",
    "    total_loss = 0.0\n",
    "    num_classes = -1 # Will be determined from first prediction\n",
    "\n",
    "    # --- Aggregation Containers (CPU tensors recommended) ---\n",
    "    # For Dice (using the memory-efficient method's components)\n",
    "    total_dice_intersect = None # Shape [C]\n",
    "    total_dice_sum_pred = None  # Shape [C]\n",
    "    total_dice_sum_gt = None    # Shape [C]\n",
    "\n",
    "    # For IoU\n",
    "    total_iou_intersection = None # Shape [C]\n",
    "    total_iou_union = None      # Shape [C]\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # Use ignore_index from the main loss function for consistency\n",
    "    ignore_index = getattr(loss_fn, 'ignore_index', 3)\n",
    "    smooth_eval = getattr(loss_fn.dice, 'smooth', 1e-5) if hasattr(loss_fn, 'dice') else 1e-5 # Match smooth\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch_list, y_batch_list in tqdm(dataloader, desc=\"Eval\"):\n",
    "            # 1. Forward Transform Inputs\n",
    "            try:\n",
    "                 X_processed, meta_list = process_batch_forward(X_batch_list, target_size=target_size)\n",
    "            except NameError: \n",
    "                raise ValueError(\"`process_batch_forward` not found.\")\n",
    "\n",
    "            X_processed = X_processed.to(device)\n",
    "\n",
    "            # 2. Model Inference\n",
    "            pred_processed = model(X_processed) # Logits [N, C, target, target]\n",
    "\n",
    "            # --- Determine num_classes once ---\n",
    "            if num_classes == -1:\n",
    "                num_classes = pred_processed.shape[1]\n",
    "                # Initialize aggregation tensors now that we know num_classes\n",
    "                total_dice_intersect = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_dice_sum_pred = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_dice_sum_gt = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_iou_intersection = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_iou_union = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "            # -----------------------------------\n",
    "\n",
    "            # 3. Reverse Transform Outputs\n",
    "            try:\n",
    "                 pred_original_list = process_batch_reverse(pred_processed, meta_list, interpolation='bilinear')\n",
    "            except NameError: raise ValueError(\"`process_batch_reverse` not found.\")\n",
    "\n",
    "            # 4. Compute Loss & Accumulate Metrics per Image\n",
    "            current_batch_size = len(y_batch_list)\n",
    "            for i in range(current_batch_size):\n",
    "                # --- Prepare single image prediction and label ---\n",
    "                pred_single_logits = pred_original_list[i].to(device) # [C, H_orig, W_orig]\n",
    "                label_single_orig = y_batch_list[i].to(device) # [H_orig, W_orig] or [1, H_orig, W_orig]\n",
    "\n",
    "                pred_single_batched = pred_single_logits.unsqueeze(0) # [1, C, H, W]\n",
    "                label_single_batched = label_single_orig.unsqueeze(0) # [1, H, W] or [1, 1, H, W]\n",
    "\n",
    "                # Convert label to index map if needed [1, H, W]\n",
    "                if label_single_batched.ndim == 4 and label_single_batched.shape[1] == 1:\n",
    "                    label_single_idxmap = label_single_batched.squeeze(1) # [1, H, W]\n",
    "                elif label_single_batched.ndim == 3:\n",
    "                    label_single_idxmap = label_single_batched # Already [1, H, W]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported label shape: {label_single_batched.shape}\")\n",
    "\n",
    "                # --- Calculate Loss ---\n",
    "                loss = loss_fn(pred_single_batched, label_single_batched) # Use original batch dim label for loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # --- Get Hard Predictions ---\n",
    "                pred_single_hard = torch.argmax(pred_single_logits, dim=0) # [H_orig, W_orig]\n",
    "\n",
    "                # --- Calculate & Accumulate Dice Components ---\n",
    "                # Re-calculate necessary components for Dice aggregation\n",
    "                probs_single = F.softmax(pred_single_logits, dim=0) # [C, H, W]\n",
    "                gt_single_long = label_single_idxmap.squeeze(0).long() # [H, W] Long type needed\n",
    "\n",
    "                mask = None\n",
    "                if ignore_index is not None:\n",
    "                    mask = (gt_single_long != ignore_index) # [H,W]\n",
    "\n",
    "                gt_onehot = F.one_hot(gt_single_long, num_classes=num_classes).permute(2, 0, 1).float() # [C, H, W]\n",
    "\n",
    "                if mask is not None:\n",
    "                    gt_onehot = gt_onehot * mask.unsqueeze(0) # Apply mask [C, H, W]\n",
    "                    probs_single_masked = probs_single * mask.unsqueeze(0)\n",
    "                else:\n",
    "                    probs_single_masked = probs_single\n",
    "\n",
    "                # Sum over spatial H, W -> Shape [C]\n",
    "                intersect_dice = (probs_single_masked * gt_onehot).sum(dim=(1, 2))\n",
    "                sum_pred_dice = probs_single_masked.sum(dim=(1, 2))\n",
    "                sum_gt_dice = gt_onehot.sum(dim=(1, 2)) # Use masked gt\n",
    "\n",
    "                # Accumulate on CPU\n",
    "                total_dice_intersect += intersect_dice.cpu().to(torch.float64)\n",
    "                total_dice_sum_pred += sum_pred_dice.cpu().to(torch.float64)\n",
    "                total_dice_sum_gt += sum_gt_dice.cpu().to(torch.float64)\n",
    "\n",
    "                # --- Calculate & Accumulate IoU Components ---\n",
    "                pred_hard_onehot = F.one_hot(pred_single_hard, num_classes=num_classes).permute(2, 0, 1).bool() # [C, H, W]\n",
    "                gt_onehot_bool = gt_onehot.bool() # Use the already created (and potentially masked) one-hot GT\n",
    "\n",
    "                if mask is not None:\n",
    "                    pred_hard_onehot_masked = pred_hard_onehot & mask.unsqueeze(0) # Apply ignore mask\n",
    "                else:\n",
    "                    pred_hard_onehot_masked = pred_hard_onehot\n",
    "\n",
    "                # Calculate intersection and union per class using boolean logic\n",
    "                intersection_iou = (pred_hard_onehot_masked & gt_onehot_bool).sum(dim=(1, 2)) # [C]\n",
    "                union_iou = (pred_hard_onehot_masked | gt_onehot_bool).sum(dim=(1, 2)) # [C]\n",
    "\n",
    "                # Accumulate on CPU\n",
    "                total_iou_intersection += intersection_iou.cpu().to(torch.float64)\n",
    "                total_iou_union += union_iou.cpu().to(torch.float64)\n",
    "                # -----------------------------------------------\n",
    "\n",
    "                num_images_processed += 1\n",
    "\n",
    "    # --- Calculate Final Average Metrics ---\n",
    "    if num_images_processed == 0: # Handle empty dataloader case\n",
    "        print(\"Evaluation dataloader was empty.\")\n",
    "        return 0.0, 0.0, 0.0 # Loss, Dice, IoU\n",
    "\n",
    "    avg_loss = total_loss / num_images_processed\n",
    "\n",
    "    # --- Final Aggregated Dice Calculation ---\n",
    "    # Using Micro-average: (Sum of numerators) / (Sum of denominators)\n",
    "    dice_numerator = 2. * total_dice_intersect + smooth_eval\n",
    "    dice_denominator = total_dice_sum_pred + total_dice_sum_gt + smooth_eval\n",
    "    # Create mask for valid classes (excluding ignore_index)\n",
    "    valid_class_mask_dice = torch.ones(num_classes, dtype=torch.bool)\n",
    "    if ignore_index is not None and 0 <= ignore_index < num_classes:\n",
    "        valid_class_mask_dice[ignore_index] = False\n",
    "\n",
    "    # Calculate micro average score over valid classes\n",
    "    avg_dice_micro = 0.0\n",
    "    if valid_class_mask_dice.sum() > 0:\n",
    "        avg_dice_micro = (dice_numerator[valid_class_mask_dice].sum() /\n",
    "                         torch.clip(dice_denominator[valid_class_mask_dice].sum(), 1e-8)).item()\n",
    "\n",
    "    # --- Optional: Macro Average Dice ---\n",
    "    per_class_dice = dice_numerator / torch.clip(dice_denominator, 1e-8)\n",
    "    avg_dice_macro = 0.0\n",
    "    if valid_class_mask_dice.sum() > 0:\n",
    "        avg_dice_macro = per_class_dice[valid_class_mask_dice].mean().item()\n",
    "\n",
    "\n",
    "    # --- Final Aggregated IoU Calculation ---\n",
    "    # Create mask for valid classes (excluding ignore_index) for IoU\n",
    "    valid_class_mask_iou = torch.ones(num_classes, dtype=torch.bool)\n",
    "    if ignore_index is not None and 0 <= ignore_index < num_classes:\n",
    "        valid_class_mask_iou[ignore_index] = False\n",
    "\n",
    "    # Calculate per-class IoU using aggregated counts\n",
    "    # Add epsilon to denominator for stability\n",
    "    epsilon = 1e-8\n",
    "    per_class_iou = total_iou_intersection / (total_iou_union + epsilon)\n",
    "\n",
    "    # Calculate Mean IoU (mIoU) over valid classes\n",
    "    mean_iou = 0.0\n",
    "    if valid_class_mask_iou.sum() > 0:\n",
    "        mean_iou = per_class_iou[valid_class_mask_iou].mean().item()\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"  Images Processed: {num_images_processed}\")\n",
    "    print(f\"  Average Loss (Original Size): {avg_loss:>8f}\")\n",
    "    print(f\"  Micro Avg Dice Score ({valid_class_mask_dice.sum().item()} classes): {avg_dice_micro:>8f}\")\n",
    "    print(f\"  Macro Avg Dice Score ({valid_class_mask_dice.sum().item()} classes): {avg_dice_macro:>8f}\")\n",
    "    print(f\"  Mean IoU (mIoU) ({valid_class_mask_iou.sum().item()} classes): {mean_iou:>8f}\")\n",
    "    print(f\"  --- Per-Class IoU ---\")\n",
    "    for c in range(num_classes):\n",
    "        if valid_class_mask_iou[c]: # Only print for valid classes\n",
    "            print(f\"    Class {c}: {per_class_iou[c].item():>8f}\")\n",
    "        else:\n",
    "            print(f\"    Class {c}: Ignored\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # Return relevant metrics for model saving (e.g., micro dice and mIoU)\n",
    "    return avg_loss, avg_dice_micro, mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "if 'accumulation_steps' not in locals() :\n",
    "    # ... (keep your accumulation steps calculation) ...\n",
    "    assert 'target_batch_size' in globals() and 'batch_size' in globals(), \\\n",
    "           \"Please define target_batch_size and batch_size\"\n",
    "    assert target_batch_size >= batch_size, \"target_batch_size must be >= batch_size\"\n",
    "    assert target_batch_size % batch_size == 0, \"target_batch_size must be divisible by batch_size for simple accumulation\"\n",
    "    accumulation_steps = target_batch_size // batch_size\n",
    "    print(f\"Using Gradient Accumulation: effective batch size {target_batch_size} ({accumulation_steps} steps)\")\n",
    "\n",
    "\n",
    "# --- Define Model, Loss, Optimizer, Scheduler ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = unet().to(device) # Assuming unet() exists and returns your U-Net model\n",
    "\n",
    "# --- Define Loss, Optimizer, Scheduler ---\n",
    "# Configure ignore_index (e.g., 3 to ignore class 3, or 255 if used in labels, None otherwise)\n",
    "EVAL_IGNORE_INDEX = 3 # Example: ignore class 3 during evaluation metric calculation\n",
    "TRAIN_IGNORE_INDEX = None  # Example: train on all classes (0,1,2,3)\n",
    "\n",
    "w1, w2 = calculate_class_weights_v3(training_data, 4, None, \"dataset\", 3)\n",
    "\n",
    "loss_fn = WeightedDiceCELossgi(ignore_index=TRAIN_IGNORE_INDEX, smooth_dice=1, class_weights=w1) # Training loss\n",
    "# Evaluation loss object used inside eval loop only to get settings like ignore_index\n",
    "# It is NOT used to calculate the loss score reported for eval (that uses training loss object)\n",
    "# But we pass it to eval_loop so it knows which index to ignore for metric calc if needed\n",
    "eval_settings_provider = WeightedDiceCELoss(ignore_index=EVAL_IGNORE_INDEX, class_weights=w1)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=SGD_MOMENTUM,\n",
    "                      weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "\n",
    "steps_per_epoch = (len(train_dataloader) // accumulation_steps) + (1 if len(train_dataloader) % accumulation_steps != 0 else 0)\n",
    "total_iters = steps_per_epoch * EPOCHS\n",
    "scheduler = PolynomialLR(optimizer, total_iters=total_iters, power=0.9)\n",
    "\n",
    "start_epoch = 0\n",
    "best_dev_dice = -np.inf # Track best Dice score\n",
    "best_dev_miou = -np.inf # Track best mIoU\n",
    "best_dev_loss = np.inf # Track loss corresponding to best metric\n",
    "\n",
    "if os.path.isfile(\"/unet_best_dice_31.pytorch\"):\n",
    "    print(f\"Loading checkpoint from: unet/unet_best_dice_31.pytorch\")\n",
    "    # Load the checkpoint dictionary; move tensors to the correct device\n",
    "    checkpoint = torch.load(\"unet/unet_best_dice_31.pytorch\", map_location=device)\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(\" -> Model state loaded.\")\n",
    "\n",
    "    # Load optimizer state\n",
    "    try:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\" -> Optimizer state loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Warning: Could not load optimizer state: {e}. Optimizer will start from scratch.\")\n",
    "\n",
    "    # Load scheduler state\n",
    "    try:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        print(\" -> Scheduler state loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Warning: Could not load scheduler state: {e}. Scheduler will start from scratch.\")\n",
    "\n",
    "\n",
    "    # Load training metadata\n",
    "    start_epoch = checkpoint.get(\"epoch\", 0) # Load last completed epoch, training continues from next one\n",
    "    best_dev_dice = checkpoint.get(\"best_dev_dice\", -np.inf)\n",
    "    best_dev_miou = checkpoint.get(\"best_dev_miou\", -np.inf)\n",
    "    best_dev_loss = checkpoint.get(\"best_dev_loss\", np.inf)\n",
    "\n",
    "    print(f\" -> Resuming training from epoch {start_epoch + 1}\")\n",
    "    print(f\" -> Loaded best metrics: Dice={best_dev_dice:.6f}, mIoU={best_dev_miou:.6f}, Loss={best_dev_loss:.6f}\")\n",
    "    loaded_notes = checkpoint.get(\"notes\", \"N/A\")\n",
    "    print(f\" -> Notes from checkpoint: {loaded_notes}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Checkpoint file not found at unet/unet_best_dice_31.pytorch. Starting training from scratch.\")\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "print(\"\\nStarting Training (nnU-Net style)...\")\n",
    "for t in range(start_epoch, EPOCHS):\n",
    "    current_epoch = t + 1\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, accumulation_steps, device)\n",
    "\n",
    "    # Pass the eval_settings_provider to eval_loop\n",
    "    val_loss, val_dice_micro, val_miou = eval_loop(val_dataloader, model, eval_settings_provider, device)\n",
    "\n",
    "    # Save model based on validation MICRO DICE score improvement\n",
    "    # Could also choose mIoU validation by changing 'val_dice_micro > best_dev_dice'\n",
    "    if val_dice_micro > best_dev_dice:\n",
    "        best_dev_dice = val_dice_micro\n",
    "        best_dev_miou = val_miou # Save corresponding mIoU\n",
    "        best_dev_loss = val_loss # Save corresponding loss\n",
    "        print(f\"Validation Micro Dice score improved ({best_dev_dice:.6f}). Saving model...\")\n",
    "        checkpoint_path = os.path.join(MODEL_SAVE_DIR, \"unet_best_dice.pytorch\") # Changed name\n",
    "        checkpoint = {\n",
    "            \"epoch\": t + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"best_dev_dice\": best_dev_dice,\n",
    "            \"best_dev_miou\": best_dev_miou,\n",
    "            \"best_dev_loss\": best_dev_loss,\n",
    "            \"notes\": f\"Model saved based on best Micro Dice. Ignored index for metric: {EVAL_IGNORE_INDEX}\"\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    else:\n",
    "        print(f\"Validation Micro Dice score did not improve from {best_dev_dice:.6f}\")\n",
    "\n",
    "print(\"\\n--- Training Finished! ---\")\n",
    "print(f\"Best validation Micro Dice score achieved: {best_dev_dice:.6f}\")\n",
    "print(f\"Corresponding validation mIoU: {best_dev_miou:.6f}\")\n",
    "print(f\"Corresponding validation loss: {best_dev_loss:.6f}\")\n",
    "print(f\"Best model saved to: {os.path.join(MODEL_SAVE_DIR, 'unet_best_dice.pytorch')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVAL Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet().to(device)\n",
    "loss_fn = DiceCELoss(ignore_index=3)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=SGD_MOMENTUM,\n",
    "                      weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "\n",
    "total_iters = (len(train_dataloader) // accumulation_steps) * EPOCHS\n",
    "scheduler = PolynomialLR(optimizer, total_iters=total_iters, power=0.9)\n",
    "\n",
    "checkpoint = torch.load(\"unet/checkpoint\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "# scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "new_dev_loss = eval_loop(val_dataloader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "dice = MemoryEfficientDiceLoss()\n",
    "\n",
    "model = unet().to(device)\n",
    "checkpoint = torch.load(\"unet/checkpoint\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "pred = model(X[0].unsqueeze(0))\n",
    "\n",
    "print(dice(pred, y[0].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
