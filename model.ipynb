{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dout, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "    \n",
    "class up(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dout, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scale = 1 # maybe using the original channels might be better\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.down1 = down(3, self.scale * 64)\n",
    "        self.down2 = down(self.scale * 64, self.scale * 128)\n",
    "        self.down3 = down(self.scale * 128, self.scale * 256)\n",
    "        self.down4 = down(self.scale * 256, self.scale * 512)\n",
    "        self.down5 = down(self.scale * 512, self.scale * 1024)\n",
    "        self.up1 = up(self.scale * 1024, self.scale * 512)\n",
    "        self.up2 = up(self.scale * 512, self.scale * 256)\n",
    "        self.up3 = up(self.scale * 256, self.scale * 128)\n",
    "        self.up4 = up(self.scale * 128, self.scale * 64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.upconv1 = nn.Conv2d(self.scale * 1024, self.scale * 512, kernel_size=3, padding=1)\n",
    "        self.upconv2 = nn.Conv2d(self.scale * 512, self.scale * 256, kernel_size=3, padding=1)\n",
    "        self.upconv3 = nn.Conv2d(self.scale * 256, self.scale * 128, kernel_size=3, padding=1)\n",
    "        self.upconv4 = nn.Conv2d(self.scale * 128, self.scale * 64, kernel_size=3, padding=1)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(self.scale * 64, 4, kernel_size=1),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(self.maxpool(x1))\n",
    "        x3 = self.down3(self.maxpool(x2))\n",
    "        x4 = self.down4(self.maxpool(x3))\n",
    "        x5 = self.down5(self.maxpool(x4))\n",
    "        x = self.up1(torch.cat([x4, self.upconv1(self.upsample(x5))], dim=1))\n",
    "        x = self.up2(torch.cat([x3, self.upconv2(self.upsample(x))], dim=1))\n",
    "        x = self.up3(torch.cat([x2, self.upconv3(self.upsample(x))], dim=1))\n",
    "        x = self.up4(torch.cat([x1, self.upconv4(self.upsample(x))], dim=1))\n",
    "        pre_output = self.output(x)\n",
    "        return pre_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, PILToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "target_batch_size = 64\n",
    "batch_size = 4\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_names = sorted([os.path.splitext(filename)[0] for filename in os.listdir(img_dir)])\n",
    "        self.len = len(self.img_names)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = decode_image(os.path.join(self.img_dir, self.img_names[idx] + \".jpg\")).float()/255\n",
    "        label = decode_image(os.path.join(self.label_dir, self.img_names[idx] + \".png\"))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "def display_img_label(data, idx):\n",
    "    img, label = data[idx]\n",
    "    figure = plt.figure(figsize=(10,20))\n",
    "    figure.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    figure.add_subplot(1, 2, 2)\n",
    "    plt.imshow(label.permute(1, 2, 0), cmap='grey')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class target_remap(object):\n",
    "    def __call__(self, img):\n",
    "        img[img == 255] = 3\n",
    "        return img\n",
    "\n",
    "def diff_size_collate(batch):\n",
    "    imgs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "\n",
    "training_data = dataset(\"atrain/color\", \"atrain/label\", target_transform=target_remap())\n",
    "val_data = dataset(\"Val/color\", \"Val/label\", target_transform=target_remap())\n",
    "test_data = dataset(\"Test/color\", \"Test/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizers for Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume these are defined somewhere in your code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_amp = True  # or False depending on your setup\n",
    "\n",
    "def resize_with_padding(image, target_size=512):\n",
    "    \"\"\"\n",
    "    Resize a single image (Tensor of shape (C, H, W)) so that the longer side\n",
    "    equals target_size, preserving aspect ratio; add black padding as needed.\n",
    "    Returns the resized and padded image, plus a metadata dictionary.\n",
    "    \"\"\"\n",
    "    _, orig_h, orig_w = image.shape\n",
    "    scale = min(target_size / orig_w, target_size / orig_h)\n",
    "    new_w = int(round(orig_w * scale))\n",
    "    new_h = int(round(orig_h * scale))\n",
    "    \n",
    "    # Resize the image\n",
    "    image_resized = TF.resize(image, size=(new_h, new_w))\n",
    "    \n",
    "    # Compute padding on each side\n",
    "    pad_w = target_size - new_w\n",
    "    pad_h = target_size - new_h\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "\n",
    "    # Pad the image (padding order: left, top, right, bottom)\n",
    "    image_padded = TF.pad(image_resized, padding=(pad_left, pad_top, pad_right, pad_bottom), fill=0)\n",
    "\n",
    "    meta = {\n",
    "        \"original_size\": (orig_h, orig_w),\n",
    "        \"new_size\": (new_h, new_w),\n",
    "        \"pad\": (pad_left, pad_top, pad_right, pad_bottom),\n",
    "        \"scale\": scale\n",
    "    }\n",
    "    return image_padded, meta\n",
    "\n",
    "def reverse_resize_and_padding(image, meta, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Remove the padding from image (Tensor of shape (C, target_size, target_size))\n",
    "    using metadata and then resize the cropped image back to the original size.\n",
    "    interpolation: \"bilinear\" for continuous outputs; use \"nearest\" for label maps.\n",
    "    \"\"\"\n",
    "    pad_left, pad_top, pad_right, pad_bottom = meta[\"pad\"]\n",
    "    new_h, new_w = meta[\"new_size\"]\n",
    "    \n",
    "    # Crop out the padding: from pad_top to pad_top+new_h and pad_left to pad_left+new_w.\n",
    "    image_cropped = image[..., pad_top: pad_top + new_h, pad_left: pad_left + new_w]\n",
    "    \n",
    "    # Resize the cropped image back to the original size.\n",
    "    orig_h, orig_w = meta[\"original_size\"]\n",
    "    # F.interpolate expects a 4D tensor.\n",
    "    image_original = F.interpolate(image_cropped.unsqueeze(0),\n",
    "                                   size=(orig_h, orig_w),\n",
    "                                   mode=interpolation,\n",
    "                                   align_corners=False if interpolation != \"nearest\" else None)\n",
    "    return image_original.squeeze(0)\n",
    "\n",
    "def process_batch_forward(batch_images, target_size=512):\n",
    "    \"\"\"\n",
    "    Process a batch (Tensor of shape (N, C, H, W)) by resizing each image to target_size\n",
    "    with aspect ratio preserved (adding black padding).\n",
    "    Returns the processed batch and a list of meta dictionaries.\n",
    "    \"\"\"\n",
    "    resized_batch = []\n",
    "    meta_list = []\n",
    "    for image in batch_images:\n",
    "        if image.ndim == 3 and image.shape[0] == 4:\n",
    "            image = image[:3, ...] # Slice to keep only the first 3 channels (R, G, B)\n",
    "        image_resized, meta = resize_with_padding(image, target_size)\n",
    "        resized_batch.append(image_resized)\n",
    "        meta_list.append(meta)\n",
    "    return torch.stack(resized_batch), meta_list\n",
    "\n",
    "def process_batch_reverse(batch_outputs, meta_list, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Given a batch of network outputs of shape (N, C, target_size, target_size) and the\n",
    "    corresponding meta info, reverse the transform for each one to obtain predictions at their\n",
    "    original sizes.\n",
    "    \"\"\"\n",
    "    original_outputs = []\n",
    "    for output, meta in zip(batch_outputs, meta_list):\n",
    "        restored = reverse_resize_and_padding(output, meta, interpolation=interpolation)\n",
    "        original_outputs.append(restored)\n",
    "    return original_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import PolynomialLR # Import PolynomialLR\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F # Needed for softmax in Dice Loss\n",
    "\n",
    "# --- Configuration ---\n",
    "EPOCHS = 100\n",
    "MODEL_SAVE_DIR = \"unet\" # Changed path\n",
    "INITIAL_LR = 0.01 # Standard nnU-Net initial LR for SGD\n",
    "WEIGHT_DECAY = 3e-5 # A common weight decay value, adjust if needed\n",
    "SGD_MOMENTUM = 0.99 # nnU-Net standard momentum\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "class MemoryEfficientDiceLoss(nn.Module):\n",
    "    \"\"\" Version using ignore_index \"\"\"\n",
    "    def __init__(self, apply_softmax: bool = True, ignore_index: int = None, smooth: float = 1e-5):\n",
    "        super(MemoryEfficientDiceLoss, self).__init__()\n",
    "        self.apply_softmax = apply_softmax\n",
    "        self.ignore_index = ignore_index\n",
    "        self.smooth = smooth\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        num_classes = x.shape[1]\n",
    "        shp_y = y.shape\n",
    "        if self.apply_softmax:\n",
    "            probs = F.softmax(x, dim=1)\n",
    "        else:\n",
    "            probs = x\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if len(shp_y) != len(probs.shape):\n",
    "                 if len(shp_y) == len(probs.shape) - 1 and len(shp_y) >= 2 and shp_y == probs.shape[2:]:\n",
    "                      y = y.unsqueeze(1)\n",
    "                 elif len(shp_y) == len(probs.shape) and shp_y[1] == 1: pass\n",
    "                 else: raise ValueError(f\"Shape mismatch...\")\n",
    "            y_long = y.long()\n",
    "            mask = None\n",
    "            if self.ignore_index is not None:\n",
    "                mask = (y_long != self.ignore_index)\n",
    "\n",
    "            if probs.shape == y.shape:\n",
    "                 y_onehot = y.float()\n",
    "                 if mask is not None:\n",
    "                      warnings.warn(\"Input y has same shape...\") # Shortened warning\n",
    "                      y_indices_for_mask = torch.argmax(y_onehot, dim=1, keepdim=True)\n",
    "                      mask = (y_indices_for_mask != self.ignore_index)\n",
    "                      y_onehot = y_onehot * mask\n",
    "            else:\n",
    "                y_onehot = torch.zeros_like(probs, device=probs.device)\n",
    "                y_onehot.scatter_(1, y_long, 1)\n",
    "                if mask is not None: y_onehot = y_onehot * mask\n",
    "\n",
    "            sum_gt = y_onehot.sum(dim=(2, 3))\n",
    "\n",
    "        if mask is not None: probs = probs * mask\n",
    "\n",
    "        intersect = (probs * y_onehot).sum(dim=(2, 3))\n",
    "        sum_pred = probs.sum(dim=(2, 3))\n",
    "\n",
    "        intersect = intersect.sum(0)\n",
    "        sum_pred = sum_pred.sum(0)\n",
    "        sum_gt = sum_gt.sum(0)\n",
    "\n",
    "        denominator = sum_pred + sum_gt\n",
    "        dc = (2. * intersect + self.smooth) / (torch.clip(denominator + self.smooth, 1e-8))\n",
    "\n",
    "        # --- Average Dice Logic ---\n",
    "        # Decide how to average if ignore_index is used.\n",
    "        # If a class index IS the ignore_index, should its Dice score contribute to mean?\n",
    "        # Standard practice often excludes the ignored class from the final average.\n",
    "        valid_classes_mask = torch.ones_like(dc, dtype=torch.bool)\n",
    "        if self.ignore_index is not None and 0 <= self.ignore_index < num_classes:\n",
    "            valid_classes_mask[self.ignore_index] = False\n",
    "\n",
    "        # Only average over valid classes\n",
    "        if valid_classes_mask.sum() > 0:\n",
    "            dc_mean = dc[valid_classes_mask].mean()\n",
    "        else: # Avoid NaN if all classes are ignored (edge case)\n",
    "            dc_mean = torch.tensor(0.0, device=dc.device) # Or handle as error\n",
    "\n",
    "        return -dc_mean\n",
    "\n",
    "class DiceCELoss(nn.Module):\n",
    "    \"\"\"Combines MemoryEfficientDiceLoss and Cross Entropy Loss\"\"\"\n",
    "    def __init__(self, dice_weight=1.0, ce_weight=1.0, ignore_index: int = None, smooth_dice=1e-5, ce_kwargs={}):\n",
    "        super(DiceCELoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        # Use the memory efficient Dice loss\n",
    "        self.dice = MemoryEfficientDiceLoss(apply_softmax=True, ignore_index=ignore_index, smooth=smooth_dice)\n",
    "        ce_ignore_kwargs = ce_kwargs.copy()\n",
    "\n",
    "        if ignore_index is not None:\n",
    "            ce_ignore_kwargs['ignore_index'] = ignore_index\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(**ce_ignore_kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # --- Dice Loss ---\n",
    "        # Dice loss expects logits (apply_softmax=True handles conversion inside)\n",
    "        dice_loss = self.dice(outputs, targets)\n",
    "\n",
    "        # --- Cross Entropy Loss ---\n",
    "        # Ensure target is long and shape [N, H, W] for CE\n",
    "        if targets.ndim == 4 and targets.shape[1] == 1:\n",
    "             targets_ce = targets.squeeze(1).long()\n",
    "        else:\n",
    "             # Assume targets are already [N, H, W] or convert if needed\n",
    "             targets_ce = targets.long()\n",
    "\n",
    "        ce_loss = self.cross_entropy(outputs, targets_ce)\n",
    "\n",
    "        # --- Combine ---\n",
    "        combined_loss = (self.dice_weight * dice_loss) + (self.ce_weight * ce_loss)\n",
    "        return combined_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Eval loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Training Loop (Adapted for nnU-Net style) ---\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, accumulation_steps, device):\n",
    "    \"\"\"Performs one epoch of training resembling nnU-Net practices.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0 # Tracks effective batches (after accumulation)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Determine total iterations for this epoch for tqdm progress bar\n",
    "    total_iters_in_epoch = len(dataloader)\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=total_iters_in_epoch, desc=\"Training\")\n",
    "    for batch_idx, (X, y) in pbar:\n",
    "        #### !!! CRITICAL FOR NNUNET STYLE !!! ####\n",
    "        # Apply extensive Data Augmentation HERE\n",
    "        # This is ideally done inside your Dataset __getitem__ or using\n",
    "        # a Pytorch augmentation library (Albumentations, batchgenerators)\n",
    "        # Examples: Random rotations, scaling, elastic deform, gamma, contrast...\n",
    "        # X, y = your_augmentation_function(X, y)\n",
    "        #### ------------------------------------ ####\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass (No AMP used by default in nnU-Net)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) # Combined Dice+CE loss\n",
    "\n",
    "        # Scale loss for gradient accumulation\n",
    "        scaled_loss = loss / accumulation_steps\n",
    "\n",
    "        # Backward pass\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        # Optimizer step after accumulation_steps batches\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == total_iters_in_epoch:\n",
    "            optimizer.step()\n",
    "            scheduler.step() # Step the scheduler after optimizer step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log loss and update progress bar\n",
    "            # Note: Logging unscaled loss from the *last* micro-batch in accumulation cycle\n",
    "            total_loss += loss.item()\n",
    "            processed_batches += 1\n",
    "            pbar.set_postfix({'loss': loss.item(), 'lr': optimizer.param_groups[0]['lr']})\n",
    "\n",
    "            # Optional memory check (keep if useful)\n",
    "            if processed_batches == 1:\n",
    "                try:\n",
    "                    # print(f\"Effective batch done. Memory allocated: {torch.cuda.memory_allocated(device)} bytes\", flush=True)\n",
    "                    pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    avg_loss = total_loss / processed_batches if processed_batches > 0 else 0\n",
    "    print(f\"Training Avg loss (per effective batch): {avg_loss:>8f}\")\n",
    "    print(f\"End of Epoch LR: {optimizer.param_groups[0]['lr']:>8f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# --- Evaluation Loop (Modified for aggregated IoU) ---\n",
    "def eval_loop(dataloader, model, loss_fn, device, target_size=512):\n",
    "    \"\"\"\n",
    "    Evaluation loop calculating loss, aggregated Dice, and aggregated IoU.\n",
    "\n",
    "    Args:\n",
    "        dataloader: yields batches of (list[Tensor(C,H,W)], list[Tensor(H,W)])\n",
    "        model: the neural network model (on device)\n",
    "        loss_fn: the combined loss function (e.g., DiceCELoss) used for training\n",
    "        device: the torch device (cuda or cpu)\n",
    "        target_size: the size the model expects for input\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_images_processed = 0\n",
    "    total_loss = 0.0\n",
    "    num_classes = -1 # Will be determined from first prediction\n",
    "\n",
    "    # --- Aggregation Containers (CPU tensors recommended) ---\n",
    "    # For Dice (using the memory-efficient method's components)\n",
    "    total_dice_intersect = None # Shape [C]\n",
    "    total_dice_sum_pred = None  # Shape [C]\n",
    "    total_dice_sum_gt = None    # Shape [C]\n",
    "\n",
    "    # For IoU\n",
    "    total_iou_intersection = None # Shape [C]\n",
    "    total_iou_union = None      # Shape [C]\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # Use ignore_index from the main loss function for consistency\n",
    "    ignore_index = getattr(loss_fn, 'ignore_index', None)\n",
    "    smooth_eval = getattr(loss_fn.dice, 'smooth', 1e-6) if hasattr(loss_fn, 'dice') else 1e-6 # Match smooth\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch_list, y_batch_list in tqdm(dataloader, desc=\"Eval\"):\n",
    "            # 1. Forward Transform Inputs\n",
    "            try:\n",
    "                 X_processed, meta_list = process_batch_forward(X_batch_list, target_size=target_size)\n",
    "            except NameError: \n",
    "                raise ValueError(\"`process_batch_forward` not found.\")\n",
    "\n",
    "            X_processed = X_processed.to(device)\n",
    "\n",
    "            # 2. Model Inference\n",
    "            pred_processed = model(X_processed) # Logits [N, C, target, target]\n",
    "\n",
    "            # --- Determine num_classes once ---\n",
    "            if num_classes == -1:\n",
    "                num_classes = pred_processed.shape[1]\n",
    "                # Initialize aggregation tensors now that we know num_classes\n",
    "                total_dice_intersect = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_dice_sum_pred = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_dice_sum_gt = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_iou_intersection = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "                total_iou_union = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "            # -----------------------------------\n",
    "\n",
    "            # 3. Reverse Transform Outputs\n",
    "            try:\n",
    "                 pred_original_list = process_batch_reverse(pred_processed, meta_list, interpolation='bilinear')\n",
    "            except NameError: raise ValueError(\"`process_batch_reverse` not found.\")\n",
    "\n",
    "            # 4. Compute Loss & Accumulate Metrics per Image\n",
    "            current_batch_size = len(y_batch_list)\n",
    "            for i in range(current_batch_size):\n",
    "                # --- Prepare single image prediction and label ---\n",
    "                pred_single_logits = pred_original_list[i].to(device) # [C, H_orig, W_orig]\n",
    "                label_single_orig = y_batch_list[i].to(device) # [H_orig, W_orig] or [1, H_orig, W_orig]\n",
    "\n",
    "                pred_single_batched = pred_single_logits.unsqueeze(0) # [1, C, H, W]\n",
    "                label_single_batched = label_single_orig.unsqueeze(0) # [1, H, W] or [1, 1, H, W]\n",
    "\n",
    "                # Convert label to index map if needed [1, H, W]\n",
    "                if label_single_batched.ndim == 4 and label_single_batched.shape[1] == 1:\n",
    "                    label_single_idxmap = label_single_batched.squeeze(1) # [1, H, W]\n",
    "                elif label_single_batched.ndim == 3:\n",
    "                    label_single_idxmap = label_single_batched # Already [1, H, W]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported label shape: {label_single_batched.shape}\")\n",
    "\n",
    "                # --- Calculate Loss ---\n",
    "                loss = loss_fn(pred_single_batched, label_single_batched) # Use original batch dim label for loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # --- Get Hard Predictions ---\n",
    "                pred_single_hard = torch.argmax(pred_single_logits, dim=0) # [H_orig, W_orig]\n",
    "\n",
    "                # --- Calculate & Accumulate Dice Components ---\n",
    "                # Re-calculate necessary components for Dice aggregation\n",
    "                probs_single = F.softmax(pred_single_logits, dim=0) # [C, H, W]\n",
    "                gt_single_long = label_single_idxmap.squeeze(0).long() # [H, W] Long type needed\n",
    "\n",
    "                mask = None\n",
    "                if ignore_index is not None:\n",
    "                    mask = (gt_single_long != ignore_index) # [H,W]\n",
    "\n",
    "                gt_onehot = F.one_hot(gt_single_long, num_classes=num_classes).permute(2, 0, 1).float() # [C, H, W]\n",
    "\n",
    "                if mask is not None:\n",
    "                    gt_onehot = gt_onehot * mask.unsqueeze(0) # Apply mask [C, H, W]\n",
    "                    probs_single_masked = probs_single * mask.unsqueeze(0)\n",
    "                else:\n",
    "                    probs_single_masked = probs_single\n",
    "\n",
    "                # Sum over spatial H, W -> Shape [C]\n",
    "                intersect_dice = (probs_single_masked * gt_onehot).sum(dim=(1, 2))\n",
    "                sum_pred_dice = probs_single_masked.sum(dim=(1, 2))\n",
    "                sum_gt_dice = gt_onehot.sum(dim=(1, 2)) # Use masked gt\n",
    "\n",
    "                # Accumulate on CPU\n",
    "                total_dice_intersect += intersect_dice.cpu().to(torch.float64)\n",
    "                total_dice_sum_pred += sum_pred_dice.cpu().to(torch.float64)\n",
    "                total_dice_sum_gt += sum_gt_dice.cpu().to(torch.float64)\n",
    "\n",
    "                # --- Calculate & Accumulate IoU Components ---\n",
    "                pred_hard_onehot = F.one_hot(pred_single_hard, num_classes=num_classes).permute(2, 0, 1).bool() # [C, H, W]\n",
    "                gt_onehot_bool = gt_onehot.bool() # Use the already created (and potentially masked) one-hot GT\n",
    "\n",
    "                if mask is not None:\n",
    "                    pred_hard_onehot_masked = pred_hard_onehot & mask.unsqueeze(0) # Apply ignore mask\n",
    "                else:\n",
    "                    pred_hard_onehot_masked = pred_hard_onehot\n",
    "\n",
    "                # Calculate intersection and union per class using boolean logic\n",
    "                intersection_iou = (pred_hard_onehot_masked & gt_onehot_bool).sum(dim=(1, 2)) # [C]\n",
    "                union_iou = (pred_hard_onehot_masked | gt_onehot_bool).sum(dim=(1, 2)) # [C]\n",
    "\n",
    "                # Accumulate on CPU\n",
    "                total_iou_intersection += intersection_iou.cpu().to(torch.float64)\n",
    "                total_iou_union += union_iou.cpu().to(torch.float64)\n",
    "                # -----------------------------------------------\n",
    "\n",
    "                num_images_processed += 1\n",
    "\n",
    "    # --- Calculate Final Average Metrics ---\n",
    "    if num_images_processed == 0: # Handle empty dataloader case\n",
    "        print(\"Evaluation dataloader was empty.\")\n",
    "        return 0.0, 0.0, 0.0 # Loss, Dice, IoU\n",
    "\n",
    "    avg_loss = total_loss / num_images_processed\n",
    "\n",
    "    # --- Final Aggregated Dice Calculation ---\n",
    "    # Using Micro-average: (Sum of numerators) / (Sum of denominators)\n",
    "    dice_numerator = 2. * total_dice_intersect + smooth_eval\n",
    "    dice_denominator = total_dice_sum_pred + total_dice_sum_gt + smooth_eval\n",
    "    # Create mask for valid classes (excluding ignore_index)\n",
    "    valid_class_mask_dice = torch.ones(num_classes, dtype=torch.bool)\n",
    "    if ignore_index is not None and 0 <= ignore_index < num_classes:\n",
    "        valid_class_mask_dice[ignore_index] = False\n",
    "\n",
    "    # Calculate micro average score over valid classes\n",
    "    avg_dice_micro = 0.0\n",
    "    if valid_class_mask_dice.sum() > 0:\n",
    "        avg_dice_micro = (dice_numerator[valid_class_mask_dice].sum() /\n",
    "                         torch.clip(dice_denominator[valid_class_mask_dice].sum(), 1e-8)).item()\n",
    "\n",
    "    # --- Optional: Macro Average Dice ---\n",
    "    per_class_dice = dice_numerator / torch.clip(dice_denominator, 1e-8)\n",
    "    avg_dice_macro = 0.0\n",
    "    if valid_class_mask_dice.sum() > 0:\n",
    "        avg_dice_macro = per_class_dice[valid_class_mask_dice].mean().item()\n",
    "\n",
    "\n",
    "    # --- Final Aggregated IoU Calculation ---\n",
    "    # Create mask for valid classes (excluding ignore_index) for IoU\n",
    "    valid_class_mask_iou = torch.ones(num_classes, dtype=torch.bool)\n",
    "    if ignore_index is not None and 0 <= ignore_index < num_classes:\n",
    "        valid_class_mask_iou[ignore_index] = False\n",
    "\n",
    "    # Calculate per-class IoU using aggregated counts\n",
    "    # Add epsilon to denominator for stability\n",
    "    epsilon = 1e-8\n",
    "    per_class_iou = total_iou_intersection / (total_iou_union + epsilon)\n",
    "\n",
    "    # Calculate Mean IoU (mIoU) over valid classes\n",
    "    mean_iou = 0.0\n",
    "    if valid_class_mask_iou.sum() > 0:\n",
    "        mean_iou = per_class_iou[valid_class_mask_iou].mean().item()\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"  Images Processed: {num_images_processed}\")\n",
    "    print(f\"  Average Loss (Original Size): {avg_loss:>8f}\")\n",
    "    print(f\"  Micro Avg Dice Score ({valid_class_mask_dice.sum().item()} classes): {avg_dice_micro:>8f}\")\n",
    "    print(f\"  Macro Avg Dice Score ({valid_class_mask_dice.sum().item()} classes): {avg_dice_macro:>8f}\")\n",
    "    print(f\"  Mean IoU (mIoU) ({valid_class_mask_iou.sum().item()} classes): {mean_iou:>8f}\")\n",
    "    print(f\"  --- Per-Class IoU ---\")\n",
    "    for c in range(num_classes):\n",
    "        if valid_class_mask_iou[c]: # Only print for valid classes\n",
    "            print(f\"    Class {c}: {per_class_iou[c].item():>8f}\")\n",
    "        else:\n",
    "            print(f\"    Class {c}: Ignored\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # Return relevant metrics for model saving (e.g., micro dice and mIoU)\n",
    "    return avg_loss, avg_dice_micro, mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "if 'accumulation_steps' not in locals() :\n",
    "    # ... (keep your accumulation steps calculation) ...\n",
    "    assert 'target_batch_size' in globals() and 'batch_size' in globals(), \\\n",
    "           \"Please define target_batch_size and batch_size\"\n",
    "    assert target_batch_size >= batch_size, \"target_batch_size must be >= batch_size\"\n",
    "    assert target_batch_size % batch_size == 0, \"target_batch_size must be divisible by batch_size for simple accumulation\"\n",
    "    accumulation_steps = target_batch_size // batch_size\n",
    "    print(f\"Using Gradient Accumulation: effective batch size {target_batch_size} ({accumulation_steps} steps)\")\n",
    "\n",
    "\n",
    "# --- Define Model, Loss, Optimizer, Scheduler ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = unet().to(device) # Assuming unet() exists and returns your U-Net model\n",
    "\n",
    "# --- Define Loss, Optimizer, Scheduler ---\n",
    "# Configure ignore_index (e.g., 3 to ignore class 3, or 255 if used in labels, None otherwise)\n",
    "EVAL_IGNORE_INDEX = 3 # Example: ignore class 3 during evaluation metric calculation\n",
    "TRAIN_IGNORE_INDEX = None  # Example: train on all classes (0,1,2,3)\n",
    "\n",
    "loss_fn = DiceCELoss(ignore_index=TRAIN_IGNORE_INDEX, smooth_dice=1) # Training loss\n",
    "# Evaluation loss object used inside eval loop only to get settings like ignore_index\n",
    "# It is NOT used to calculate the loss score reported for eval (that uses training loss object)\n",
    "# But we pass it to eval_loop so it knows which index to ignore for metric calc if needed\n",
    "eval_settings_provider = DiceCELoss(ignore_index=EVAL_IGNORE_INDEX)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=SGD_MOMENTUM,\n",
    "                      weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "\n",
    "steps_per_epoch = (len(train_dataloader) // accumulation_steps) + (1 if len(train_dataloader) % accumulation_steps != 0 else 0)\n",
    "total_iters = steps_per_epoch * EPOCHS\n",
    "scheduler = PolynomialLR(optimizer, total_iters=total_iters, power=0.9)\n",
    "\n",
    "best_dev_dice = -np.inf # Track best Dice score\n",
    "best_dev_miou = -np.inf # Track best mIoU\n",
    "best_dev_loss = np.inf # Track loss corresponding to best metric\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "print(\"\\nStarting Training (nnU-Net style)...\")\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, accumulation_steps, device)\n",
    "\n",
    "    # Pass the eval_settings_provider to eval_loop\n",
    "    val_loss, val_dice_micro, val_miou = eval_loop(val_dataloader, model, eval_settings_provider, device)\n",
    "\n",
    "    # Save model based on validation MICRO DICE score improvement\n",
    "    # Could also choose mIoU validation by changing 'val_dice_micro > best_dev_dice'\n",
    "    if val_dice_micro > best_dev_dice:\n",
    "        best_dev_dice = val_dice_micro\n",
    "        best_dev_miou = val_miou # Save corresponding mIoU\n",
    "        best_dev_loss = val_loss # Save corresponding loss\n",
    "        print(f\"Validation Micro Dice score improved ({best_dev_dice:.6f}). Saving model...\")\n",
    "        checkpoint_path = os.path.join(MODEL_SAVE_DIR, \"unet_best_dice.pytorch\") # Changed name\n",
    "        checkpoint = {\n",
    "            \"epoch\": t + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"best_dev_dice\": best_dev_dice,\n",
    "            \"best_dev_miou\": best_dev_miou,\n",
    "            \"best_dev_loss\": best_dev_loss,\n",
    "            \"notes\": f\"Model saved based on best Micro Dice. Ignored index for metric: {EVAL_IGNORE_INDEX}\"\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    else:\n",
    "        print(f\"Validation Micro Dice score did not improve from {best_dev_dice:.6f}\")\n",
    "\n",
    "print(\"\\n--- Training Finished! ---\")\n",
    "print(f\"Best validation Micro Dice score achieved: {best_dev_dice:.6f}\")\n",
    "print(f\"Corresponding validation mIoU: {best_dev_miou:.6f}\")\n",
    "print(f\"Corresponding validation loss: {best_dev_loss:.6f}\")\n",
    "print(f\"Best model saved to: {os.path.join(MODEL_SAVE_DIR, 'unet_best_dice.pytorch')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVAL Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet().to(device)\n",
    "loss_fn = DiceCELoss(ignore_index=3)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=SGD_MOMENTUM,\n",
    "                      weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "\n",
    "total_iters = (len(train_dataloader) // accumulation_steps) * EPOCHS\n",
    "scheduler = PolynomialLR(optimizer, total_iters=total_iters, power=0.9)\n",
    "\n",
    "checkpoint = torch.load(\"unet/checkpoint\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "# scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "new_dev_loss = eval_loop(val_dataloader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "dice = MemoryEfficientDiceLoss()\n",
    "\n",
    "model = unet().to(device)\n",
    "checkpoint = torch.load(\"unet/checkpoint\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "pred = model(X[0].unsqueeze(0))\n",
    "\n",
    "print(dice(pred, y[0].unsqueeze(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
