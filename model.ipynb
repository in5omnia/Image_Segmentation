{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dout, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "    \n",
    "class up(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dout, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scale = 1 # maybe using the original channels might be better\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.down1 = down(3, self.scale * 64)\n",
    "        self.down2 = down(self.scale * 64, self.scale * 128)\n",
    "        self.down3 = down(self.scale * 128, self.scale * 256)\n",
    "        self.down4 = down(self.scale * 256, self.scale * 512)\n",
    "        self.down5 = down(self.scale * 512, self.scale * 1024)\n",
    "        self.up1 = up(self.scale * 1024, self.scale * 512)\n",
    "        self.up2 = up(self.scale * 512, self.scale * 256)\n",
    "        self.up3 = up(self.scale * 256, self.scale * 128)\n",
    "        self.up4 = up(self.scale * 128, self.scale * 64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.upconv1 = nn.Conv2d(self.scale * 1024, self.scale * 512, kernel_size=3, padding=1)\n",
    "        self.upconv2 = nn.Conv2d(self.scale * 512, self.scale * 256, kernel_size=3, padding=1)\n",
    "        self.upconv3 = nn.Conv2d(self.scale * 256, self.scale * 128, kernel_size=3, padding=1)\n",
    "        self.upconv4 = nn.Conv2d(self.scale * 128, self.scale * 64, kernel_size=3, padding=1)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(self.scale * 64, 4, kernel_size=1),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(self.maxpool(x1))\n",
    "        x3 = self.down3(self.maxpool(x2))\n",
    "        x4 = self.down4(self.maxpool(x3))\n",
    "        x5 = self.down5(self.maxpool(x4))\n",
    "        x = self.up1(torch.cat([x4, self.upconv1(self.upsample(x5))], dim=1))\n",
    "        x = self.up2(torch.cat([x3, self.upconv2(self.upsample(x))], dim=1))\n",
    "        x = self.up3(torch.cat([x2, self.upconv3(self.upsample(x))], dim=1))\n",
    "        x = self.up4(torch.cat([x1, self.upconv4(self.upsample(x))], dim=1))\n",
    "        pre_output = self.output(x)\n",
    "        return pre_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, PILToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "target_batch_size = 64\n",
    "batch_size = 4\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_names = sorted([os.path.splitext(filename)[0] for filename in os.listdir(img_dir)])\n",
    "        self.len = len(self.img_names)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = decode_image(os.path.join(self.img_dir, self.img_names[idx] + \".jpg\")).float()/255\n",
    "        label = decode_image(os.path.join(self.label_dir, self.img_names[idx] + \".png\"))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "def display_img_label(data, idx):\n",
    "    img, label = data[idx]\n",
    "    figure = plt.figure(figsize=(10,20))\n",
    "    figure.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    figure.add_subplot(1, 2, 2)\n",
    "    plt.imshow(label.permute(1, 2, 0), cmap='grey')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class target_remap(object):\n",
    "    def __call__(self, img):\n",
    "        img[img == 255] = 3\n",
    "        return img\n",
    "\n",
    "def diff_size_collate(batch):\n",
    "    imgs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "\n",
    "training_data = dataset(\"atrain/color\", \"atrain/label\", target_transform=target_remap())\n",
    "val_data = dataset(\"Val/color\", \"Val/label\", target_transform=target_remap())\n",
    "test_data = dataset(\"Test/color\", \"Test/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# use_amp = True\n",
    "\n",
    "# def train_loop(dataloader, model, loss_fn, optimizer, scaler):\n",
    "#     size = len(dataloader.dataset)\n",
    "\n",
    "#     to_print=True\n",
    "#     losses = []\n",
    "#     model.train()\n",
    "#     for batch, (X, y) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "#     # for batch, (X, y) in enumerate(dataloader):\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "#         with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "#             pred = model(X)\n",
    "#             loss = loss_fn(pred, y.squeeze(1).long())\n",
    "#         # print(f\"batch {batch} occupying: {torch.mps.current_allocated_memory()}\", flush=True)\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#         if batch % (target_batch_size//batch_size) == 0:\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             if to_print:\n",
    "#                 print(f\"batch {batch} occupying: {torch.cuda.device_memory_used()}\", flush=True)\n",
    "#                 to_print = False\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn):\n",
    "#     # size = len(dataloader.dataset)\n",
    "#     model.eval()\n",
    "\n",
    "#     num_batches = len(dataloader)\n",
    "#     loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in tqdm(dataloader, total=len(dataloader), desc=\"Test\"):\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "\n",
    "#             with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "#                 pred = model(X)\n",
    "#                 loss += loss_fn(pred, y.squeeze(1).long()).item()\n",
    "\n",
    "#     loss /= num_batches\n",
    "#     print(f\"Test Error: \\nAvg loss: {loss:>8f} \\n\")\n",
    "#     return loss\n",
    "\n",
    "# model = unet().to(device)\n",
    "# # model = UNet(3,4).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# scaler = torch.amp.GradScaler(\"cuda\" ,enabled=use_amp)\n",
    "\n",
    "# model_path = \"unet\"\n",
    "\n",
    "# epochs = 100\n",
    "# dev_loss = np.inf\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer, scaler)\n",
    "#     new_dev_loss = test_loop(test_dataloader, model, loss_fn)\n",
    "    \n",
    "#     if new_dev_loss <= dev_loss:\n",
    "#         dev_loss = new_dev_loss\n",
    "#         checkpoint = {\"model\": model.state_dict(),\n",
    "#               \"optimizer\": optimizer.state_dict(),\n",
    "#               \"scaler\": scaler.state_dict()}\n",
    "#         torch.save(checkpoint, f\"unet/checkpoint\")\n",
    "        \n",
    "# print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume these are defined somewhere in your code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_amp = True  # or False depending on your setup\n",
    "\n",
    "def resize_with_padding(image, target_size=512):\n",
    "    \"\"\"\n",
    "    Resize a single image (Tensor of shape (C, H, W)) so that the longer side\n",
    "    equals target_size, preserving aspect ratio; add black padding as needed.\n",
    "    Returns the resized and padded image, plus a metadata dictionary.\n",
    "    \"\"\"\n",
    "    _, orig_h, orig_w = image.shape\n",
    "    scale = min(target_size / orig_w, target_size / orig_h)\n",
    "    new_w = int(round(orig_w * scale))\n",
    "    new_h = int(round(orig_h * scale))\n",
    "    \n",
    "    # Resize the image\n",
    "    image_resized = TF.resize(image, size=(new_h, new_w))\n",
    "    \n",
    "    # Compute padding on each side\n",
    "    pad_w = target_size - new_w\n",
    "    pad_h = target_size - new_h\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "\n",
    "    # Pad the image (padding order: left, top, right, bottom)\n",
    "    image_padded = TF.pad(image_resized, padding=(pad_left, pad_top, pad_right, pad_bottom), fill=0)\n",
    "\n",
    "    meta = {\n",
    "        \"original_size\": (orig_h, orig_w),\n",
    "        \"new_size\": (new_h, new_w),\n",
    "        \"pad\": (pad_left, pad_top, pad_right, pad_bottom),\n",
    "        \"scale\": scale\n",
    "    }\n",
    "    return image_padded, meta\n",
    "\n",
    "def reverse_resize_and_padding(image, meta, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Remove the padding from image (Tensor of shape (C, target_size, target_size))\n",
    "    using metadata and then resize the cropped image back to the original size.\n",
    "    interpolation: \"bilinear\" for continuous outputs; use \"nearest\" for label maps.\n",
    "    \"\"\"\n",
    "    pad_left, pad_top, pad_right, pad_bottom = meta[\"pad\"]\n",
    "    new_h, new_w = meta[\"new_size\"]\n",
    "    \n",
    "    # Crop out the padding: from pad_top to pad_top+new_h and pad_left to pad_left+new_w.\n",
    "    image_cropped = image[..., pad_top: pad_top + new_h, pad_left: pad_left + new_w]\n",
    "    \n",
    "    # Resize the cropped image back to the original size.\n",
    "    orig_h, orig_w = meta[\"original_size\"]\n",
    "    # F.interpolate expects a 4D tensor.\n",
    "    image_original = F.interpolate(image_cropped.unsqueeze(0),\n",
    "                                   size=(orig_h, orig_w),\n",
    "                                   mode=interpolation,\n",
    "                                   align_corners=False if interpolation != \"nearest\" else None)\n",
    "    return image_original.squeeze(0)\n",
    "\n",
    "def process_batch_forward(batch_images, target_size=512):\n",
    "    \"\"\"\n",
    "    Process a batch (Tensor of shape (N, C, H, W)) by resizing each image to target_size\n",
    "    with aspect ratio preserved (adding black padding).\n",
    "    Returns the processed batch and a list of meta dictionaries.\n",
    "    \"\"\"\n",
    "    resized_batch = []\n",
    "    meta_list = []\n",
    "    for image in batch_images:\n",
    "        if image.ndim == 3 and image.shape[0] == 4:\n",
    "            image = image[:3, ...] # Slice to keep only the first 3 channels (R, G, B)\n",
    "        image_resized, meta = resize_with_padding(image, target_size)\n",
    "        resized_batch.append(image_resized)\n",
    "        meta_list.append(meta)\n",
    "    return torch.stack(resized_batch), meta_list\n",
    "\n",
    "def process_batch_reverse(batch_outputs, meta_list, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Given a batch of network outputs of shape (N, C, target_size, target_size) and the\n",
    "    corresponding meta info, reverse the transform for each one to obtain predictions at their\n",
    "    original sizes.\n",
    "    \"\"\"\n",
    "    original_outputs = []\n",
    "    for output, meta in zip(batch_outputs, meta_list):\n",
    "        restored = reverse_resize_and_padding(output, meta, interpolation=interpolation)\n",
    "        original_outputs.append(restored)\n",
    "    return original_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "EPOCHS = 1000\n",
    "MODEL_SAVE_DIR = \"unet\" # Changed path to avoid overwriting original\n",
    "\n",
    "# Ensure model save directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Training Loop (No AMP, No GradScaler, With Gradient Accumulation) ---\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, accumulation_steps, device):\n",
    "    \"\"\"Performs one epoch of training with gradient accumulation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0\n",
    "\n",
    "    optimizer.zero_grad() # Initialize gradients to zero at the start of the epoch\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.squeeze(1).long())\n",
    "\n",
    "        # Scale loss for gradient accumulation\n",
    "        # Average gradients over accumulation_steps batches\n",
    "        scaled_loss = loss / accumulation_steps\n",
    "\n",
    "        # Backward pass (accumulates gradients)\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        # Optimizer step (perform after accumulation_steps)\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step() # Update weights\n",
    "            optimizer.zero_grad() # Reset gradients for the next accumulation cycle\n",
    "            total_loss += loss.item() # Log the unscaled loss for monitoring\n",
    "            processed_batches += 1\n",
    "\n",
    "            if processed_batches == 1:\n",
    "                try:\n",
    "                    print(f\"Effective batch done. Memory allocated: {torch.cuda.memory_allocated(device)} bytes\", flush=True)\n",
    "                except Exception:\n",
    "                    pass # Handle cases where CUDA might not be available or memory check fails\n",
    "\n",
    "\n",
    "    # Handle any leftover gradients at the end of the epoch\n",
    "    # This occurs if len(dataloader) is not perfectly divisible by accumulation_steps\n",
    "    if len(dataloader) % accumulation_steps != 0:\n",
    "        print(\"Performing final optimizer step for leftover batches.\")\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Note: Loss from these leftover batches isn't fully added to total_loss avg\n",
    "        # For simplicity, we often ignore this slight imprecision in reporting epoch loss\n",
    "\n",
    "    avg_loss = total_loss / processed_batches if processed_batches > 0 else 0\n",
    "    print(f\"Training Avg loss (per effective batch): {avg_loss:>8f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def eval_loop(dataloader, model, loss_fn, device, target_size=512):\n",
    "    \"\"\"\n",
    "    Evaluation loop handling variable input sizes without AMP.\n",
    "\n",
    "    Args:\n",
    "       dataloader: yields batches of (list[Tensor(C,H,W)], list[Tensor(C,H,W)])\n",
    "       model: the neural network model (on device)\n",
    "       loss_fn: the loss function (e.g., CrossEntropyLoss)\n",
    "       device: the torch device (cuda or cpu)\n",
    "       target_size: the size for model input (default: 512)\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    num_images_processed = 0\n",
    "    total_loss = 0.0\n",
    "    interpolation_mode = \"bilinear\" # Use nearest for reversing label predictions\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for X_batch_list, y_batch_list in tqdm(dataloader, desc=\"Eval\"):\n",
    "\n",
    "            # 1. Forward Transform Inputs: Resize + Pad\n",
    "            # Input: list[Tensor(C,H,W)] -> Output: Tensor(N,C,target,target), list[meta]\n",
    "            X_processed, meta_list = process_batch_forward(X_batch_list, target_size=target_size)\n",
    "            X_processed = X_processed.to(device) # Move stacked batch to device\n",
    "\n",
    "            # 2. Model Inference (No Autocast)\n",
    "            # Input: Tensor(N,C,target,target) -> Output: Tensor(N,NumClasses,target,target)\n",
    "            pred_processed = model(X_processed)\n",
    "\n",
    "            # 3. Reverse Transform Outputs: Crop + Resize back\n",
    "            # Input: Tensor(N,NumClasses,target,target), list[meta] -> Output: list[Tensor(NumClasses,H_orig,W_orig)]\n",
    "            # Move preds to CPU if reverse func requires it, otherwise keep on device\n",
    "            pred_original_list = process_batch_reverse(\n",
    "                pred_processed, # Keep on device if reverse function supports it\n",
    "                # pred_processed.cpu(), # Uncomment if reverse expects CPU tensors\n",
    "                meta_list,\n",
    "                interpolation=interpolation_mode\n",
    "            )\n",
    "\n",
    "            # 4. Compute Loss for each image in the batch\n",
    "            for pred_single, label_single in zip(pred_original_list, y_batch_list):\n",
    "                # Ensure label is on device for loss calculation\n",
    "                label_single = label_single.to(device)\n",
    "\n",
    "                label_single = label_single.long()\n",
    "\n",
    "                # Prepare prediction (on device) and add batch dimension\n",
    "                # Input pred_single shape: [NumClasses, H_orig, W_orig]\n",
    "                pred_single = pred_single.to(device).unsqueeze(0) # Shape: [1, NumClasses, H_orig, W_orig]\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(pred_single, label_single)\n",
    "                total_loss += loss.item()\n",
    "                num_images_processed += 1\n",
    "\n",
    "    # Calculate Average Loss\n",
    "    avg_loss = total_loss / num_images_processed if num_images_processed > 0 else 0.0\n",
    "\n",
    "    print(f\"\\nEvaluation Complete.\")\n",
    "    print(f\"  Images Processed: {num_images_processed}\")\n",
    "    print(f\"  Average Loss: {avg_loss:>8f}\\n\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "if 'accumulation_steps' not in locals() : # Calculate accumulation steps if not already defined\n",
    "    assert 'target_batch_size' in globals() and 'batch_size' in globals(), \\\n",
    "           \"Please define target_batch_size and batch_size\"\n",
    "    assert target_batch_size >= batch_size, \"target_batch_size must be >= batch_size\"\n",
    "    assert target_batch_size % batch_size == 0, \"target_batch_size must be divisible by batch_size for simple accumulation\"\n",
    "    accumulation_steps = target_batch_size // batch_size\n",
    "    print(f\"Using Gradient Accumulation: effective batch size {target_batch_size} ({accumulation_steps} steps)\")\n",
    "\n",
    "\n",
    "model = unet().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters()) # Adjust learning rate if needed\n",
    "\n",
    "best_dev_loss = np.inf\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "print(\"Starting Training...\")\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer, accumulation_steps, device)\n",
    "    new_dev_loss = eval_loop(val_dataloader, model, loss_fn, device)\n",
    "\n",
    "    # Save model checkpoint if validation loss improves\n",
    "    if new_dev_loss < best_dev_loss:\n",
    "        best_dev_loss = new_dev_loss\n",
    "        print(f\"Validation loss improved ({best_dev_loss:.6f}). Saving model...\")\n",
    "        checkpoint_path = os.path.join(MODEL_SAVE_DIR, \"unet_aug.pytorch\")\n",
    "        checkpoint = {\n",
    "            \"epoch\": t + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"best_dev_loss\": best_dev_loss,\n",
    "            # Add any other info you want to save (e.g., learning rate scheduler state)\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    else:\n",
    "        print(f\"Validation loss did not improve from {best_dev_loss:.6f}\")\n",
    "\n",
    "print(\"\\nTraining Finished!\")\n",
    "print(f\"Best validation loss achieved: {best_dev_loss:.6f}\")\n",
    "print(f\"Best model saved to: {os.path.join(MODEL_SAVE_DIR, 'best_checkpoint.pytorch')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# model.cpu()\n",
    "# del model\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters()) # Adjust learning rate if needed\n",
    "\n",
    "checkpoint = torch.load(\"unet/checkpoint\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "# scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "new_dev_loss = eval_loop(val_dataloader, model, loss_fn, device)\n",
    "\n",
    "# model.eval()\n",
    "# X, y = next(iter(test_dataloader))\n",
    "\n",
    "# X = X.to(device)\n",
    "# with torch.no_grad():\n",
    "#   with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):    \n",
    "#     res = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(res[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "def plot_tensor_with_custom_colors(tensor, color_map):\n",
    "    \"\"\"\n",
    "    Plots a tensor with custom colors using matplotlib.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): A 2D or 3D tensor with integer values representing different categories.\n",
    "                                The values should correspond to keys in the color_map.\n",
    "        color_map (dict): A dictionary mapping integer values to RGB color tuples (e.g., (0, 0, 0) for black).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy if it's a tensor\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        tensor = tensor.cpu().numpy()\n",
    "\n",
    "    # Ensure the tensor is 2D or 3D\n",
    "    if len(tensor.shape) not in [2, 3]:\n",
    "        raise ValueError(\"Tensor must be 2D or 3D\")\n",
    "\n",
    "    # Create an RGB image from the color map\n",
    "    height, width = tensor.shape[:2]  # Handle both 2D and 3D tensors\n",
    "    rgb_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    for value, color in color_map.items():\n",
    "        mask = (tensor == value)\n",
    "        rgb_image[mask] = color\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "idx=1\n",
    "# Create a dummy tensor\n",
    "image_tensor = torch.argmax(res[idx], dim=0)  # Values 0, 1, 2, 3\n",
    "\n",
    "# Define the color map\n",
    "color_map = {\n",
    "    0: (0, 0, 0),      # Black\n",
    "    1: (255, 0, 0),    # Red\n",
    "    2: (0, 255, 0),    # Green\n",
    "    3: (255, 255, 255),  # White\n",
    "    255: (255,255,255)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Plot the tensor with custom colors\n",
    "plot_tensor_with_custom_colors(image_tensor, color_map)\n",
    "img_np = X[idx].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.show()\n",
    "plot_tensor_with_custom_colors(y[idx].squeeze(), color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "eval_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = decode_image(\"Test/color/american_bulldog_5.jpg\").float()/255\n",
    "plt.imshow(img.permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "\n",
    "img, meta = resize_with_padding(img)\n",
    "plt.imshow(img.permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "\n",
    "img = img.to(device)\n",
    "pred = model(img.unsqueeze(0))\n",
    "rpred = reverse_resize_and_padding(pred.squeeze(0), meta, interpolation=\"nearest\")\n",
    "pred = torch.argmax(pred[0], dim=0)\n",
    "rpred = torch.argmax(rpred, dim=0)\n",
    "plot_tensor_with_custom_colors(pred, color_map)\n",
    "plot_tensor_with_custom_colors(rpred, color_map)\n",
    "\n",
    "label = decode_image(\"Test/label/american_bulldog_5.png\")\n",
    "plot_tensor_with_custom_colors(label.squeeze(0), color_map)\n",
    "# print(label.size())\n",
    "# plt.imshow(label.permute(1,2,0).numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
