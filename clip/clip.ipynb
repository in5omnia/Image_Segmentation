{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T00:05:54.955728Z",
     "iopub.status.busy": "2025-04-03T00:05:54.955402Z",
     "iopub.status.idle": "2025-04-03T00:06:16.411135Z",
     "shell.execute_reply": "2025-04-03T00:06:16.410237Z",
     "shell.execute_reply.started": "2025-04-03T00:05:54.955704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPVisionModel, CLIPVisionConfig, CLIPImageProcessor\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "IMG_SIZE = 224\n",
    "SKIP_LAYER_INDICES = [3, 5, 7, 9]\n",
    "\n",
    "# --- CLIP ViT Encoder ---\n",
    "class ClipViTEncoder(nn.Module):\n",
    "    def __init__(self, model_name=PRETRAINED_MODEL_NAME, freeze_encoder=True, skip_indices=SKIP_LAYER_INDICES):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.skip_indices = sorted(skip_indices, reverse=True) # Process deeper layers first for bottleneck logic\n",
    "\n",
    "        self.config = CLIPVisionConfig.from_pretrained(model_name)\n",
    "        self.clip_vit = CLIPVisionModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for param in self.clip_vit.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Calculate grid size\n",
    "        self.patch_size = self.config.patch_size\n",
    "        # Ensure image size in config matches expected if not default\n",
    "        if self.config.image_size != IMG_SIZE:\n",
    "             warnings.warn(f\"Model config image size {self.config.image_size} differs from specified IMG_SIZE {IMG_SIZE}. Using model config size for grid calculation.\")\n",
    "        self.grid_size = self.config.image_size // self.patch_size\n",
    "        self.hidden_dim = self.config.hidden_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "\n",
    "        # print(f\"  Model: {model_name}\")\n",
    "        # print(f\"  Expected Input Image Size (from config): {self.config.image_size}x{self.config.image_size}\")\n",
    "        # print(f\"  Patch Size: {self.patch_size}x{self.patch_size}\")\n",
    "        # print(f\"  Calculated Grid Size (G): {self.grid_size}x{self.grid_size}\")\n",
    "        # print(f\"  Calculated Num Patches (N_p = G*G): {self.num_patches}\")\n",
    "        # print(f\"  Hidden Dim (D_vit): {self.hidden_dim}\")\n",
    "        # print(f\"  Targeting hidden states at indices: {self.skip_indices}\")\n",
    "        # print(f\"  Encoder frozen: {freeze_encoder}\")\n",
    "        # print(\"--- ClipViTEncoder Initialized ---\\n\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"\\n--- ClipViTEncoder Forward ---\")\n",
    "        # print(f\"  Input shape: {x.shape}\")\n",
    "        if x.shape[2] != self.config.image_size or x.shape[3] != self.config.image_size:\n",
    "             warnings.warn(\n",
    "                 f\"Input image size ({x.shape[2]}x{x.shape[3]}) doesn't match \"\n",
    "                 f\"CLIP expected size ({self.config.image_size}x{self.config.image_size}). \"\n",
    "                 f\"Behavior may be unexpected. Consider resizing input.\"\n",
    "             )\n",
    "\n",
    "        # Pass image through CLIP ViT\n",
    "        outputs = self.clip_vit(pixel_values=x, output_hidden_states=True)\n",
    "        all_hidden_states = outputs.hidden_states\n",
    "\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        patch_embeddings = last_hidden_state[:, 1:, :]\n",
    "\n",
    "        if patch_embeddings.shape[1] != self.num_patches:\n",
    "             print(\"here1\")\n",
    "             current_num_patches = patch_embeddings.shape[1]\n",
    "             current_grid_size = int(sqrt(current_num_patches))\n",
    "             if current_grid_size * current_grid_size != current_num_patches:\n",
    "                 raise ValueError(f\"Cannot reshape patch embeddings.\")\n",
    "             warnings.warn(f\"Patch count mismatch. Reshaping to {current_grid_size}x{current_grid_size}.\")\n",
    "             grid_h, grid_w = current_grid_size, current_grid_size\n",
    "        else:\n",
    "             grid_h, grid_w = self.grid_size, self.grid_size\n",
    "\n",
    "        bottleneck_features = patch_embeddings.reshape(x.shape[0], grid_h, grid_w, self.hidden_dim).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        skip_features_list = []\n",
    "\n",
    "        # print(\"  Extracting features from specified hidden state indices:\")\n",
    "        for i in self.skip_indices: # Iterates [12, 9, 6, 3]\n",
    "            # print(f\"    Processing hidden state index: {i}\")\n",
    "            hidden_state = all_hidden_states[i]\n",
    "            # print(f\"      Original hidden_state shape: {hidden_state.shape}\") # Should be (B, N_p+1, D_vit)\n",
    "\n",
    "            # Remove the CLS token embedding\n",
    "            patch_embeddings = hidden_state[:, 1:, :]\n",
    "            # print(f\"      Shape after removing CLS token: {patch_embeddings.shape}\") # Should be (B, N_p, D_vit)\n",
    "\n",
    "            # Check num_patches consistency\n",
    "            if patch_embeddings.shape[1] != self.num_patches:\n",
    "                 # Handle potential mismatch (e.g., if input size wasn't exactly config.image_size)\n",
    "                 current_num_patches = patch_embeddings.shape[1]\n",
    "                 current_grid_size = int(sqrt(current_num_patches))\n",
    "                 if current_grid_size * current_grid_size != current_num_patches:\n",
    "                     raise ValueError(f\"Cannot reliably reshape patch embeddings. Non-square grid? Expected {self.num_patches} patches, got {current_num_patches}.\")\n",
    "                 warnings.warn(f\"Actual patch count {current_num_patches} differs from expected {self.num_patches}. Reshaping to {current_grid_size}x{current_grid_size}.\")\n",
    "                 grid_h, grid_w = current_grid_size, current_grid_size\n",
    "            else:\n",
    "                 grid_h, grid_w = self.grid_size, self.grid_size\n",
    "\n",
    "            # Reshape to (B, grid_h, grid_w, D_vit) -> (B, D_vit, grid_h, grid_w)\n",
    "            reshaped_features = patch_embeddings.reshape(\n",
    "                x.shape[0], grid_h, grid_w, self.hidden_dim\n",
    "            )\n",
    "            reshaped_features = reshaped_features.permute(0, 3, 1, 2).contiguous()\n",
    "            # print(f\"      Shape after reshaping to grid (B, D_vit, G, G): {reshaped_features.shape}\")\n",
    "\n",
    "            skip_features_list.append(reshaped_features)\n",
    "\n",
    "        # Reverse the collected skips to be shallowest feature first\n",
    "        skip_features_list.reverse() # Now order matches [index_3_out, index_6_out, index_9_out]\n",
    "\n",
    "        # print(\"--- ClipViTEncoder Forward End ---\")\n",
    "        return bottleneck_features, skip_features_list\n",
    "\n",
    "# --- Example U-Net Decoder Block ---\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, block_index, in_channels_upsample, in_channels_skip, out_channels, skip_spatial_size, target_spatial_size):\n",
    "        super().__init__()\n",
    "        self.block_index = block_index\n",
    "        # print(f\"    Initializing DecoderBlock {self.block_index}: Upsample_in={in_channels_upsample}, Skip_in={in_channels_skip}, Out={out_channels}\")\n",
    "        self.skip_spatial_size = skip_spatial_size\n",
    "        self.target_spatial_size = target_spatial_size\n",
    "        self.upsample_out_channels = in_channels_upsample // 2\n",
    "        self.skip_conv_out_channels = in_channels_upsample // 2 # Match for concatenation\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels_upsample, self.upsample_out_channels, kernel_size=2, stride=2)\n",
    "        self.skip_conv = nn.Conv2d(in_channels_skip, self.skip_conv_out_channels, kernel_size=1) # 1x1 conv\n",
    "\n",
    "        conv_in_channels = self.upsample_out_channels + self.skip_conv_out_channels\n",
    "        # print(f\"      Block {self.block_index}: Upsample out channels={self.upsample_out_channels}, SkipConv out channels={self.skip_conv_out_channels}, Concat channels={conv_in_channels}\")\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(conv_in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_upsample, x_skip):\n",
    "        # print(f\"\\n  --- DecoderBlock {self.block_index} Forward ---\")\n",
    "        # print(f\"    Input x_upsample shape: {x_upsample.shape}\")\n",
    "        # print(f\"    Input x_skip shape: {x_skip.shape}\")\n",
    "\n",
    "        x_upsample = self.upsample(x_upsample)\n",
    "        # print(f\"    Shape after upsample: {x_upsample.shape}\")\n",
    "\n",
    "        x_skip_proj = self.skip_conv(x_skip)\n",
    "        # print(f\"    Shape after skip_conv (channel adjust): {x_skip_proj.shape}\")\n",
    "\n",
    "        # Resize skip connection spatially if needed\n",
    "        if x_skip_proj.shape[2:] != x_upsample.shape[2:]:\n",
    "            #   print(f\"    Resizing skip connection from {x_skip_proj.shape[2:]} to {x_upsample.shape[2:]}\")\n",
    "              x_skip_resized = F.interpolate(x_skip_proj, size=x_upsample.shape[2:], mode='bilinear', align_corners=False)\n",
    "            #   print(f\"    Shape after skip resize: {x_skip_resized.shape}\")\n",
    "        else:\n",
    "              x_skip_resized = x_skip_proj\n",
    "            #   print(f\"    Skip connection spatial size matches upsample, no resize needed.\")\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x_upsample, x_skip_resized], dim=1)\n",
    "        # print(f\"    Shape after concatenation: {x.shape}\")\n",
    "\n",
    "        # Apply convolutions\n",
    "        x = self.conv_block(x)\n",
    "        # print(f\"    Shape after conv_block (Output): {x.shape}\")\n",
    "        # print(f\"  --- DecoderBlock {self.block_index} Forward End ---\")\n",
    "        return x\n",
    "\n",
    "# --- Example U-Net Decoder ---\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, encoder_grid_size, decoder_channels):\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing UNetDecoder ---\")\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.encoder_grid_size = encoder_grid_size\n",
    "        self.decoder_channels = decoder_channels\n",
    "        num_blocks = len(decoder_channels)\n",
    "\n",
    "        # *** Important Check ***\n",
    "        num_expected_skips = len(SKIP_LAYER_INDICES) # One layer is bottleneck\n",
    "        # print(f\"  Encoder provides {num_expected_skips} skip connections (indices besides bottleneck).\")\n",
    "        # print(f\"  Decoder configured with {num_blocks} blocks for channels: {decoder_channels}.\")\n",
    "        if num_blocks != num_expected_skips:\n",
    "            warnings.warn(\n",
    "               f\"*** POTENTIAL MISMATCH: Number of decoder blocks ({num_blocks}) \"\n",
    "               f\"does not match the number of available skip connections ({num_expected_skips}). \"\n",
    "               f\"This might lead to an IndexError during forward pass if not handled carefully. ***\"\n",
    "            )\n",
    "        # The code proceeds to create num_blocks based on decoder_channels length\n",
    "\n",
    "        in_channels_upsample = encoder_hidden_dim\n",
    "        self.blocks = nn.ModuleList()\n",
    "        current_spatial_size = encoder_grid_size\n",
    "        # print(f\"  Creating {num_blocks} DecoderBlock(s):\")\n",
    "        for i in range(num_blocks):\n",
    "            out_ch = decoder_channels[i]\n",
    "            in_channels_skip = encoder_hidden_dim\n",
    "            skip_spatial_size = encoder_grid_size\n",
    "            target_spatial_size = current_spatial_size * 2 # Assumes stride-2 upsampling\n",
    "\n",
    "            # Pass index for clearer prints inside block\n",
    "            block = DecoderBlock(\n",
    "                block_index=i,\n",
    "                in_channels_upsample=in_channels_upsample,\n",
    "                in_channels_skip=in_channels_skip,\n",
    "                out_channels=out_ch,\n",
    "                skip_spatial_size=skip_spatial_size,\n",
    "                target_spatial_size=target_spatial_size\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "            in_channels_upsample = out_ch # Next block's upsample input is this block's output\n",
    "            current_spatial_size = target_spatial_size\n",
    "\n",
    "        # print(\"--- UNetDecoder Initialized ---\\n\")\n",
    "\n",
    "\n",
    "    def forward(self, bottleneck_features, skip_features):\n",
    "        # print(\"\\n--- UNetDecoder Forward ---\")\n",
    "        # print(f\"  Input bottleneck_features shape: {bottleneck_features.shape}\")\n",
    "        # print(f\"  Input skip_features list length: {len(skip_features)}\")\n",
    "        x = bottleneck_features\n",
    "        num_available_skips = len(skip_features)\n",
    "\n",
    "        # Iterate through decoder blocks (indices 0 to num_blocks-1)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            # print(f\"  Processing Decoder Block {i}\")\n",
    "            # Calculate index for skip_features list (shallowest first)\n",
    "            # Block 0 (deepest decoder) uses last skip (deepest skip feature)\n",
    "            skip_idx = num_available_skips - 1 - i\n",
    "            # print(f\"    Attempting to use skip feature at index: {skip_idx} (from list of {num_available_skips})\")\n",
    "\n",
    "            if skip_idx < 0 or skip_idx >= num_available_skips:\n",
    "                 # This will happen if num_blocks > num_available_skips\n",
    "                 raise IndexError(f\"Attempted to access skip_features[{skip_idx}] but only {num_available_skips} skips are available. Mismatch between decoder blocks and encoder skips.\")\n",
    "\n",
    "            skip = skip_features[skip_idx]\n",
    "            # print(f\"    Using skip feature with shape: {skip.shape}\")\n",
    "            x = block(x, skip) # Pass current features and corresponding skip\n",
    "\n",
    "        # print(f\"  Final output shape from UNetDecoder: {x.shape}\")\n",
    "        # print(\"--- UNetDecoder Forward End ---\")\n",
    "        return x\n",
    "\n",
    "# --- Combined CLIP-U-Net Model ---\n",
    "class ClipUNet(nn.Module):\n",
    "    def __init__(self, num_classes=4, decoder_channels=[512, 256, 128, 64], freeze_encoder=True):\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing ClipUNet ---\")\n",
    "        # print(f\"  Num output classes: {num_classes}\")\n",
    "        # print(f\"  Decoder channels specified: {decoder_channels}\")\n",
    "\n",
    "        self.encoder = ClipViTEncoder(freeze_encoder=freeze_encoder)\n",
    "\n",
    "        self.decoder = UNetDecoder(\n",
    "            encoder_hidden_dim=self.encoder.hidden_dim,\n",
    "            encoder_grid_size=self.encoder.grid_size,\n",
    "            decoder_channels=decoder_channels\n",
    "        )\n",
    "\n",
    "        # Final 1x1 convolution\n",
    "        last_decoder_channel = decoder_channels[-1] if decoder_channels else self.encoder.hidden_dim # Handle no decoder case?\n",
    "        # print(f\"  Final Conv: {last_decoder_channel} -> {num_classes} channels\")\n",
    "        self.final_conv = nn.Conv2d(last_decoder_channel, num_classes, kernel_size=1)\n",
    "\n",
    "        # Final upsampling layer check\n",
    "        final_decoder_size = self.encoder.grid_size * (2**len(decoder_channels))\n",
    "        # print(f\"  Calculated final decoder spatial size: {final_decoder_size}x{final_decoder_size}\")\n",
    "        if final_decoder_size != IMG_SIZE:\n",
    "            #  print(f\"  Final decoder size {final_decoder_size} != Target Image Size {IMG_SIZE}. Adding final Upsample layer.\")\n",
    "             self.final_upsample = nn.Upsample(size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            #  print(f\"  Final decoder size matches target image size. Final upsample is Identity.\")\n",
    "             self.final_upsample = nn.Identity()\n",
    "\n",
    "        # print(\"--- ClipUNet Initialized ---\\n\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"\\n\\n========== ClipUNet Forward Pass Start ==========\")\n",
    "        # print(f\"Overall Input shape: {x.shape}\")\n",
    "\n",
    "        # 1. Encoder\n",
    "        bottleneck, skips = self.encoder(x)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Encoder) ---\")\n",
    "        # print(f\"  Encoder returned bottleneck shape: {bottleneck.shape}\")\n",
    "        # print(f\"  Encoder returned {len(skips)} skip features.\")\n",
    "\n",
    "        # 2. Decoder\n",
    "        decoder_output = self.decoder(bottleneck, skips)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Decoder) ---\")\n",
    "        # print(f\"  Decoder returned output shape: {decoder_output.shape}\")\n",
    "\n",
    "        # 3. Final Convolution\n",
    "        output = self.final_conv(decoder_output)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Final Conv) ---\")\n",
    "        # print(f\"  Shape after final_conv: {output.shape}\")\n",
    "\n",
    "        # 4. Final Upsampling (if needed)\n",
    "        output = self.final_upsample(output)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Final Upsample) ---\")\n",
    "        # print(f\"  Shape after final_upsample: {output.shape}\")\n",
    "\n",
    "        # print(\"========== ClipUNet Forward Pass End ==========\\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T00:06:16.412803Z",
     "iopub.status.busy": "2025-04-03T00:06:16.412273Z",
     "iopub.status.idle": "2025-04-03T00:06:16.899270Z",
     "shell.execute_reply": "2025-04-03T00:06:16.898250Z",
     "shell.execute_reply.started": "2025-04-03T00:06:16.412768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, PILToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "target_batch_size = 64\n",
    "batch_size = 16\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_names = sorted([os.path.splitext(filename)[0] for filename in os.listdir(img_dir)])\n",
    "        self.len = len(self.img_names)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = decode_image(os.path.join(self.img_dir, self.img_names[idx] + \".jpg\")).float()/255\n",
    "        label = decode_image(os.path.join(self.label_dir, self.img_names[idx] + \".png\"))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "def display_img_label(data, idx):\n",
    "    img, label = data[idx]\n",
    "    figure = plt.figure(figsize=(10,20))\n",
    "    figure.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    figure.add_subplot(1, 2, 2)\n",
    "    plt.imshow(label.permute(1, 2, 0), cmap='grey')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class target_remap(object):\n",
    "    def __call__(self, img):\n",
    "        img[img == 255] = 3\n",
    "        return img\n",
    "\n",
    "def diff_size_collate(batch):\n",
    "    imgs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    return imgs, labels\n",
    "    \n",
    "training_data = dataset(\"astrain/color\", \"astrain/label\", target_transform=target_remap())\n",
    "val_data = dataset(\"Val/color\", \"Val/label\", target_transform=target_remap())\n",
    "test_data = dataset(\"Test/color\", \"Test/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T00:06:16.901831Z",
     "iopub.status.busy": "2025-04-03T00:06:16.901463Z",
     "iopub.status.idle": "2025-04-03T00:06:16.980519Z",
     "shell.execute_reply": "2025-04-03T00:06:16.979577Z",
     "shell.execute_reply.started": "2025-04-03T00:06:16.901797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume these are defined somewhere in your code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_amp = True  # or False depending on your setup\n",
    "\n",
    "def resize_with_padding(image, target_size=512):\n",
    "    \"\"\"\n",
    "    Resize a single image (Tensor of shape (C, H, W)) so that the longer side\n",
    "    equals target_size, preserving aspect ratio; add black padding as needed.\n",
    "    Returns the resized and padded image, plus a metadata dictionary.\n",
    "    \"\"\"\n",
    "    _, orig_h, orig_w = image.shape\n",
    "    scale = min(target_size / orig_w, target_size / orig_h)\n",
    "    new_w = int(round(orig_w * scale))\n",
    "    new_h = int(round(orig_h * scale))\n",
    "    \n",
    "    # Resize the image\n",
    "    image_resized = TF.resize(image, size=(new_h, new_w))\n",
    "    \n",
    "    # Compute padding on each side\n",
    "    pad_w = target_size - new_w\n",
    "    pad_h = target_size - new_h\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "\n",
    "    # Pad the image (padding order: left, top, right, bottom)\n",
    "    image_padded = TF.pad(image_resized, padding=(pad_left, pad_top, pad_right, pad_bottom), fill=0)\n",
    "\n",
    "    meta = {\n",
    "        \"original_size\": (orig_h, orig_w),\n",
    "        \"new_size\": (new_h, new_w),\n",
    "        \"pad\": (pad_left, pad_top, pad_right, pad_bottom),\n",
    "        \"scale\": scale\n",
    "    }\n",
    "    return image_padded, meta\n",
    "\n",
    "def reverse_resize_and_padding(image, meta, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Remove the padding from image (Tensor of shape (C, target_size, target_size))\n",
    "    using metadata and then resize the cropped image back to the original size.\n",
    "    interpolation: \"bilinear\" for continuous outputs; use \"nearest\" for label maps.\n",
    "    \"\"\"\n",
    "    pad_left, pad_top, pad_right, pad_bottom = meta[\"pad\"]\n",
    "    new_h, new_w = meta[\"new_size\"]\n",
    "    \n",
    "    # Crop out the padding: from pad_top to pad_top+new_h and pad_left to pad_left+new_w.\n",
    "    image_cropped = image[..., pad_top: pad_top + new_h, pad_left: pad_left + new_w]\n",
    "    \n",
    "    # Resize the cropped image back to the original size.\n",
    "    orig_h, orig_w = meta[\"original_size\"]\n",
    "    # F.interpolate expects a 4D tensor.\n",
    "    image_original = F.interpolate(image_cropped.unsqueeze(0),\n",
    "                                   size=(orig_h, orig_w),\n",
    "                                   mode=interpolation,\n",
    "                                   align_corners=False if interpolation != \"nearest\" else None)\n",
    "    return image_original.squeeze(0)\n",
    "\n",
    "def process_batch_forward(batch_images, target_size=512):\n",
    "    \"\"\"\n",
    "    Process a batch (Tensor of shape (N, C, H, W)) by resizing each image to target_size\n",
    "    with aspect ratio preserved (adding black padding).\n",
    "    Returns the processed batch and a list of meta dictionaries.\n",
    "    \"\"\"\n",
    "    resized_batch = []\n",
    "    meta_list = []\n",
    "    for image in batch_images:\n",
    "        if image.ndim == 3 and image.shape[0] == 4:\n",
    "            image = image[:3, ...] # Slice to keep only the first 3 channels (R, G, B) (RGBA to RGB)\n",
    "        image_resized, meta = resize_with_padding(image, target_size)\n",
    "        resized_batch.append(image_resized)\n",
    "        meta_list.append(meta)\n",
    "    return torch.stack(resized_batch), meta_list\n",
    "\n",
    "def process_batch_reverse(batch_outputs, meta_list, interpolation=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Given a batch of network outputs of shape (N, C, target_size, target_size) and the\n",
    "    corresponding meta info, reverse the transform for each one to obtain predictions at their\n",
    "    original sizes.\n",
    "    \"\"\"\n",
    "    original_outputs = []\n",
    "    for output, meta in zip(batch_outputs, meta_list):\n",
    "        restored = reverse_resize_and_padding(output, meta, interpolation=interpolation)\n",
    "        original_outputs.append(restored)\n",
    "    return original_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsHistory:\n",
    "    \"\"\"\n",
    "    Accumulates TP, FP, FN, TN over an epoch for multi-class segmentation\n",
    "    and computes Dice, IoU, and Accuracy metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, ignore_index: int = None, device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): Number of classes including background.\n",
    "            ignore_index (int, optional): Index of the class to ignore during metric calculation. Defaults to None.\n",
    "            device (str): Device to perform initial calculations, results accumulated on CPU.\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        self.total_tp = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "        self.total_fp = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "        self.total_fn = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "        self.total_tn = torch.zeros(num_classes, dtype=torch.float64, device='cpu')\n",
    "\n",
    "        # History lists for epoch metrics\n",
    "        self.epoch_mean_dice_history = []\n",
    "        self.epoch_mean_iou_history = []\n",
    "        self.epoch_mean_acc_history = []\n",
    "        \n",
    "        self.epoch_per_class_dice_history = []\n",
    "        self.epoch_per_class_iou_history = []\n",
    "        self.epoch_per_class_acc_history = []\n",
    "        \n",
    "        self.last_per_class_iou = None\n",
    "        self.last_per_class_dice = None\n",
    "        self.last_per_class_acc = None\n",
    "\n",
    "        # Metric mask (calculated once)\n",
    "        self.mask = torch.ones(num_classes, dtype=torch.bool)\n",
    "        if self.ignore_index is not None and 0 <= self.ignore_index < self.num_classes:\n",
    "            self.mask[self.ignore_index] = False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the accumulated TP, FP, FN, TN counts.\"\"\"\n",
    "        self.total_tp.zero_()\n",
    "        self.total_fp.zero_()\n",
    "        self.total_fn.zero_()\n",
    "        self.total_tn.zero_()\n",
    "\n",
    "    def accumulate(self, pred: torch.Tensor, label: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Accumulates statistics for a single prediction-label pair.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted logits or probabilities (C, H, W). Should be on self.device or moved.\n",
    "            label (torch.Tensor): Ground truth label map (H, W), LongTensor. Should be on self.device or moved.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get hard predictions\n",
    "        pred_hard = torch.argmax(pred.squeeze(0), dim=0) # (H, W)\n",
    "\n",
    "        # One-hot encode\n",
    "        label_onehot = F.one_hot(label.squeeze(0), num_classes=self.num_classes).permute(2, 0, 1).bool() # (C, H, W)\n",
    "        pred_onehot = F.one_hot(pred_hard, num_classes=self.num_classes).permute(2, 0, 1).bool() # (C, H, W)\n",
    "\n",
    "        # Calculate TP, FP, FN, TN per class\n",
    "        tp = (pred_onehot & label_onehot).sum(dim=(1, 2))\n",
    "        fp = (pred_onehot & ~label_onehot).sum(dim=(1, 2))\n",
    "        fn = (~pred_onehot & label_onehot).sum(dim=(1, 2))\n",
    "        tn = (~pred_onehot & ~label_onehot).sum(dim=(1, 2))\n",
    "        \n",
    "        # tp = (pred_onehot & label_onehot).sum(dim=(1, 2))\n",
    "        # fp = pred_onehot.sum(dim=(1, 2)) - tp\n",
    "        # fn = label_onehot.sum(dim=(1, 2)) - tp\n",
    "        # tn = label.numel() - fn - fp - tp\n",
    "\n",
    "        # Accumulate on CPU with float64\n",
    "        self.total_tp += tp.cpu().to(torch.float64)\n",
    "        self.total_fp += fp.cpu().to(torch.float64)\n",
    "        self.total_fn += fn.cpu().to(torch.float64)\n",
    "        self.total_tn += tn.cpu().to(torch.float64) # Accumulate TN if needed for accuracy\n",
    "\n",
    "\n",
    "    def compute_epoch_metrics(self, epsilon: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Computes the macro-averaged metrics for the accumulated epoch statistics,\n",
    "        appends them to the history lists, and returns the computed mean metrics.\n",
    "\n",
    "        Args:\n",
    "            epsilon (float): Small value to avoid division by zero.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (mean_dice, mean_iou, mean_acc) for the current epoch.\n",
    "        \"\"\"\n",
    "\n",
    "        tp = self.total_tp\n",
    "        fp = self.total_fp\n",
    "        fn = self.total_fn\n",
    "        tn = self.total_tn\n",
    "\n",
    "        per_class_iou = tp / (tp + fp + fn)\n",
    "        per_class_dice = (2 * tp) / (2 * tp + fp + fn)\n",
    "        per_class_acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "        mean_iou = per_class_iou[self.mask].mean().item()\n",
    "        mean_dice = per_class_dice[self.mask].mean().item()\n",
    "        mean_acc = per_class_acc[self.mask].mean().item()\n",
    "\n",
    "        # Append to history\n",
    "        self.epoch_mean_iou_history.append(mean_iou)\n",
    "        self.epoch_mean_dice_history.append(mean_dice)\n",
    "        self.epoch_mean_acc_history.append(mean_acc)\n",
    "\n",
    "        self.epoch_per_class_iou_history.append(per_class_iou.numpy())\n",
    "        self.epoch_per_class_dice_history.append(per_class_dice.numpy())\n",
    "        self.epoch_per_class_acc_history.append(per_class_acc.numpy())\n",
    "\n",
    "        self.last_per_class_iou = per_class_iou\n",
    "        self.last_per_class_dice = per_class_dice\n",
    "        self.last_per_class_acc = per_class_acc\n",
    "\n",
    "        return mean_dice, mean_iou, mean_acc\n",
    "    \n",
    "    def get_ignore_index(self):\n",
    "        return self.ignore_index\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_mean_dice_history(self):\n",
    "        return self.epoch_mean_dice_history\n",
    "\n",
    "    def get_mean_iou_history(self):\n",
    "        return self.epoch_mean_iou_history\n",
    "\n",
    "    def get_mean_acc_history(self):\n",
    "        return self.epoch_mean_acc_history\n",
    "    \n",
    "    def get_class_dice_history(self):\n",
    "        return self.epoch_per_class_dice_history\n",
    "\n",
    "    def get_class_iou_history(self):\n",
    "        return self.epoch_per_class_iou_history\n",
    "\n",
    "    def get_class_acc_history(self):\n",
    "        return self.epoch_per_class_acc_history\n",
    "\n",
    "    def get_last_per_class_dice(self):\n",
    "        return self.last_per_class_dice\n",
    "    \n",
    "    def get_last_per_class_iou(self):\n",
    "        return self.last_per_class_iou\n",
    "    \n",
    "    def get_last_per_class_acc(self):\n",
    "        return self.last_per_class_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T00:06:16.981852Z",
     "iopub.status.busy": "2025-04-03T00:06:16.981604Z",
     "iopub.status.idle": "2025-04-03T00:06:17.006402Z",
     "shell.execute_reply": "2025-04-03T00:06:17.005549Z",
     "shell.execute_reply.started": "2025-04-03T00:06:16.981831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Training Loop (Adapted for nnU-Net style) ---\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, accumulation_steps, device, target_size = None):\n",
    "    \"\"\"Performs one epoch of training resembling nnU-Net practices.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    processed_batches = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\")\n",
    "    for batch_idx, (X, y) in pbar:\n",
    "\n",
    "        if target_size is not None:\n",
    "            X, _ = process_batch_forward(X, target_size=target_size)\n",
    "            y, _ = process_batch_forward(y, target_size=target_size)\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        scaled_loss = loss / accumulation_steps\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        # Optimizer step after accumulation_steps batches\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            processed_batches += 1\n",
    "            pbar.set_postfix({'loss': loss.item(), 'lr': optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    avg_loss = total_loss / processed_batches if processed_batches > 0 else 0\n",
    "    print(f\"Training Avg loss (per effective batch): {avg_loss:>8f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# --- Evaluation Loop (Modified for aggregated IoU) ---\n",
    "def eval_loop(dataloader, model, loss_fn, device, target_size, agg):\n",
    "    \"\"\"\n",
    "    Evaluation loop calculating loss, aggregated Dice, and aggregated IoU.\n",
    "\n",
    "    Args:\n",
    "        dataloader: yields batches of (list[Tensor(C,H,W)], list[Tensor(H,W)])\n",
    "        model: the neural network model (on device)\n",
    "        loss_fn: the combined loss function (e.g., DiceCELoss) used for training\n",
    "        device: the torch device (cuda or cpu)\n",
    "        target_size: the size the model expects for input\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_images_processed = 0\n",
    "    total_loss = 0.0\n",
    "    num_classes = 4\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(dataloader, desc=\"Eval\"):\n",
    "\n",
    "            X, meta_list = process_batch_forward(X, target_size=target_size)\n",
    "            X = X.to(device)\n",
    "            preds = model(X) # Logits [N, C, target, target]\n",
    "\n",
    "            preds = process_batch_reverse(preds, meta_list, interpolation='bilinear')\n",
    "\n",
    "            for pred, label in zip(preds, y):\n",
    "                pred = pred.to(device) # (C,H,W)\n",
    "                label = label.to(device).long() # (H,W)\n",
    "\n",
    "                loss = loss_fn(pred.unsqueeze(0), label.unsqueeze(0)) # Add batch dimension\n",
    "                total_loss += loss.item()\n",
    "                agg.accumulate(pred, label)\n",
    "                \n",
    "                num_images_processed += 1\n",
    "\n",
    "    avg_loss = total_loss / num_images_processed\n",
    "\n",
    "    mean_dice, mean_iou, mean_acc = agg.compute_epoch_metrics()\n",
    "    per_class_iou = agg.get_last_per_class_iou()\n",
    "    ignore_index = agg.get_ignore_index()\n",
    "\n",
    "    print(f\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"  Images Processed: {num_images_processed}\")\n",
    "    print(f\"  Average Loss (Original Size): {avg_loss:>8f}\")\n",
    "    print(f\"  Ignored Class : {ignore_index}\")\n",
    "    print(f\"  Macro Avg Acc score: {mean_acc:>8f}\")\n",
    "    print(f\"  Macro Avg Dice Score: {mean_dice:>8f}\")\n",
    "    print(f\"  Mean IoU (mIoU): {mean_iou:>8f}\")\n",
    "    print(f\"  --- Per-Class IoU ---\")\n",
    "    for c in range(num_classes):\n",
    "        print(f\"    Class {c}: {per_class_iou[c].item():>8f}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    return avg_loss, mean_dice, mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T00:21:27.219588Z",
     "iopub.status.busy": "2025-04-03T00:21:27.219202Z",
     "iopub.status.idle": "2025-04-03T00:34:37.668633Z",
     "shell.execute_reply": "2025-04-03T00:34:37.667065Z",
     "shell.execute_reply.started": "2025-04-03T00:21:27.219533Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Checkpoint file not found at clip/test.pytorch. Starting training from scratch.\n",
      "\n",
      "Starting Training...\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 398/398 [03:02<00:00,  2.18it/s, loss=0.848, lr=0.00991]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Avg loss (per effective batch): -0.131471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 47/47 [00:12<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "  Images Processed: 738\n",
      "  Average Loss (Original Size): -0.360313\n",
      "  Ignored Class : 3\n",
      "  Macro Avg Acc score: 0.949696\n",
      "  Macro Avg Dice Score: 0.894777\n",
      "  Mean IoU (mIoU): 0.811788\n",
      "  --- Per-Class IoU ---\n",
      "    Class 0: 0.899327\n",
      "    Class 1: 0.749620\n",
      "    Class 2: 0.786417\n",
      "    Class 3: 0.033430\n",
      "-------------------------\n",
      "Validation Micro Dice score improved (0.894777). Saving model...\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 398/398 [03:03<00:00,  2.17it/s, loss=-0.233, lr=0.00982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Avg loss (per effective batch): -0.456696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 47/47 [00:12<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "  Images Processed: 738\n",
      "  Average Loss (Original Size): -0.368965\n",
      "  Ignored Class : 3\n",
      "  Macro Avg Acc score: 0.953437\n",
      "  Macro Avg Dice Score: 0.903795\n",
      "  Mean IoU (mIoU): 0.826104\n",
      "  --- Per-Class IoU ---\n",
      "    Class 0: 0.904047\n",
      "    Class 1: 0.787335\n",
      "    Class 2: 0.786932\n",
      "    Class 3: 0.167725\n",
      "-------------------------\n",
      "Validation Micro Dice score improved (0.903795). Saving model...\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/398 [00:04<03:08,  2.06it/s, loss=-0.529, lr=0.00982]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 402\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, EPOCHS):\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 402\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     val_loss, val_dice_micro, val_miou \u001b[38;5;241m=\u001b[39m eval_loop(val_dataloader, model, eval_settings_provider, device, \u001b[38;5;241m224\u001b[39m, agg)\n\u001b[1;32m    405\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m: agg\n\u001b[1;32m    408\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, scheduler, accumulation_steps, device, target_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m scaled_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[1;32m     22\u001b[0m scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 282\u001b[0m, in \u001b[0;36mWeightedDiceCELoss.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs, targets):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# outputs are expected to be logits [N, C, H, W]\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# targets are expected to be class indices [N, H, W] or [N, 1, H, W]\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# --- Dice Loss ---\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# WeightedMemoryEfficientDiceLoss handles softmax internally\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     dice_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdice\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# --- Cross Entropy Loss ---\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Prepare targets for CE\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m targets\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 218\u001b[0m, in \u001b[0;36mWeightedMemoryEfficientDiceLoss.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index \u001b[38;5;241m<\u001b[39m num_classes:\n\u001b[1;32m    216\u001b[0m     valid_classes_mask[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m dc_final \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Default loss if no valid classes\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_classes_mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Proceed only if there are valid classes\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     dc_valid \u001b[38;5;241m=\u001b[39m dc[valid_classes_mask] \u001b[38;5;66;03m# Dice scores for valid classes\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from typing import Optional, List # Added List\n",
    "\n",
    "def calculate_class_weights_v3(\n",
    "    label_source,\n",
    "    num_classes: int,\n",
    "    ignore_index: Optional[int] = None,\n",
    "    source_type: str = 'files',\n",
    "    unimportant_class_indices: Optional[List[int]] = None, # Indices to down-weight\n",
    "    target_unimportant_weight: float = 1.0, # Target weight for unimportant classes\n",
    "    normalize_target_sum: float = -1.0 # Normalize weights sum (-1 means num_classes)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates class weights based on inverse frequency, then adjusts weights\n",
    "    for specified unimportant classes and re-normalizes.\n",
    "\n",
    "    Args:\n",
    "        # ... (Args same as v2 except for target_unimportant_weight) ...\n",
    "        target_unimportant_weight: The desired weight value for unimportant classes\n",
    "                                   BEFORE final re-normalization. 1.0 is often neutral.\n",
    "    Returns:\n",
    "        A torch.Tensor of shape [num_classes] containing weights.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating class weights for {num_classes} classes (v3)...\")\n",
    "    if unimportant_class_indices:\n",
    "        print(f\"  Adjusting weights for classes {unimportant_class_indices} post-calculation.\")\n",
    "\n",
    "    # 1. Count all classes (Same counting loop as v2)\n",
    "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
    "    total_valid_pixels = 0\n",
    "    # --- Start of loop ---\n",
    "    # (Identical loop structure as before to get raw 'counts')\n",
    "    iterator = None\n",
    "    num_labels = 0\n",
    "    if source_type == 'files': assert isinstance(label_source, (list, tuple)); iterator = label_source; num_labels = len(label_source)\n",
    "    elif source_type == 'dataset': assert hasattr(label_source, '__getitem__') and hasattr(label_source, '__len__'); iterator = range(len(label_source)); num_labels = len(label_source)\n",
    "    else: raise ValueError(\"source_type must be either 'files' or 'dataset'\")\n",
    "    print(f\"Processing {num_labels} labels...\")\n",
    "    pbar = tqdm(iterator, total=num_labels)\n",
    "\n",
    "    for idx_or_path in pbar:\n",
    "        try:\n",
    "            label_data = None\n",
    "            # ... (Identical label loading logic as before) ...\n",
    "            if source_type == 'files':\n",
    "                path = idx_or_path; img = Image.open(path); label_data = torch.from_numpy(np.array(img)) # Example\n",
    "            elif source_type == 'dataset':\n",
    "                 _, label_data = label_source[idx_or_path]; label_data = torch.tensor(label_data) if not isinstance(label_data, torch.Tensor) else label_data\n",
    "            if label_data is None: continue\n",
    "            label_long = label_data.long().view(-1)\n",
    "            if ignore_index is not None: valid_mask = (label_long != ignore_index); label_valid = label_long[valid_mask]\n",
    "            else: label_valid = label_long\n",
    "            label_valid = torch.clamp(label_valid, 0, num_classes - 1)\n",
    "            if label_valid.numel() > 0:\n",
    "                 counts += torch.bincount(label_valid, minlength=num_classes).double()\n",
    "                 total_valid_pixels += label_valid.numel()\n",
    "\n",
    "        except Exception as e:\n",
    "             item_id = idx_or_path if source_type=='files' else f\"index {idx_or_path}\"; print(f\"\\nError processing {item_id}: {e}. Skipping.\")\n",
    "             continue\n",
    "    # --- End of loop ---\n",
    "    print(\"\\nFinished counting.\")\n",
    "    print(f\"Raw pixel counts per class: {counts.long().tolist()}\")\n",
    "    print(f\"Total valid pixels counted: {total_valid_pixels}\")\n",
    "\n",
    "    if total_valid_pixels == 0:\n",
    "        print(\"Warning: No valid pixels found. Returning equal weights.\")\n",
    "        return torch.ones(num_classes, dtype=torch.float32)\n",
    "\n",
    "    # 2. Calculate initial inverse frequency weights for ALL classes\n",
    "    frequencies = counts / total_valid_pixels\n",
    "    epsilon = 1e-6\n",
    "    inverse_frequencies = 1.0 / (frequencies + epsilon)\n",
    "\n",
    "    # Intermediate weights (no normalization yet)\n",
    "    weights = inverse_frequencies\n",
    "\n",
    "    # 3. Adjust weights for unimportant classes\n",
    "    if unimportant_class_indices:\n",
    "        for idx in unimportant_class_indices:\n",
    "            if 0 <= idx < num_classes:\n",
    "                # Find the 'base' frequency corresponding to the target weight\n",
    "                # If target is 1.0, it corresponds to average frequency's inverse\n",
    "                # For simplicity, let's just scale relative to others,\n",
    "                # or simply set it directly. Setting directly is easier.\n",
    "                # Option A: Set directly relative to the *initial* mean inverse frequency\n",
    "                # mean_inv_freq = inverse_frequencies.mean()\n",
    "                # weights[idx] = mean_inv_freq * target_unimportant_weight\n",
    "                # Option B: Just set it towards 1 (average weight after normalization)\n",
    "                # This value might need tuning\n",
    "                if target_unimportant_weight == -1:\n",
    "                    weights[idx] = min(weights) # Assign the target weight DIRECTLY.\n",
    "                else:\n",
    "                    weights[idx] = target_unimportant_weight\n",
    "                 # Note: This works well if target_unimportant_weight=1.0 and normalization\n",
    "                 # makes the average weight 1.0 later.\n",
    "\n",
    "            else:\n",
    "                warnings.warn(f\"unimportant_class_index {idx} is out of bounds.\")\n",
    "\n",
    "    # 4. Normalize the *adjusted* weights\n",
    "    target_sum = normalize_target_sum if normalize_target_sum > 0 else float(num_classes)\n",
    "    final_weights = weights / weights.sum() * target_sum\n",
    "\n",
    "    print(f\"Calculated Final Class Weights: {final_weights.tolist()}\")\n",
    "\n",
    "    return final_weights.float()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import PolynomialLR # Import PolynomialLR\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F # Needed for softmax in Dice Loss\n",
    "import warnings # Keep warnings\n",
    "from typing import Optional, List, Dict, Tuple, Callable # Keep typing\n",
    "\n",
    "# --- Configuration ---\n",
    "EPOCHS = 100\n",
    "MODEL_SAVE_DIR = \"unet\" # Changed path\n",
    "INITIAL_LR = 0.01 # Standard nnU-Net initial LR for SGD\n",
    "WEIGHT_DECAY = 3e-5 # A common weight decay value, adjust if needed\n",
    "SGD_MOMENTUM = 0.99 # nnU-Net standard momentum\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Modified Dice Loss with Class Weights ---\n",
    "class WeightedMemoryEfficientDiceLoss(nn.Module):\n",
    "    \"\"\" Version using ignore_index and supporting class weights \"\"\"\n",
    "    def __init__(self,\n",
    "                 apply_softmax: bool = True,\n",
    "                 ignore_index: Optional[int] = None,\n",
    "                 class_weights: Optional[torch.Tensor] = None, # New parameter\n",
    "                 smooth: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.apply_softmax = apply_softmax\n",
    "        self.ignore_index = ignore_index\n",
    "        self.smooth = smooth\n",
    "\n",
    "        # Store class weights, ensuring they are a Tensor if provided\n",
    "        if class_weights is not None:\n",
    "            assert isinstance(class_weights, torch.Tensor), \"class_weights must be a torch.Tensor\"\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        num_classes = x.shape[1]\n",
    "        shp_y = y.shape\n",
    "        if self.apply_softmax:\n",
    "            probs = F.softmax(x, dim=1)\n",
    "        else:\n",
    "            probs = x\n",
    "\n",
    "        # --- One-Hot Encoding and Masking ---\n",
    "        with torch.no_grad():\n",
    "            # Shape adjustments (same as before)\n",
    "            if len(shp_y) != len(probs.shape):\n",
    "                if len(shp_y) == len(probs.shape) - 1 and len(shp_y) >= 2 and shp_y == probs.shape[2:]:\n",
    "                     y = y.unsqueeze(1)\n",
    "                elif len(shp_y) == len(probs.shape) and shp_y[1] == 1: pass # ok\n",
    "                else: raise ValueError(f\"Shape mismatch: probs {probs.shape}, y {shp_y}\")\n",
    "            y_long = y.long()\n",
    "\n",
    "            # Spatial mask based on ignore_index\n",
    "            mask = None\n",
    "            # if self.ignore_index is not None:\n",
    "            #     mask = (y_long != self.ignore_index)\n",
    "\n",
    "            # Create one-hot ground truth (potentially masked)\n",
    "            if probs.shape == y.shape: # Already one-hot\n",
    "                 y_onehot = y.float()\n",
    "                 if mask is not None:\n",
    "                      y_indices_for_mask = torch.argmax(y_onehot, dim=1, keepdim=True)\n",
    "                      mask = (y_indices_for_mask != self.ignore_index)\n",
    "                      y_onehot = y_onehot * mask\n",
    "            else: # Create from index map\n",
    "                y_onehot = torch.zeros_like(probs, device=probs.device)\n",
    "                y_onehot.scatter_(1, y_long, 1)\n",
    "                if mask is not None: y_onehot = y_onehot * mask\n",
    "\n",
    "            sum_gt = y_onehot.sum(dim=(2, 3)) # Pre-calculate GT sum needed later [N, C]\n",
    "        # --- End One-Hot Encoding ---\n",
    "\n",
    "        # Apply spatial mask to probabilities before summation\n",
    "        if mask is not None:\n",
    "             probs = probs * mask\n",
    "\n",
    "        # Calculate intersection and prediction sum (still per-sample)\n",
    "        intersect_persample = (probs * y_onehot).sum(dim=(2, 3)) # Shape [N, C]\n",
    "        sum_pred_persample = probs.sum(dim=(2, 3))              # Shape [N, C]\n",
    "        sum_gt_persample = sum_gt                               # Shape [N, C]\n",
    "\n",
    "        # --- Aggregate across batch ---\n",
    "        intersect = intersect_persample.sum(0) # Shape [C]\n",
    "        sum_pred = sum_pred_persample.sum(0)   # Shape [C]\n",
    "        sum_gt = sum_gt_persample.sum(0)     # Shape [C]\n",
    "\n",
    "        # --- Calculate per-class Dice ---\n",
    "        denominator = sum_pred + sum_gt\n",
    "        dc = (2. * intersect + self.smooth) / (torch.clip(denominator + self.smooth, 1e-8)) # Shape [C]\n",
    "\n",
    "        # --- Average Dice Logic (Weighted or Unweighted) ---\n",
    "        # Mask for valid (non-ignored) classes\n",
    "        valid_classes_mask = torch.ones_like(dc, dtype=torch.bool)\n",
    "        if self.ignore_index is not None and 0 <= self.ignore_index < num_classes:\n",
    "            valid_classes_mask[self.ignore_index] = False\n",
    "\n",
    "        dc_final = torch.tensor(0.0, device=dc.device) # Default loss if no valid classes\n",
    "\n",
    "        if valid_classes_mask.sum() > 0: # Proceed only if there are valid classes\n",
    "            dc_valid = dc[valid_classes_mask] # Dice scores for valid classes\n",
    "\n",
    "            if self.class_weights is not None:\n",
    "                # Use weighted average for valid classes\n",
    "                weights = self.class_weights.to(dc_valid.device) # Ensure weights are on correct device\n",
    "                weights_valid = weights[valid_classes_mask] # Select weights for valid classes\n",
    "                # Calculate weighted mean: sum(value*weight) / sum(weight)\n",
    "                weighted_sum = (dc_valid * weights_valid).sum()\n",
    "                weight_sum = weights_valid.sum()\n",
    "                dc_final = weighted_sum / weight_sum.clamp(min=1e-8) # Avoid division by zero\n",
    "            else:\n",
    "                # Use simple mean if no weights are provided\n",
    "                dc_final = dc_valid.mean()\n",
    "\n",
    "        return -dc_final # Return negative Dice score as loss\n",
    "\n",
    "\n",
    "# --- Modified Combined Loss to use Weighted Dice and accept CE weights ---\n",
    "class WeightedDiceCELoss(nn.Module): # Renamed for clarity\n",
    "    \"\"\"Combines WeightedMemoryEfficientDiceLoss and Cross Entropy Loss with class_weights support.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dice_weight: float = 1.0,\n",
    "                 ce_weight: float = 1.0,\n",
    "                 ignore_index: Optional[int] = None,\n",
    "                 class_weights: Optional[torch.Tensor] = None, # Pass weights here\n",
    "                 smooth_dice: float = 1e-5, # Adjust smooth value as needed (e.g., 1.0 for training)\n",
    "                 ce_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ignore_index = ignore_index # Store ignore_index\n",
    "\n",
    "        # Ensure class_weights is a Tensor if provided\n",
    "        if class_weights is not None:\n",
    "             assert isinstance(class_weights, torch.Tensor), \"class_weights must be a torch.Tensor\"\n",
    "\n",
    "        # Instantiate the modified Dice loss, passing weights\n",
    "        self.dice = WeightedMemoryEfficientDiceLoss(\n",
    "            apply_softmax=True, # Dice loss usually works on probabilities\n",
    "            ignore_index=ignore_index,\n",
    "            class_weights=class_weights, # Pass weights to Dice component\n",
    "            smooth=smooth_dice\n",
    "        )\n",
    "\n",
    "        # Prepare kwargs for standard CrossEntropyLoss\n",
    "        ce_final_kwargs = ce_kwargs.copy()\n",
    "        if ignore_index is not None:\n",
    "            ce_final_kwargs['ignore_index'] = ignore_index\n",
    "        if class_weights is not None:\n",
    "            # Pass the weights tensor to CE's 'weight' parameter\n",
    "            ce_final_kwargs['weight'] = class_weights\n",
    "            # Note: CE will handle moving the weights tensor to the correct device internally\n",
    "\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(**ce_final_kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # outputs are expected to be logits [N, C, H, W]\n",
    "        # targets are expected to be class indices [N, H, W] or [N, 1, H, W]\n",
    "\n",
    "        # --- Dice Loss ---\n",
    "        # WeightedMemoryEfficientDiceLoss handles softmax internally\n",
    "        dice_loss = self.dice(outputs, targets)\n",
    "\n",
    "        # --- Cross Entropy Loss ---\n",
    "        # Prepare targets for CE\n",
    "        if targets.ndim == 4 and targets.shape[1] == 1:\n",
    "             targets_ce = targets.squeeze(1).long()\n",
    "        elif targets.ndim == 3:\n",
    "             targets_ce = targets.long() # Assuming [N, H, W]\n",
    "        else:\n",
    "             # Added more specific error message for common cases\n",
    "             if targets.ndim == outputs.ndim and targets.shape[1] != 1:\n",
    "                 raise ValueError(f\"Target shape {targets.shape} has multiple channels but expected class indices [N, H, W] or [N, 1, H, W] for CE.\")\n",
    "             else:\n",
    "                 raise ValueError(f\"Unsupported target shape {targets.shape} for CE. Expected [N, H, W] or [N, 1, H, W].\")\n",
    "\n",
    "        # Weights are handled internally by nn.CrossEntropyLoss via its 'weight' parameter\n",
    "        ce_loss = self.cross_entropy(outputs, targets_ce)\n",
    "        \n",
    "        # --- Combine ---\n",
    "        combined_loss = (self.dice_weight * dice_loss) + (self.ce_weight * ce_loss)\n",
    "        return combined_loss\n",
    "\n",
    "class_weight = [0.30711034803008996, 1.5412496145750956, 1.8445296893647247, 0.30711034803008996]\n",
    "# class_weight = [1, 1, 1, 1]\n",
    "class_weight = Tensor(class_weight)\n",
    "# class_weight = calculate_class_weights_v3(training_data, 4, None, \"dataset\", [3], 0)\n",
    "\n",
    "accumulation_steps = target_batch_size // batch_size\n",
    "\n",
    "# --- Define Model, Loss, Optimizer, Scheduler ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "model = ClipUNet().to(device) # Assuming unet() exists and returns your U-Net model\n",
    "\n",
    "# --- Define Loss, Optimizer, Scheduler ---\n",
    "# Configure ignore_index (e.g., 3 to ignore class 3, or 255 if used in labels, None otherwise)\n",
    "EVAL_IGNORE_INDEX = 3 # Example: ignore class 3 during evaluation metric calculation\n",
    "TRAIN_IGNORE_INDEX = None  # Example: train on all classes (0,1,2,3)\n",
    "\n",
    "class_weight = class_weight.to(device)\n",
    "loss_fn = WeightedDiceCELoss(ignore_index=TRAIN_IGNORE_INDEX, smooth_dice=1, class_weights=class_weight) # Training loss\n",
    "# Evaluation loss object used inside eval loop only to get settings like ignore_index\n",
    "# It is NOT used to calculate the loss score reported for eval (that uses training loss object)\n",
    "# But we pass it to eval_loop so it knows which index to ignore for metric calc if needed\n",
    "eval_settings_provider = WeightedDiceCELoss(ignore_index=EVAL_IGNORE_INDEX, class_weights=class_weight)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=SGD_MOMENTUM,\n",
    "                      weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "\n",
    "steps_per_epoch = (len(train_dataloader) // accumulation_steps) + (1 if len(train_dataloader) % accumulation_steps != 0 else 0)\n",
    "total_iters = steps_per_epoch * EPOCHS\n",
    "scheduler = PolynomialLR(optimizer, total_iters=total_iters, power=0.9)\n",
    "NUM_CLASSES = 4\n",
    "processor = CLIPImageProcessor.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "agg = MetricsHistory(NUM_CLASSES, EVAL_IGNORE_INDEX)\n",
    "\n",
    "start_epoch = 0\n",
    "best_dev_dice = -np.inf # Track best Dice score\n",
    "best_dev_miou = -np.inf # Track best mIoU\n",
    "best_dev_loss = np.inf # Track loss corresponding to best metric\n",
    "\n",
    "MODEL_NAME = \"test.pytorch\"\n",
    "MODEL_SAVE_DIR = \"clip\"\n",
    "checkpoint_path = os.path.join(MODEL_SAVE_DIR, MODEL_NAME)\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "if os.path.isfile(f\"{MODEL_SAVE_DIR}/{MODEL_NAME}\"):\n",
    "    print(f\"Loading checkpoint from: {MODEL_SAVE_DIR}/{MODEL_NAME}\")\n",
    "    # Load the checkpoint dictionary; move tensors to the correct device\n",
    "    checkpoint = torch.load(f\"{MODEL_SAVE_DIR}/{MODEL_NAME}\", map_location=device)\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(\" -> Model state loaded.\")\n",
    "\n",
    "    # Load optimizer state\n",
    "    try:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\" -> Optimizer state loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Warning: Could not load optimizer state: {e}. Optimizer will start from scratch.\")\n",
    "\n",
    "    # Load scheduler state\n",
    "    try:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        print(\" -> Scheduler state loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Warning: Could not load scheduler state: {e}. Scheduler will start from scratch.\")\n",
    "\n",
    "    try:\n",
    "        agg = checkpoint.get(\"history\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> No metric history saved\")\n",
    "        agg = MetricsHistory(NUM_CLASSES, EVAL_IGNORE_INDEX)\n",
    "\n",
    "    # Load training metadata\n",
    "    start_epoch = checkpoint.get(\"epoch\", 0) # Load last completed epoch, training continues from next one\n",
    "    best_dev_dice = checkpoint.get(\"best_dev_dice\", -np.inf)\n",
    "    best_dev_miou = checkpoint.get(\"best_dev_miou\", -np.inf)\n",
    "    best_dev_loss = checkpoint.get(\"best_dev_loss\", np.inf)\n",
    "\n",
    "    print(f\" -> Resuming training from epoch {start_epoch + 1}\")\n",
    "    print(f\" -> Loaded best metrics: Dice={best_dev_dice:.6f}, mIoU={best_dev_miou:.6f}, Loss={best_dev_loss:.6f}\")\n",
    "    loaded_notes = checkpoint.get(\"notes\", \"N/A\")\n",
    "    print(f\" -> Notes from checkpoint: {loaded_notes}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Checkpoint file not found at {MODEL_SAVE_DIR}/{MODEL_NAME}. Starting training from scratch.\")\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "print(\"\\nStarting Training...\")\n",
    "for t in range(start_epoch, EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, accumulation_steps, device, 224)\n",
    "    val_loss, val_dice_micro, val_miou = eval_loop(val_dataloader, model, eval_settings_provider, device, 224, agg)\n",
    "\n",
    "    metrics = {\n",
    "        \"epoch\": t + 1,\n",
    "        \"history\": agg\n",
    "    }\n",
    "    torch.save(metrics, f\"{MODEL_SAVE_DIR}/metrics_{MODEL_NAME}\")\n",
    "\n",
    "    # Save model based on validation MICRO DICE score improvement\n",
    "    # Could also choose mIoU validation by changing 'val_dice_micro > best_dev_dice'\n",
    "    if val_dice_micro > best_dev_dice:\n",
    "        best_dev_dice = val_dice_micro\n",
    "        best_dev_miou = val_miou # Save corresponding mIoU\n",
    "        best_dev_loss = val_loss # Save corresponding loss\n",
    "        print(f\"Validation Micro Dice score improved ({best_dev_dice:.6f}). Saving model...\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": t + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"best_dev_dice\": best_dev_dice,\n",
    "            \"best_dev_miou\": best_dev_miou,\n",
    "            \"best_dev_loss\": best_dev_loss,\n",
    "            \"history\": agg,\n",
    "            \"notes\": f\"Model saved based on best Micro Dice. Ignored index for metric: {EVAL_IGNORE_INDEX}\"\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    else:\n",
    "        print(f\"Validation Micro Dice score did not improve from {best_dev_dice:.6f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Finished! ---\")\n",
    "print(f\"Best validation Micro Dice score achieved: {best_dev_dice:.6f}\")\n",
    "print(f\"Corresponding validation mIoU: {best_dev_miou:.6f}\")\n",
    "print(f\"Corresponding validation loss: {best_dev_loss:.6f}\")\n",
    "print(f\"Best model saved to: {os.path.join(MODEL_SAVE_DIR, 'unet_best_dice.pytorch')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T00:07:25.262683Z",
     "iopub.status.busy": "2025-04-03T00:07:25.262370Z",
     "iopub.status.idle": "2025-04-03T00:09:36.725584Z",
     "shell.execute_reply": "2025-04-03T00:09:36.724491Z",
     "shell.execute_reply.started": "2025-04-03T00:07:25.262658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   8%|▊         | 18/231 [00:05<01:05,  3.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# new_dev_loss = eval_loop(test_dataloader, model, loss_fn, device, 224)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# images, labels = next(iter(test_dataloader))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# plt.imshow(labels[0].permute(1,2,0).cpu().numpy())\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m agg \u001b[38;5;241m=\u001b[39m MetricsHistory(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m new_dev_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 68\u001b[0m, in \u001b[0;36meval_loop\u001b[0;34m(dataloader, model, loss_fn, device, target_size, agg)\u001b[0m\n\u001b[1;32m     65\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (C,H,W)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;66;03m# (H,W)\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     69\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     70\u001b[0m agg\u001b[38;5;241m.\u001b[39maccumulate(pred, label)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 282\u001b[0m, in \u001b[0;36mWeightedDiceCELoss.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs, targets):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# outputs are expected to be logits [N, C, H, W]\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# targets are expected to be class indices [N, H, W] or [N, 1, H, W]\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# --- Dice Loss ---\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# WeightedMemoryEfficientDiceLoss handles softmax internally\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     dice_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdice\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# --- Cross Entropy Loss ---\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Prepare targets for CE\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m targets\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 191\u001b[0m, in \u001b[0;36mWeightedMemoryEfficientDiceLoss.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    188\u001b[0m         y_onehot\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, y_long, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: y_onehot \u001b[38;5;241m=\u001b[39m y_onehot \u001b[38;5;241m*\u001b[39m mask\n\u001b[0;32m--> 191\u001b[0m     sum_gt \u001b[38;5;241m=\u001b[39m \u001b[43my_onehot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pre-calculate GT sum needed later [N, C]\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# --- End One-Hot Encoding ---\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Apply spatial mask to probabilities before summation\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "model = ClipUNet().to(device)\n",
    "class_weight = [0.30711034803008996, 1.5412496145750956, 1.8445296893647247, 0.30711034803008996]\n",
    "class_weight = Tensor(class_weight)\n",
    "class_weight = class_weight.to(device)\n",
    "loss_fn = WeightedDiceCELoss(ignore_index=3, class_weights=class_weight)\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=SGD_MOMENTUM,\n",
    "#                       weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "\n",
    "# total_iters = (len(train_dataloader) // accumulation_steps) * EPOCHS\n",
    "# scheduler = PolynomialLR(optimizer, total_iters=total_iters, power=0.9)\n",
    "\n",
    "checkpoint = torch.load(\"clip/clip_weighted_no_mask_loss.pytorch\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "# scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# new_dev_loss = eval_loop(test_dataloader, model, loss_fn, device, 224)\n",
    "\n",
    "# images, labels = next(iter(test_dataloader))\n",
    "# images, meta = process_batch_forward(images, 224)\n",
    "\n",
    "# pred = model(images.to(device))\n",
    "\n",
    "# images = process_batch_reverse(pred, meta)\n",
    "\n",
    "# print(images[0].shape)\n",
    "\n",
    "# plt.imshow(pred[0].argmax(dim=0).cpu().numpy())\n",
    "# plt.show()\n",
    "# plt.imshow(images[0].argmax(dim=0).cpu().numpy())\n",
    "# plt.show()\n",
    "# plt.imshow(labels[0].permute(1,2,0).cpu().numpy())\n",
    "# plt.show()\n",
    "agg = MetricsHistory(4, 3)\n",
    "new_dev_loss = eval_loop(test_dataloader, model, loss_fn, device, 224, agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:57:14.239148Z",
     "iopub.status.busy": "2025-04-02T18:57:14.238823Z",
     "iopub.status.idle": "2025-04-02T18:57:14.938300Z",
     "shell.execute_reply": "2025-04-02T18:57:14.937437Z",
     "shell.execute_reply.started": "2025-04-02T18:57:14.239125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_dataloader))\n",
    "images, meta = process_batch_forward(images, 224)\n",
    "\n",
    "pred = model(images.to(device))\n",
    "\n",
    "images = process_batch_reverse(pred, meta)\n",
    "\n",
    "print(images[0].shape)\n",
    "\n",
    "plt.imshow(pred[0].argmax(dim=0).cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(images[0].argmax(dim=0).cpu().numpy())\n",
    "plt.show()\n",
    "plt.imshow(labels[0].permute(1,2,0).cpu().numpy())\n",
    "plt.show()\n",
    "# # new_dev_loss = eval_loop(test_dataloader, model, loss_fn, device, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T00:13:15.760396Z",
     "iopub.status.busy": "2025-04-03T00:13:15.759980Z",
     "iopub.status.idle": "2025-04-03T00:13:15.781379Z",
     "shell.execute_reply": "2025-04-03T00:13:15.780420Z",
     "shell.execute_reply.started": "2025-04-03T00:13:15.760365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Output class definition might vary slightly across transformers versions\n",
    "# Try importing potential base classes if direct attribute access fails later\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "from transformers import CLIPVisionModel, CLIPVisionConfig, CLIPImageProcessor\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "PRETRAINED_MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "IMG_SIZE = 224\n",
    "# BOTTLENECK_LAYER_INDEX is no longer needed for this encoder version\n",
    "\n",
    "# --- Modified CLIP ViT Encoder (Uses last_hidden_state) ---\n",
    "class ClipViTEncoderNoSkips(nn.Module):\n",
    "    def __init__(self, model_name=PRETRAINED_MODEL_NAME, freeze_encoder=True): # Removed bottleneck_index\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing ClipViTEncoderNoSkips (Using last_hidden_state) ---\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.config = CLIPVisionConfig.from_pretrained(model_name)\n",
    "        self.clip_vit = CLIPVisionModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for param in self.clip_vit.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.patch_size = self.config.patch_size\n",
    "        if self.config.image_size != IMG_SIZE:\n",
    "             warnings.warn(f\"Model config image size {self.config.image_size} differs from specified IMG_SIZE {IMG_SIZE}.\")\n",
    "        self.grid_size = self.config.image_size // self.patch_size\n",
    "        self.hidden_dim = self.config.hidden_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "\n",
    "        # print(f\"  Model: {model_name}\")\n",
    "        # print(f\"  Grid Size (G): {self.grid_size}x{self.grid_size}\")\n",
    "        # print(f\"  Hidden Dim (D_vit): {self.hidden_dim}\")\n",
    "        # print(f\"  Will extract bottleneck from standard 'last_hidden_state' output.\")\n",
    "        # print(f\"  Encoder frozen: {freeze_encoder}\")\n",
    "        # print(\"--- ClipViTEncoderNoSkips Initialized ---\\n\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"\\n--- ClipViTEncoderNoSkips Forward ---\")\n",
    "        # print(f\"  Input shape: {x.shape}\")\n",
    "        if x.shape[2] != self.config.image_size or x.shape[3] != self.config.image_size:\n",
    "             warnings.warn(f\"Input image size mismatch.\")\n",
    "\n",
    "        # Pass image through CLIP ViT, NO need for output_hidden_states=True\n",
    "        outputs = self.clip_vit(pixel_values=x)\n",
    "\n",
    "        # Access the output of the final layer directly\n",
    "        # The output object type might vary, usually has last_hidden_state\n",
    "        if isinstance(outputs, BaseModelOutputWithPooling) or hasattr(outputs, 'last_hidden_state'):\n",
    "             last_hidden_state = outputs.last_hidden_state\n",
    "            #  print(f\"  Accessed 'last_hidden_state', shape: {last_hidden_state.shape}\") # Should be (B, N_p+1, D_vit)\n",
    "        else:\n",
    "             # Fallback or error if the output structure is unexpected\n",
    "             # Sometimes the raw tuple output is returned if BaseClass isn't used\n",
    "             if isinstance(outputs, tuple):\n",
    "                  last_hidden_state = outputs[0] # Usually the first element\n",
    "                  warnings.warn(\"Accessed model output via tuple indexing (index 0). Assumed it's last_hidden_state.\")\n",
    "                #   print(f\"  Accessed output via tuple index 0, shape: {last_hidden_state.shape}\")\n",
    "             else:\n",
    "                  raise TypeError(f\"Unexpected output type from CLIPVisionModel: {type(outputs)}. Cannot extract last_hidden_state.\")\n",
    "\n",
    "\n",
    "        # --- Reshape Final Layer Output (Bottleneck) ---\n",
    "        # print(f\"    Original last_hidden_state shape: {last_hidden_state.shape}\")\n",
    "\n",
    "        patch_embeddings = last_hidden_state[:, 1:, :] # Remove CLS token\n",
    "        # print(f\"    Shape after removing CLS token: {patch_embeddings.shape}\")\n",
    "\n",
    "        if patch_embeddings.shape[1] != self.num_patches:\n",
    "             current_num_patches = patch_embeddings.shape[1]\n",
    "             current_grid_size = int(sqrt(current_num_patches))\n",
    "             if current_grid_size * current_grid_size != current_num_patches:\n",
    "                 raise ValueError(f\"Cannot reshape patch embeddings.\")\n",
    "             warnings.warn(f\"Patch count mismatch. Reshaping to {current_grid_size}x{current_grid_size}.\")\n",
    "             grid_h, grid_w = current_grid_size, current_grid_size\n",
    "        else:\n",
    "             grid_h, grid_w = self.grid_size, self.grid_size\n",
    "\n",
    "        bottleneck_features = patch_embeddings.reshape(\n",
    "            x.shape[0], grid_h, grid_w, self.hidden_dim\n",
    "        ).permute(0, 3, 1, 2).contiguous()\n",
    "        # print(f\"    Bottleneck feature shape (B, D_vit, G, G): {bottleneck_features.shape}\")\n",
    "\n",
    "        # print(\"--- ClipViTEncoderNoSkips Forward End ---\")\n",
    "        return bottleneck_features # Only return the bottleneck\n",
    "\n",
    "# --- Modified U-Net Decoder Block (No Skip Connection Logic) ---\n",
    "# (Keep the DecoderBlockNoSkip from the previous version - no changes needed)\n",
    "class DecoderBlockNoSkip(nn.Module):\n",
    "    def __init__(self, block_index, in_channels_upsample, out_channels): # Removed skip-related args\n",
    "        super().__init__()\n",
    "        self.block_index = block_index\n",
    "        # print(f\"    Initializing DecoderBlockNoSkip {self.block_index}: Upsample_in={in_channels_upsample}, Out={out_channels}\")\n",
    "        self.upsample_out_channels = in_channels_upsample // 2\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels_upsample, self.upsample_out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "        conv_in_channels = self.upsample_out_channels\n",
    "        # print(f\"      Block {self.block_index}: Input channels to conv_block = {conv_in_channels}\")\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(conv_in_channels, out_channels, kernel_size=3, padding=1, bias=False), # Adjusted input channels here\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_upsample): # Only takes x_upsample as input\n",
    "        # print(f\"\\n  --- DecoderBlockNoSkip {self.block_index} Forward ---\")\n",
    "        # print(f\"    Input x_upsample shape: {x_upsample.shape}\")\n",
    "        x_upsample = self.upsample(x_upsample)\n",
    "        # print(f\"    Shape after upsample: {x_upsample.shape}\")\n",
    "        x = self.conv_block(x_upsample) # Pass upsampled output directly to conv block\n",
    "        # print(f\"    Shape after conv_block (Output): {x.shape}\")\n",
    "        # print(f\"  --- DecoderBlockNoSkip {self.block_index} Forward End ---\")\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Modified U-Net Decoder (No Skip Connection Logic) ---\n",
    "# (Keep the UNetDecoderNoSkips from the previous version - no changes needed)\n",
    "class UNetDecoderNoSkips(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, encoder_grid_size, decoder_channels):\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing UNetDecoderNoSkips ---\")\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.encoder_grid_size = encoder_grid_size\n",
    "        self.decoder_channels = decoder_channels\n",
    "        num_blocks = len(decoder_channels)\n",
    "\n",
    "        in_channels_upsample = encoder_hidden_dim # Starts with bottleneck channels\n",
    "        self.blocks = nn.ModuleList()\n",
    "        current_spatial_size = encoder_grid_size\n",
    "        # print(f\"  Creating {num_blocks} DecoderBlockNoSkip(s):\")\n",
    "        for i in range(num_blocks):\n",
    "            out_ch = decoder_channels[i]\n",
    "            block = DecoderBlockNoSkip( # Use the modified block\n",
    "                block_index=i,\n",
    "                in_channels_upsample=in_channels_upsample,\n",
    "                out_channels=out_ch,\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "            in_channels_upsample = out_ch # Next block takes output channels from this block\n",
    "            current_spatial_size *= 2 # Update expected spatial size\n",
    "        # print(\"--- UNetDecoderNoSkips Initialized ---\\n\")\n",
    "\n",
    "    def forward(self, bottleneck_features): # Only takes bottleneck_features\n",
    "        # print(\"\\n--- UNetDecoderNoSkips Forward ---\")\n",
    "        # print(f\"  Input bottleneck_features shape: {bottleneck_features.shape}\")\n",
    "        x = bottleneck_features\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            # print(f\"  Processing Decoder Block {i}\")\n",
    "            x = block(x) # Pass output of previous block/bottleneck directly\n",
    "        # print(f\"  Final output shape from UNetDecoderNoSkips: {x.shape}\")\n",
    "        # print(\"--- UNetDecoderNoSkips Forward End ---\")\n",
    "        return x\n",
    "\n",
    "# --- Combined CLIP-U-Net Model (No Skips - uses modified encoder) ---\n",
    "# (Keep the ClipUNetNoSkips from the previous version - no changes needed,\n",
    "# as it already uses the encoder named ClipViTEncoderNoSkips)\n",
    "class ClipUNetNoSkips(nn.Module):\n",
    "    def __init__(self, num_classes=4, decoder_channels=[512, 256, 128, 64], freeze_encoder=True):\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing ClipUNetNoSkips ---\")\n",
    "        # Uses the newly defined ClipViTEncoderNoSkips which accesses last_hidden_state\n",
    "        self.encoder = ClipViTEncoderNoSkips(freeze_encoder=freeze_encoder)\n",
    "        self.decoder = UNetDecoderNoSkips( # Uses the decoder without skip logic\n",
    "            encoder_hidden_dim=self.encoder.hidden_dim,\n",
    "            encoder_grid_size=self.encoder.grid_size,\n",
    "            decoder_channels=decoder_channels\n",
    "        )\n",
    "        last_decoder_channel = decoder_channels[-1] if decoder_channels else self.encoder.hidden_dim\n",
    "        self.final_conv = nn.Conv2d(last_decoder_channel, num_classes, kernel_size=1)\n",
    "        final_decoder_size = self.encoder.grid_size * (2**len(decoder_channels))\n",
    "        if final_decoder_size != IMG_SIZE:\n",
    "             self.final_upsample = nn.Upsample(size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "             self.final_upsample = nn.Identity()\n",
    "        # print(\"--- ClipUNetNoSkips Initialized ---\\n\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"\\n\\n========== ClipUNetNoSkips Forward Pass Start ==========\")\n",
    "        # print(f\"Overall Input shape: {x.shape}\")\n",
    "        bottleneck = self.encoder(x) # Encoder now uses last_hidden_state implicitly\n",
    "        # print(\"\\n--- Back in ClipUNetNoSkips Forward (after Encoder) ---\")\n",
    "        # print(f\"  Encoder returned bottleneck shape: {bottleneck.shape}\")\n",
    "        decoder_output = self.decoder(bottleneck) # Decoder takes only bottleneck\n",
    "        # print(\"\\n--- Back in ClipUNetNoSkips Forward (after Decoder) ---\")\n",
    "        # print(f\"  Decoder returned output shape: {decoder_output.shape}\")\n",
    "        output = self.final_conv(decoder_output)\n",
    "        # print(\"\\n--- Back in ClipUNetNoSkips Forward (after Final Conv) ---\")\n",
    "        # print(f\"  Shape after final_conv: {output.shape}\")\n",
    "        output = self.final_upsample(output)\n",
    "        # print(\"\\n--- Back in ClipUNetNoSkips Forward (after Final Upsample) ---\")\n",
    "        # print(f\"  Shape after final_upsample: {output.shape}\")\n",
    "        # print(\"========== ClipUNetNoSkips Forward Pass End ==========\\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6998841,
     "sourceId": 11208617,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7025547,
     "sourceId": 11244242,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7025550,
     "sourceId": 11244247,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
