{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.665275Z",
     "start_time": "2025-03-11T16:48:54.644297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "from utils import *\n",
    "\n",
    "batch_size = 2\n",
    "training_data = dataset(\"../atrain/color\", \"../atrain/label\", target_transform=target_remap())\n",
    "validation_data = dataset(\"../val/color\", \"../val/label\", target_transform=target_remap())\n",
    "#test_data = dataset(\"rtest/color\", \"rtest/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True,collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a12c4",
   "metadata": {},
   "source": [
    "### Old Autoencoder (not tranformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fb997cd390592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.687341Z",
     "start_time": "2025-03-11T16:48:54.681537Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, din):\n",
    "        super().__init__()\n",
    "        self.encoderPart1 = EncoderPart(din, 64)\n",
    "        self.encoderPart2 = EncoderPart(64, 32)\n",
    "        self.encoderPart3 = EncoderPart(32, 16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoderPart1(x)\n",
    "        x = self.encoderPart2(x)\n",
    "        x = self.encoderPart3(x)\n",
    "        return x\n",
    "\n",
    "class DecoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dout):\n",
    "        super().__init__()\n",
    "        self.decoderPart1 = DecoderPart(16, 16)\n",
    "        self.decoderPart2 = DecoderPart(16, 32)\n",
    "        self.decoderPart3 = DecoderPart(32, 64)\n",
    "        self.decoderOut = nn.Sequential(\n",
    "            nn.Conv2d(64, dout, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoderPart1(x)\n",
    "        x = self.decoderPart2(x)\n",
    "        x = self.decoderPart3(x)\n",
    "        x = self.decoderOut(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(din)\n",
    "        self.decoder = Decoder(dout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f918ffff43a85a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.886132Z",
     "start_time": "2025-03-11T16:48:54.695476Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = Autoencoder(3, 3)\n",
    "image, label = training_data[0]\n",
    "print(image)\n",
    "print(image.size())\n",
    "pred = model(image)\n",
    "print(pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34123c9",
   "metadata": {},
   "source": [
    "### Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converts image into patch embeddings\"\"\"\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # Number of patches\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_dim = in_channels * patch_size * patch_size  # Flattened patch size\n",
    "        self.projection = nn.Linear(self.patch_dim, embed_dim)  # Linear projection to embedding size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Batch, Channels, Height, Width\n",
    "        # Convert image (B, C, H, W) into patches:  \n",
    "        # → (B, num_patches_height, num_patches_width, patch_size, patch_size, C):\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size).permute(0, 2, 4, 3, 5, 1)\n",
    "        # → (B, num_patches, patch_dim):\n",
    "        x = x.reshape(B, self.num_patches, -1)  # Flatten patches\n",
    "        return self.projection(x)  # Project into embedding space\n",
    "\n",
    "\n",
    "\n",
    "#Uses Transformer Encoder Layers (self-attention + feedforward network).\n",
    "#Positional Encoding ensures order information is preserved.\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, embed_dim=512, num_heads=4, ff_dim=1024, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))  # Positional encoding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embedding[:, :x.shape[1], :]    # Add position information\n",
    "        return self.encoder_layers(x)\n",
    "\n",
    "\n",
    "\n",
    "# Uses cross-attention to refine the latent representation.\n",
    "class TransformerDecoder(nn.Module):    \n",
    "    def __init__(self, embed_dim=512, num_heads=4, ff_dim=1024, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        return self.decoder_layers(x, memory)  # Keep the output in (B, P, embed_dim)\n",
    "\n",
    "\n",
    "class ReconstructionTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6):  #ff_dim=2*embed_dim\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.encoder = TransformerEncoder(img_size, self.patch_embed.patch_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        self.decoder = TransformerDecoder(embed_dim, num_heads, ff_dim, num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, self.patch_embed.patch_dim)    # Convert embeddings back to patch values\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x)\n",
    "        encoded = self.encoder(patches)\n",
    "        decoded = self.decoder(encoded, encoded)\n",
    "        reconstructed_patches = self.fc_out(decoded)\n",
    "        B, P, D = reconstructed_patches.shape  # (batch, num_patches, patch_dim)\n",
    "        img_size = int(P ** 0.5) * self.patch_embed.patch_size\n",
    "        return reconstructed_patches.view(B, 3, img_size, img_size)  # Reshape back to image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SegmentationTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding to break image into patches\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        \n",
    "        # Transformer encoder for feature extraction\n",
    "        self.encoder = TransformerEncoder(img_size, self.patch_embed.patch_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        \n",
    "        # Transformer decoder to output class logits (for segmentation)\n",
    "        self.decoder = TransformerDecoder(embed_dim, num_heads, ff_dim, num_layers)\n",
    "        \n",
    "        # Segmentation head to output logits for 4 classes (background, boundary, cat, dog)\n",
    "        self.segmentation_head = nn.Conv2d(embed_dim, num_classes, kernel_size=1)  # Output 4 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"[Segmentation] Forward pass\")\n",
    "        # Embed the input image into patches\n",
    "        patches = self.patch_embed(x)\n",
    "        encoded = self.encoder(patches)\n",
    "        decoded = self.decoder(encoded, encoded)  # (B, num_patches, embedding_dim)\n",
    "   \n",
    "        # Reshape the decoded output to match image dimensions\n",
    "        B, P, D = decoded.shape\n",
    "        img_size = int(P ** 0.5)    # Calculate the original image size: \n",
    "                                    # int(P ** 0.5) = sqrt(num_patches) = num patches along each side\n",
    "        # Reshape the decoded output into a 2D spatial representation\n",
    "        decoded = decoded.view(B, D, img_size, img_size)  # Reshape to (B, H, W, D) and change to (B, D, H, W) for Conv2d\n",
    "\n",
    "        # Segmentation head: output logits for 4 classes (background, boundary, cat, dog)\n",
    "        segmentation_logits = self.segmentation_head(decoded)  # (B, num_classes, H, W)\n",
    "\n",
    "        # Upsample to original image size\n",
    "        segmentation_logits_upsampled = F.interpolate(segmentation_logits, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return segmentation_logits_upsampled  # Output segmentation mask (logits for each class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21005fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from tqdm import tqdm\n",
    "import torch\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model_1 = ReconstructionTransformerAutoencoder().to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model_1.parameters())\n",
    "print(f\"Total parameters in model_1 (SegmentationTransformerAutoencoder): {pytorch_total_params}\")\n",
    "model_2 = SegmentationTransformerAutoencoder().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() # or MSE for reconstruction\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "losses = []\n",
    "target_batch_size = 1  #TODO before submission\n",
    "batch_size = 1          #TODO before submission\n",
    "to_print = True\n",
    "for model in [model_2, model_1]:\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for batch, (X, y) in enumerate(tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")):\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        print(f\"pred shape: {pred.shape}\")\n",
    "        print(f\"y shape: {y.shape}\")    #comment for reconstruction\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        # Compute loss\n",
    "        #loss = loss_fn(pred, X) # for reconstruction\n",
    "        loss = loss_fn(pred, y.squeeze(1)) #for segmentation\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Ensure gradients are reset to 0 for new batch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "           \n",
    "        plt.imshow(X[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.show()\n",
    "        plt.imshow(pred[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.show()\n",
    "    print(f\"losses: {losses}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d11d48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainReconstruction(dataloader, model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    target_batch_size = 64  #TODO before submission\n",
    "    #batch_size = 16          #TODO before submission\n",
    "    to_print = True\n",
    "    for batch, (X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "        X = X.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, X)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if batch % (target_batch_size/batch_size) == 0:\n",
    "            # Ensure gradients are reset to 0 for new batch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # if to_print:\n",
    "            #     print(f\"memory: {torch.cuda.device_memory_used()}\")\n",
    "            #     to_print = False\n",
    "        \n",
    "    return np.mean(losses)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326529982cdc3fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.897473Z",
     "start_time": "2025-03-11T16:48:54.894249Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainSegmentation(dataloader, model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    target_batch_size = 64  #TODO before submission\n",
    "    batch_size = 16          #TODO before submission\n",
    "    to_print = True\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, y.squeeze(1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if batch % (target_batch_size/batch_size) == 0:\n",
    "            # Ensure gradients are reset to 0 for new batch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # if to_print:\n",
    "            #     print(f\"memory: {torch.cuda.device_memory_used()}\")\n",
    "            #     to_print = False\n",
    "        \n",
    "    return np.mean(losses)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff920a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a079fe9e2df3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.907981Z",
     "start_time": "2025-03-11T16:48:54.905263Z"
    }
   },
   "outputs": [],
   "source": [
    "target_size = 512\n",
    "interpolation = 'bilinear'\n",
    "\n",
    "def evalReconstruction(dataloader, model, loss_fn):\n",
    "    #print(\"Evaluating reconstruction\")\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, (original_X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Evaluation\")):\n",
    "            resized_X, meta_list = process_batch_forward(original_X, target_size=target_size)   # resize X for network\n",
    "            resized_X = resized_X.to(device)\n",
    "            \n",
    "            # Compute prediction\n",
    "            pred = model(resized_X)\n",
    "\n",
    "            pred = process_batch_reverse(pred, meta_list, interpolation=interpolation)\n",
    "\n",
    "            for p, label in zip(pred, original_X):\n",
    "                # Move individual prediction and label to the device\n",
    "                p = p.to(device).unsqueeze(0)  # Add batch dimension\n",
    "                label = label.to(device).unsqueeze(0)  # Add batch dimension and ensure type is long\n",
    "\n",
    "                if label.shape[1] == 4 and label.ndim == 4:  \n",
    "                    print(f\"    Converting original image from RGBA to RGB\")\n",
    "                    label = label[:, :3, :, :] # Keep only the first 3 channels (R, G, B)\n",
    "\n",
    "                #print(f\"p shape: {p.shape}\")\n",
    "                #print(f\"label shape: {label.shape}\")\n",
    "                # print(p.size(), flush=True)\n",
    "                # print(label.size())\n",
    "                # Calculate the loss for the current pair\n",
    "                loss = loss_fn(p, label.squeeze(1))\n",
    "                total_loss += loss.item()\n",
    "            # Compute loss\n",
    "            # loss = loss_fn(pred, X)\n",
    "            # losses.append(loss.item())\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c051de",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 512\n",
    "interpolation = 'bilinear'\n",
    "\n",
    "def evalSegmentation(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Evaluation\")):\n",
    "            X, meta_list = process_batch_forward(X, target_size=target_size)\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            if X.shape[1] == 4 and X.ndim == 4:  \n",
    "                #print(f\"    Converting RGBA image to RGB image\")\n",
    "                X = X[:, :3, :, :] # Keep only the first 3 channels (R, G, B)\n",
    "\n",
    "            # Compute prediction\n",
    "            pred = model(X)\n",
    "\n",
    "            pred = process_batch_reverse(pred, meta_list, interpolation=interpolation)\n",
    "\n",
    "            for p, label in zip(pred, y):\n",
    "                \n",
    "                # Move individual prediction and label to the device\n",
    "                p = p.to(device).unsqueeze(0)  # Add batch dimension\n",
    "                label = label.to(device).unsqueeze(0)  # Add batch dimension and ensure type is long\n",
    "                \n",
    "                # print(p.size(), flush=True)\n",
    "                # print(label.size())\n",
    "                # Calculate the loss for the current pair\n",
    "                loss = loss_fn(p, label.squeeze(1))\n",
    "                total_loss += loss.item()\n",
    "            # Compute loss\n",
    "            # loss = loss_fn(pred, X)\n",
    "            # losses.append(loss.item())\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fa696",
   "metadata": {},
   "source": [
    "### Run Reconstruction Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44f2b0ab103c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:52:07.123524Z",
     "start_time": "2025-03-11T16:48:54.920460Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model = ReconstructionTransformerAutoencoder(img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_eval_loss = np.inf \n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = trainReconstruction(train_dataloader, model, loss_fn, optimizer)\n",
    "    eval_loss = evalReconstruction(val_dataloader, model, loss_fn)\n",
    "    print(f\"Eval Loss: {eval_loss} for epoch {epoch}\")\n",
    "    with open(\"test.txt\", \"a\") as file:\n",
    "        file.write(f\"Eval Loss: {eval_loss} for epoch {epoch}\\n\")\n",
    "        \n",
    "    if eval_loss <= best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        checkpoint = {\"model\": model.state_dict(),\n",
    "              \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(checkpoint, f\"reconstruction/autoencoder/checkpoint_{epoch}.pytorch\")\n",
    "        img, label = training_data[0]\n",
    "        img = img.to(device)\n",
    "\n",
    "        res = model(img.unsqueeze(0))\n",
    "        plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.savefig(f\"./reconstruction/images/test{epoch}.png\", format=\"png\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b734b",
   "metadata": {},
   "source": [
    "### Run Segmentation Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "#for param in model.encoder.parameters():\n",
    "    #param.requires_grad = False\n",
    "model = SegmentationTransformerAutoencoder(img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_eval_loss = np.inf \n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = trainSegmentation(train_dataloader, model, loss_fn, optimizer)\n",
    "    eval_loss = evalSegmentation(val_dataloader, model, loss_fn)\n",
    "    print(f\"Eval Loss: {eval_loss} for epoch {epoch}\")\n",
    "    with open(\"test.txt\", \"a\") as file:\n",
    "        file.write(f\"Eval Loss: {eval_loss} for epoch {epoch}\\n\")\n",
    "        \n",
    "    if eval_loss <= best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        checkpoint = {\"model\": model.state_dict(),\n",
    "              \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(checkpoint, f\"autoencoder/checkpoint_{epoch}.pytorch\")\n",
    "        img, label = training_data[0]\n",
    "        img = img.to(device)\n",
    "\n",
    "        res = model(img.unsqueeze(0))\n",
    "        plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.savefig(f\"test{epoch}.png\", format=\"png\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496848c48c0dd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "checkpoint = torch.load(\"autoencoder/checkpoint_20.pytorch\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "img, label = training_data[0]\n",
    "img = img.to(device)\n",
    "\n",
    "res = model(img.unsqueeze(0))\n",
    "print(res.size())\n",
    "plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "plt.show\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
