{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.665275Z",
     "start_time": "2025-03-11T16:48:54.644297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataset import *\n",
    "from utils import *\n",
    "\n",
    "batch_size = 1\n",
    "training_data = dataset(\"../ResizedTrainVal/color\", \"../ResizedTrainVal/label\", target_transform=target_remap())\n",
    "#test_data = dataset(\"rtest/color\", \"rtest/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True,collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32026ffa6859c3f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.672030Z",
     "start_time": "2025-03-11T16:48:54.669422Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fb997cd390592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.687341Z",
     "start_time": "2025-03-11T16:48:54.681537Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, din):\n",
    "        super().__init__()\n",
    "        self.encoderPart1 = EncoderPart(din, 64)\n",
    "        self.encoderPart2 = EncoderPart(64, 32)\n",
    "        self.encoderPart3 = EncoderPart(32, 16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoderPart1(x)\n",
    "        x = self.encoderPart2(x)\n",
    "        x = self.encoderPart3(x)\n",
    "        return x\n",
    "\n",
    "class DecoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dout):\n",
    "        super().__init__()\n",
    "        self.decoderPart1 = DecoderPart(16, 16)\n",
    "        self.decoderPart2 = DecoderPart(16, 32)\n",
    "        self.decoderPart3 = DecoderPart(32, 64)\n",
    "        self.decoderOut = nn.Sequential(\n",
    "            nn.Conv2d(64, dout, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoderPart1(x)\n",
    "        x = self.decoderPart2(x)\n",
    "        x = self.decoderPart3(x)\n",
    "        x = self.decoderOut(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(din)\n",
    "        self.decoder = Decoder(dout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f918ffff43a85a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.886132Z",
     "start_time": "2025-03-11T16:48:54.695476Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(3, 3)\n",
    "image, label = training_data[0]\n",
    "print(image)\n",
    "print(image.size())\n",
    "pred = model(image)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2a5da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converts image into patch embeddings\"\"\"\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # Number of patches\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_dim = in_channels * patch_size * patch_size  # Flattened patch size\n",
    "        self.projection = nn.Linear(self.patch_dim, embed_dim)  # Linear projection to embedding size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Batch, Channels, Height, Width\n",
    "        # Convert image (B, C, H, W) into patches:  \n",
    "        # → (B, num_patches_height, num_patches_width, patch_size, patch_size, C):\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size).permute(0, 2, 4, 3, 5, 1)\n",
    "        # → (B, num_patches, patch_dim):\n",
    "        x = x.reshape(B, self.num_patches, -1)  # Flatten patches\n",
    "        return self.projection(x)  # Project into embedding space\n",
    "\n",
    "\n",
    "\n",
    "#Uses Transformer Encoder Layers (self-attention + feedforward network).\n",
    "#Positional Encoding ensures order information is preserved.\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, embed_dim=512, num_heads=4, ff_dim=1024, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))  # # Positional encoding TODO: ADJUST FIR IMG SIZE\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embedding[:, :x.shape[1], :]    # Add position information\n",
    "        return self.encoder_layers(x)\n",
    "\n",
    "\n",
    "\n",
    "# Uses cross-attention to refine the latent representation.\n",
    "class TransformerDecoder(nn.Module):    \n",
    "    def __init__(self, embed_dim=512, num_heads=4, ff_dim=1024, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        #self.fc_out = nn.Linear(embed_dim, patch_dim)    # Convert embeddings back to patch values\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        return self.decoder_layers(x, memory)  # Keep the output in (B, P, embed_dim)\n",
    "        #x = self.decoder_layers(x, memory)\n",
    "        #return self.fc_out(x)\n",
    "\n",
    "\n",
    "class ReconstructionTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=521, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6):  #ff_dim=2*embed_dim\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.encoder = TransformerEncoder(img_size, self.patch_embed.patch_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        self.decoder = TransformerDecoder(embed_dim, num_heads, ff_dim, num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, self.patch_embed.patch_dim)    # Convert embeddings back to patch values\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(\"[Reconstruction] Forward pass\")\n",
    "        patches = self.patch_embed(x)\n",
    "        encoded = self.encoder(patches)\n",
    "        decoded = self.decoder(encoded, encoded)\n",
    "        reconstructed_patches = self.fc_out(decoded)\n",
    "        B, P, D = reconstructed_patches.shape  # (batch, num_patches, patch_dim)\n",
    "        print(f\"[Reconstruction] (B, P, D): ({B}, {P}, {D}\")\n",
    "        print(f\"[Reconstruction] embed_dim (512?) vs patch_dim ({self.patch_embed.patch_dim}): {D}\")\n",
    "        img_size = int(P ** 0.5) * self.patch_embed.patch_size\n",
    "        return reconstructed_patches.view(B, 3, img_size, img_size)  # Reshape back to image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88e5d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SegmentationTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=4, ff_dim=1024, num_layers=6, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding to break image into patches\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        \n",
    "        # Transformer encoder for feature extraction\n",
    "        self.encoder = TransformerEncoder(img_size, self.patch_embed.patch_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        \n",
    "        # Transformer decoder to output class logits (for segmentation)\n",
    "        self.decoder = TransformerDecoder(embed_dim, num_heads, ff_dim, num_layers)\n",
    "        \n",
    "        # Segmentation head to output logits for 4 classes (background, boundary, cat, dog)\n",
    "        self.segmentation_head = nn.Conv2d(embed_dim, num_classes, kernel_size=1)  # Output 4 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"[Segmentation] Forward pass\")\n",
    "        # Embed the input image into patches\n",
    "        patches = self.patch_embed(x)\n",
    "        encoded = self.encoder(patches)\n",
    "        decoded = self.decoder(encoded, encoded)  # (B, num_patches, embedding_dim)\n",
    "   \n",
    "        # Reshape the decoded output to match image dimensions\n",
    "        B, P, D = decoded.shape\n",
    "        print(f\"[Segmentation] (B, P, D): ({B}, {P}, {D})\") #(1, 1024, 512)\n",
    "        print(f\"[Segmentation] patch_dim: {self.patch_embed.patch_dim}\")  #768\n",
    "        img_size = int(P ** 0.5) #* self.patch_embed.patch_size  # Calculate the original image size: \n",
    "                                                                # int(P ** 0.5) = sqrt(num_patches) = num patches along each side\n",
    "        print(f\"[Segmentation] calculated img_size: {img_size} -> img_size == 512: {img_size==512}\")    # 512 True\n",
    "        \n",
    "        # Reshape the decoded output into a 2D spatial representation\n",
    "        decoded = decoded.view(B, D, img_size, img_size)  # Reshape to (B, H, W, D) and change to (B, D, H, W) for Conv2d\n",
    "\n",
    "        \n",
    "        # Segmentation head: output logits for 4 classes (background, boundary, cat, dog)\n",
    "        segmentation_logits = self.segmentation_head(decoded)  # (B, num_classes, H, W)\n",
    "        return segmentation_logits  # Output segmentation mask (logits for each class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b21005fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model_1 (SegmentationTransformerAutoencoder): 32855296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3673 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Segmentation] Forward pass\n",
      "[Segmentation] (B, P, D): (1, 1024, 512)\n",
      "[Segmentation] patch_dim: 768\n",
      "[Segmentation] calculated img_size: 32 -> img_size == 512: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3673 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     45\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 4"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model_1 = ReconstructionTransformerAutoencoder().to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model_1.parameters())\n",
    "print(f\"Total parameters in model_1 (SegmentationTransformerAutoencoder): {pytorch_total_params}\")\n",
    "model_2 = SegmentationTransformerAutoencoder().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "losses = []\n",
    "target_batch_size = 1  #TODO before submission\n",
    "batch_size = 1          #TODO before submission\n",
    "to_print = True\n",
    "for model in [model_2, model_1]:\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for batch, (X, _) in enumerate(tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")):\n",
    "        # Resize images using imgaug\n",
    "        #X = X.permute(0, 2, 3, 1).cpu().numpy()  # Convert from [B, C, H, W] to [B, H, W, C]\n",
    "        #resize = iaa.Resize({\"height\": 256, \"width\": 256})\n",
    "        #X = resize(image=X[0])  # Apply the resize operation\n",
    "        \n",
    "        # Convert back to PyTorch tensor and send to device\n",
    "        #X = torch.tensor(X).permute(0, 3, 1, 2)  # Convert back to [B, C, H, W]\n",
    "        \n",
    "        X = X.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, X)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Ensure gradients are reset to 0 for new batch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "           \n",
    "        plt.imshow(X[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.show()\n",
    "        plt.imshow(pred[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.show()\n",
    "    print(f\"losses: {losses}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d11d48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326529982cdc3fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.897473Z",
     "start_time": "2025-03-11T16:48:54.894249Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    target_batch_size = 64  #TODO before submission\n",
    "    batch_size = 16          #TODO before submission\n",
    "    to_print = True\n",
    "    for batch, (X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "        X = X.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, X)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if batch % (target_batch_size/batch_size) == 0:\n",
    "            # Ensure gradients are reset to 0 for new batch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # if to_print:\n",
    "            #     print(f\"memory: {torch.cuda.device_memory_used()}\")\n",
    "            #     to_print = False\n",
    "        \n",
    "    return np.mean(losses)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff920a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a079fe9e2df3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.907981Z",
     "start_time": "2025-03-11T16:48:54.905263Z"
    }
   },
   "outputs": [],
   "source": [
    "target_size = 512\n",
    "interpolation = 'bilinear'\n",
    "\n",
    "def eval(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "            X, meta_list = process_batch_forward(X, target_size=target_size)\n",
    "            X = X.to(device)\n",
    "            # Compute prediction\n",
    "            pred = model(X)\n",
    "\n",
    "            pred = process_batch_reverse(pred, meta_list, interpolation=interpolation)\n",
    "\n",
    "            for p, label in zip(pred, X):\n",
    "                \n",
    "                # Move individual prediction and label to the device\n",
    "                p = p.to(device).unsqueeze(0)  # Add batch dimension\n",
    "                label = label.to(device).unsqueeze(0)  # Add batch dimension and ensure type is long\n",
    "                \n",
    "                # print(p.size(), flush=True)\n",
    "                # print(label.size())\n",
    "                # Calculate the loss for the current pair\n",
    "                loss = loss_fn(p, label.squeeze(1))\n",
    "                total_loss += loss.item()\n",
    "            # Compute loss\n",
    "            # loss = loss_fn(pred, X)\n",
    "            # losses.append(loss.item())\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fa696",
   "metadata": {},
   "source": [
    "### Run Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44f2b0ab103c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:52:07.123524Z",
     "start_time": "2025-03-11T16:48:54.920460Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model = Autoencoder(3, 3).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_eval_loss = np.inf \n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    eval_loss = eval(train_dataloader, model, loss_fn)\n",
    "    print(f\"Eval Loss: {eval_loss} for epoch {epoch}\")\n",
    "    with open(\"test.txt\", \"a\") as file:\n",
    "        file.write(f\"Eval Loss: {eval_loss} for epoch {epoch}\\n\")\n",
    "        \n",
    "    if eval_loss <= best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        checkpoint = {\"model\": model.state_dict(),\n",
    "              \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(checkpoint, f\"autoencoder/checkpoint_{epoch}.pytorch\")\n",
    "        img, label = training_data[0]\n",
    "        img = img.to(device)\n",
    "\n",
    "        res = model(img.unsqueeze(0))\n",
    "        plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.savefig(f\"test{epoch}.png\", format=\"png\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b734b",
   "metadata": {},
   "source": [
    "### Run Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model = ReconstructionTransformerAutoencoder().to(device) #using default img_size=521, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_eval_loss = np.inf \n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    eval_loss = eval(train_dataloader, model, loss_fn)\n",
    "    print(f\"Eval Loss: {eval_loss} for epoch {epoch}\")\n",
    "    with open(\"test.txt\", \"a\") as file:\n",
    "        file.write(f\"Eval Loss: {eval_loss} for epoch {epoch}\\n\")\n",
    "        \n",
    "    if eval_loss <= best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        checkpoint = {\"model\": model.state_dict(),\n",
    "              \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(checkpoint, f\"autoencoder/checkpoint_{epoch}.pytorch\")\n",
    "        img, label = training_data[0]\n",
    "        img = img.to(device)\n",
    "\n",
    "        res = model(img.unsqueeze(0))\n",
    "        plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.savefig(f\"test{epoch}.png\", format=\"png\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496848c48c0dd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "checkpoint = torch.load(\"autoencoder/checkpoint_20.pytorch\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "img, label = training_data[0]\n",
    "img = img.to(device)\n",
    "\n",
    "res = model(img.unsqueeze(0))\n",
    "print(res.size())\n",
    "plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3c00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
