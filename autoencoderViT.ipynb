{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.665275Z",
     "start_time": "2025-03-11T16:48:54.644297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "from utils import *\n",
    "\n",
    "batch_size = 2\n",
    "training_data = dataset(\"../atrain/color\", \"../atrain/label\", target_transform=target_remap())\n",
    "validation_data = dataset(\"../val/color\", \"../val/label\", target_transform=target_remap())\n",
    "#test_data = dataset(\"rtest/color\", \"rtest/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True,collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a12c4",
   "metadata": {},
   "source": [
    "### Old Autoencoder (not tranformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fb997cd390592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.687341Z",
     "start_time": "2025-03-11T16:48:54.681537Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, din):\n",
    "        super().__init__()\n",
    "        self.encoderPart1 = EncoderPart(din, 64)\n",
    "        self.encoderPart2 = EncoderPart(64, 32)\n",
    "        self.encoderPart3 = EncoderPart(32, 16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoderPart1(x)\n",
    "        x = self.encoderPart2(x)\n",
    "        x = self.encoderPart3(x)\n",
    "        return x\n",
    "\n",
    "class DecoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dout):\n",
    "        super().__init__()\n",
    "        self.decoderPart1 = DecoderPart(16, 16)\n",
    "        self.decoderPart2 = DecoderPart(16, 32)\n",
    "        self.decoderPart3 = DecoderPart(32, 64)\n",
    "        self.decoderOut = nn.Sequential(\n",
    "            nn.Conv2d(64, dout, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoderPart1(x)\n",
    "        x = self.decoderPart2(x)\n",
    "        x = self.decoderPart3(x)\n",
    "        x = self.decoderOut(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(din)\n",
    "        self.decoder = Decoder(dout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f918ffff43a85a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.886132Z",
     "start_time": "2025-03-11T16:48:54.695476Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = Autoencoder(3, 3)\n",
    "image, label = training_data[0]\n",
    "print(image)\n",
    "print(image.size())\n",
    "pred = model(image)\n",
    "print(pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34123c9",
   "metadata": {},
   "source": [
    "### Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converts image into patch embeddings\"\"\"\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # Number of patches\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_dim = in_channels * patch_size * patch_size  # Flattened patch size\n",
    "        self.projection = nn.Linear(self.patch_dim, embed_dim)  # Linear projection to embedding size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Batch, Channels, Height, Width\n",
    "        # Convert image (B, C, H, W) into patches:  \n",
    "        # → (B, num_patches_height, num_patches_width, patch_size, patch_size, C):\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size).permute(0, 2, 4, 3, 5, 1)\n",
    "        # → (B, num_patches, patch_dim):\n",
    "        x = x.reshape(B, self.num_patches, -1)  # Flatten patches\n",
    "        return self.projection(x)  # Project into embedding space\n",
    "\n",
    "\n",
    "\n",
    "#Uses Transformer Encoder Layers (self-attention + feedforward network).\n",
    "#Positional Encoding ensures order information is preserved.\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, embed_dim=512, num_heads=4, ff_dim=1024, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))  # Positional encoding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embedding[:, :x.shape[1], :]    # Add position information\n",
    "        return self.encoder_layers(x)\n",
    "\n",
    "\n",
    "\n",
    "# Uses cross-attention to refine the latent representation.\n",
    "\"\"\"\n",
    "class TransformerDecoder(nn.Module):    \n",
    "    def __init__(self, embed_dim=512, num_heads=4, ff_dim=1024, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        return self.decoder_layers(x, memory)  # Keep the output in (B, P, embed_dim)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    \"\"\"CNN Decoder for reconstructing image from patch embeddings\"\"\"\n",
    "    def __init__(self, img_size=512, embed_dim=512, num_patches=1024, patch_size=16, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        # Calculate the initial spatial dimensions of the patch grid\n",
    "        self.patches_per_side = img_size//patch_size   \n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Define the number of upsampling steps \n",
    "        # Each step doubles the resolution -> need log2(patch_size) steps.\n",
    "        num_upsample_steps = int(math.log2(patch_size)) #4\n",
    "\n",
    "        # Build the decoder layers\n",
    "        layers = []\n",
    "        current_channels = embed_dim\n",
    "\n",
    "        #Start with a Conv layer to potentially adjust channels before upsampling\n",
    "        #layers.append(nn.Conv2d(embed_dim, current_channels, kernel_size=3, stride=1, padding=1))\n",
    "        #layers.append(nn.BatchNorm2d(current_channels))\n",
    "        #layers.append(nn.ReLU()) # TODO: remove\n",
    "\n",
    "        # Add upsampling blocks\n",
    "        for i in range(num_upsample_steps):\n",
    "            out_channels_block = current_channels // 2  # Halve channels at each step (example strategy)\n",
    "            layers.extend([\n",
    "                # Use ConvTranspose2d for learnable upsampling\n",
    "                nn.ConvTranspose2d(current_channels, out_channels_block, kernel_size=2, stride=2),  # upsample image size by 2, reduce channels\n",
    "                nn.BatchNorm2d(out_channels_block),\n",
    "                nn.ReLU(),\n",
    "                # Add another Conv layer for refinement at this scale\n",
    "                nn.Conv2d(out_channels_block, out_channels_block, kernel_size=3, stride=1, padding=1),  # reduce channels\n",
    "                nn.BatchNorm2d(out_channels_block),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "            current_channels = out_channels_block\n",
    "\n",
    "        # Final layer to map to the desired output channels (e.g., 3 for RGB)\n",
    "        layers.append(nn.Conv2d(current_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        # Optional: Add Tanh or Sigmoid if you want bounded output pixels\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, P, E) - B = batch, P = num_patches, E = embed_dim\n",
    "        B, P, E = x.shape\n",
    "        if P != self.num_patches:\n",
    "             raise ValueError(f\"Input patch count {P} doesn't match expected {self.num_patches}\")\n",
    "        if E != self.embed_dim:\n",
    "             raise ValueError(f\"Input embed dim {E} doesn't match expected {self.embed_dim}\")\n",
    "\n",
    "        # Reshape to spatial grid: (B, P, E) -> (B, H', W', E)\n",
    "        x = x.view(B, self.patches_per_side, self.patches_per_side, E)\n",
    "        # Permute to PyTorch format: (B, E, H', W')\n",
    "        x = x.permute(0, 3, 1, 2).contiguous() # Ensure contiguous memory\n",
    "\n",
    "        # Expected final shape: (B, self.out_channels, self.patches_per_side, self.patches_per_side)\n",
    "\n",
    "        # Pass through the CNN decoder layers to get reconstructed image\n",
    "        return self.decoder(x)  \n",
    "\n",
    "\n",
    "class ReconstructionTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6):  #ff_dim=2*embed_dim\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.encoder = TransformerEncoder(img_size, self.patch_embed.patch_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        #self.decoder = TransformerDecoder(embed_dim, num_heads, ff_dim, num_layers)\n",
    "        #self.fc_out = nn.Linear(embed_dim, self.patch_embed.patch_dim)    # Convert embeddings back to patch values\n",
    "        self.decoder = CNNDecoder(\n",
    "            img_size=img_size,\n",
    "            embed_dim=embed_dim,\n",
    "            num_patches=self.patch_embed.num_patches,\n",
    "            patch_size=self.patch_embed.patch_size,\n",
    "            out_channels=in_channels # Output should have same channels as input\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x)\n",
    "        encoded = self.encoder(patches)\n",
    "        reconstructed_image = self.decoder(encoded)\n",
    "        return reconstructed_image\n",
    "        #decoded = self.decoder(encoded, encoded)\n",
    "        #reconstructed_patches = self.fc_out(decoded)\n",
    "        #B, P, D = reconstructed_patches.shape  # (batch, num_patches, patch_dim)\n",
    "        #img_size = int(P ** 0.5) * self.patch_embed.patch_size\n",
    "        #return reconstructed_patches.view(B, 3, img_size, img_size)  # Reshape back to image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SegmentationTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=12, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding to break image into patches\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        \n",
    "        # Transformer encoder for feature extraction\n",
    "        self.encoder = TransformerEncoder(img_size, self.patch_embed.patch_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        \n",
    "        # Transformer decoder to output class logits (for segmentation)\n",
    "        self.decoder = TransformerDecoder(embed_dim, num_heads, ff_dim, num_layers)\n",
    "        \n",
    "        # Segmentation head to output logits for 4 classes (background, boundary, cat, dog)\n",
    "        self.segmentation_head = nn.Conv2d(embed_dim, num_classes, kernel_size=1)  # Output 4 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"[Segmentation] Forward pass\")\n",
    "        # Embed the input image into patches\n",
    "        patches = self.patch_embed(x)\n",
    "        encoded = self.encoder(patches)\n",
    "        decoded = self.decoder(encoded, encoded)  # (B, num_patches, embedding_dim)\n",
    "   \n",
    "        # Reshape the decoded output to match image dimensions\n",
    "        B, P, D = decoded.shape\n",
    "        img_size = int(P ** 0.5)    # Calculate the original image size: \n",
    "                                    # int(P ** 0.5) = sqrt(num_patches) = num patches along each side\n",
    "        # Reshape the decoded output into a 2D spatial representation\n",
    "        decoded = decoded.view(B, D, img_size, img_size)  # Reshape to (B, H, W, D) and change to (B, D, H, W) for Conv2d\n",
    "\n",
    "        # Segmentation head: output logits for 4 classes (background, boundary, cat, dog)\n",
    "        segmentation_logits = self.segmentation_head(decoded)  # (B, num_classes, H, W)\n",
    "\n",
    "        # Upsample to original image size\n",
    "        segmentation_logits_upsampled = F.interpolate(segmentation_logits, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return segmentation_logits_upsampled  # Output segmentation mask (logits for each class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21005fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from tqdm import tqdm\n",
    "import torch\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model_1 = ReconstructionTransformerAutoencoder().to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model_1.parameters())\n",
    "print(f\"Total parameters in model_1 (SegmentationTransformerAutoencoder): {pytorch_total_params}\")\n",
    "model_2 = SegmentationTransformerAutoencoder().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() # or MSE for reconstruction\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "losses = []\n",
    "target_batch_size = 1  #TODO before submission\n",
    "batch_size = 1          #TODO before submission\n",
    "to_print = True\n",
    "for model in [model_2, model_1]:\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for batch, (X, y) in enumerate(tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")):\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        print(f\"pred shape: {pred.shape}\")\n",
    "        print(f\"y shape: {y.shape}\")    #comment for reconstruction\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        # Compute loss\n",
    "        #loss = loss_fn(pred, X) # for reconstruction\n",
    "        loss = loss_fn(pred, y.squeeze(1)) #for segmentation\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Ensure gradients are reset to 0 for new batch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "           \n",
    "        plt.imshow(X[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.show()\n",
    "        plt.imshow(pred[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.show()\n",
    "    print(f\"losses: {losses}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d11d48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainReconstruction(dataloader, model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    target_batch_size = 64  #TODO before submission\n",
    "    #batch_size = 16          #TODO before submission\n",
    "    to_print = True\n",
    "    for batch, (X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "        X = X.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, X)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if batch % (target_batch_size/batch_size) == 0:\n",
    "            # Ensure gradients are reset to 0 for new batch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # if to_print:\n",
    "            #     print(f\"memory: {torch.cuda.device_memory_used()}\")\n",
    "            #     to_print = False\n",
    "        \n",
    "    return np.mean(losses)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326529982cdc3fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.897473Z",
     "start_time": "2025-03-11T16:48:54.894249Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainSegmentation(dataloader, model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    target_batch_size = 64  #TODO before submission\n",
    "    batch_size = 16          #TODO before submission\n",
    "    to_print = True\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, y.squeeze(1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if batch % (target_batch_size/batch_size) == 0:\n",
    "            # Ensure gradients are reset to 0 for new batch\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # if to_print:\n",
    "            #     print(f\"memory: {torch.cuda.device_memory_used()}\")\n",
    "            #     to_print = False\n",
    "        \n",
    "    return np.mean(losses)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff920a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a079fe9e2df3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.907981Z",
     "start_time": "2025-03-11T16:48:54.905263Z"
    }
   },
   "outputs": [],
   "source": [
    "target_size = 512\n",
    "interpolation = 'bilinear'\n",
    "\n",
    "def evalReconstruction(dataloader, model, loss_fn):\n",
    "    #print(\"Evaluating reconstruction\")\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, (original_X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Evaluation\")):\n",
    "            resized_X, meta_list = process_batch_forward(original_X, target_size=target_size)   # resize X for network\n",
    "            resized_X = resized_X.to(device)\n",
    "            \n",
    "            # Compute prediction\n",
    "            pred = model(resized_X)\n",
    "\n",
    "            pred = process_batch_reverse(pred, meta_list, interpolation=interpolation)\n",
    "\n",
    "            for p, label in zip(pred, original_X):\n",
    "                # Move individual prediction and label to the device\n",
    "                p = p.to(device).unsqueeze(0)  # Add batch dimension\n",
    "                label = label.to(device).unsqueeze(0)  # Add batch dimension and ensure type is long\n",
    "\n",
    "                if label.shape[1] == 4 and label.ndim == 4:  \n",
    "                    print(f\"    Converting original image from RGBA to RGB\")\n",
    "                    label = label[:, :3, :, :] # Keep only the first 3 channels (R, G, B)\n",
    "\n",
    "                #print(f\"p shape: {p.shape}\")\n",
    "                #print(f\"label shape: {label.shape}\")\n",
    "                # print(p.size(), flush=True)\n",
    "                # print(label.size())\n",
    "                # Calculate the loss for the current pair\n",
    "                loss = loss_fn(p, label.squeeze(1))\n",
    "                total_loss += loss.item()\n",
    "                # Loss list\n",
    "                losses.append(loss.item())\n",
    "    \n",
    "    return total_loss / num_batches, np.mean(losses)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c051de",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 512\n",
    "interpolation = 'bilinear'\n",
    "\n",
    "def evalSegmentation(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Evaluation\")):\n",
    "            X, meta_list = process_batch_forward(X, target_size=target_size)\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            if X.shape[1] == 4 and X.ndim == 4:  \n",
    "                #print(f\"    Converting RGBA image to RGB image\")\n",
    "                X = X[:, :3, :, :] # Keep only the first 3 channels (R, G, B)\n",
    "\n",
    "            # Compute prediction\n",
    "            pred = model(X)\n",
    "\n",
    "            pred = process_batch_reverse(pred, meta_list, interpolation=interpolation)\n",
    "\n",
    "            for p, label in zip(pred, y):\n",
    "                \n",
    "                # Move individual prediction and label to the device\n",
    "                p = p.to(device).unsqueeze(0)  # Add batch dimension\n",
    "                label = label.to(device).unsqueeze(0)  # Add batch dimension and ensure type is long\n",
    "                \n",
    "                # print(p.size(), flush=True)\n",
    "                # print(label.size())\n",
    "                # Calculate the loss for the current pair\n",
    "                loss = loss_fn(p, label.squeeze(1))\n",
    "                total_loss += loss.item()\n",
    "            # Compute loss\n",
    "            # loss = loss_fn(pred, X)\n",
    "            # losses.append(loss.item())\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fa696",
   "metadata": {},
   "source": [
    "### Run Reconstruction Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44f2b0ab103c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:52:07.123524Z",
     "start_time": "2025-03-11T16:48:54.920460Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model = ReconstructionTransformerAutoencoder(img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "MODEL_SAVE_DIR = \"reconstruction/autoencoder\"\n",
    "MODEL_NAME = \"checkpoint.pytorch\"\n",
    "start_epoch = 0\n",
    "getPastCheckpoint = False\n",
    "\n",
    "if getPastCheckpoint and os.path.isfile(f\"{MODEL_SAVE_DIR}/{MODEL_NAME}\"):\n",
    "    print(f\"Loading checkpoint from: {MODEL_SAVE_DIR}/{MODEL_NAME}\")\n",
    "    # Load the checkpoint dictionary; move tensors to the correct device\n",
    "    checkpoint = torch.load(f\"{MODEL_SAVE_DIR}/{MODEL_NAME}\", map_location=device)\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(\" -> Model state loaded.\")\n",
    "\n",
    "    # Load optimizer state\n",
    "    try:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\" -> Optimizer state loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Warning: Could not load optimizer state: {e}. Optimizer will start from scratch.\")\n",
    "\n",
    "    # Load scheduler state\n",
    "    # try:\n",
    "    #     scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    #     print(\" -> Scheduler state loaded.\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\" -> Warning: Could not load scheduler state: {e}. Scheduler will start from scratch.\")\n",
    "\n",
    "\n",
    "    # Load training metadata\n",
    "    start_epoch = checkpoint.get(\"epoch\", 0) # Load last completed epoch, training continues from next one\n",
    "    # best_dev_dice = checkpoint.get(\"best_dev_dice\", -np.inf)\n",
    "    # best_dev_miou = checkpoint.get(\"best_dev_miou\", -np.inf)\n",
    "    best_val_loss = checkpoint.get(\"best_val_loss\", np.inf)\n",
    "\n",
    "    print(f\" -> Resuming training from epoch {start_epoch + 1}\")\n",
    "    # print(f\" -> Loaded best metrics: Dice={best_dev_dice:.6f}, mIoU={best_dev_miou:.6f}, Loss={best_dev_loss:.6f}\")\n",
    "    loaded_notes = checkpoint.get(\"notes\", \"N/A\")\n",
    "    print(f\" -> Notes from checkpoint: {loaded_notes}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Checkpoint file not found at {MODEL_SAVE_DIR}/{MODEL_NAME}. Starting training from scratch.\")\n",
    "\n",
    "\n",
    "\n",
    "best_val_loss = np.inf \n",
    "EPOCHS = 100\n",
    "print(\"\\nStarting Training (Transformer Autoencoder)...\")\n",
    "for t in range(start_epoch, EPOCHS):\n",
    "    current_epoch = t + 1\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = trainReconstruction(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    val_loss, debug_val_loss = evalReconstruction(val_dataloader, model, loss_fn)\n",
    "\n",
    "    # Save model based on validation val loss improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved ({best_val_loss:.6f} → {val_loss:.6f}). Saving model...\")\n",
    "        # best_dev_dice = val_dice_micro\n",
    "        # best_dev_miou = val_miou # Save corresponding mIoU\n",
    "        best_val_loss = val_loss # Save corresponding loss\n",
    "        # print(f\"Validation Micro Dice score improved ({best_dev_dice:.6f}). Saving model...\")\n",
    "        checkpoint_path = os.path.join(MODEL_SAVE_DIR, f\"{MODEL_NAME}\") # Changed name\n",
    "        checkpoint = {\n",
    "            \"epoch\": t + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            # \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            # \"best_dev_dice\": best_dev_dice,\n",
    "            # \"best_dev_miou\": best_dev_miou,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            # \"notes\": f\"Model saved based on best Micro Dice. Ignored index for metric: {EVAL_IGNORE_INDEX}\"\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    else:\n",
    "    #     print(f\"Validation Micro Dice score did not improve from {best_dev_dice:.6f}\")\n",
    "        print(f\"Corresponding validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    print(f\"Debug Validation loss: {debug_val_loss:.6f}\")\n",
    "    print(f\"Train loss: {train_loss:.6f}\")\n",
    "\n",
    "# PLot a training image reconstruction\n",
    "img, label = training_data[0]\n",
    "img = img.to(device)\n",
    "res = model(img.unsqueeze(0))\n",
    "plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "plt.savefig(f\"reconstruction/images/test{t}.png\", format=\"png\")\n",
    "plt.show()\n",
    "    \n",
    "print(\"\\n--- Training Finished! ---\")\n",
    "# print(f\"Best validation Micro Dice score achieved: {best_dev_dice:.6f}\")\n",
    "# print(f\"Corresponding validation mIoU: {best_dev_miou:.6f}\")\n",
    "print(f\"Best model saved to: {os.path.join(MODEL_SAVE_DIR, f'{MODEL_NAME}')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b734b",
   "metadata": {},
   "source": [
    "### Run Segmentation Transformer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "#for param in model.encoder.parameters():\n",
    "    #param.requires_grad = False\n",
    "model = SegmentationTransformerAutoencoder(img_size=512, patch_size=16, in_channels=3, embed_dim=512, num_heads=8, ff_dim=1024, num_layers=6).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_eval_loss = np.inf \n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = trainSegmentation(train_dataloader, model, loss_fn, optimizer)\n",
    "    eval_loss = evalSegmentation(val_dataloader, model, loss_fn)\n",
    "    print(f\"Eval Loss: {eval_loss} for epoch {epoch}\")\n",
    "    with open(\"test.txt\", \"a\") as file:\n",
    "        file.write(f\"Eval Loss: {eval_loss} for epoch {epoch}\\n\")\n",
    "        \n",
    "    if eval_loss <= best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        checkpoint = {\"model\": model.state_dict(),\n",
    "              \"optimizer\": optimizer.state_dict()}\n",
    "        torch.save(checkpoint, f\"autoencoder/checkpoint_{epoch}.pytorch\")\n",
    "        img, label = training_data[0]\n",
    "        img = img.to(device)\n",
    "\n",
    "        res = model(img.unsqueeze(0))\n",
    "        plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "        plt.savefig(f\"test{epoch}.png\", format=\"png\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496848c48c0dd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "checkpoint = torch.load(\"autoencoder/checkpoint_20.pytorch\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "img, label = training_data[0]\n",
    "img = img.to(device)\n",
    "\n",
    "res = model(img.unsqueeze(0))\n",
    "print(res.size())\n",
    "plt.imshow(res[0].permute(1,2,0).cpu().detach().numpy())\n",
    "plt.show\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
