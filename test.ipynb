{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "img = Image.open(\"tmp/label/Abyssinian_1.png\")\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "width, height = img.size\n",
    "pixels = img.load()\n",
    "\n",
    "colors = [[pixels[w,h] for w in range(width)] for h in range(height)]\n",
    "\n",
    "print(np.unique(colors))\n",
    "\n",
    "# img = read_image(\"TrainVal/label/american_bulldog_10.png\")\n",
    "# print(torch.unique(img))\n",
    "# img = ToPILImage()(img)\n",
    "# pixels = img.load()\n",
    "\n",
    "# colors = [[pixels[w,h] for w in range(width)] for h in range(height)]\n",
    "# print(np.unique(colors))\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "# nimg = np.array(img)\n",
    "# print(nimg)\n",
    "# np.savetxt(\"test.txt\", nimg, fmt='%.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = \"Abyssinian_1.jpg\"\n",
    "train = \"TrainVal/color/\"\n",
    "target = \"tmp/\"\n",
    "\n",
    "base_width = 512\n",
    "img = Image.open(train+image)\n",
    "wpercent = (base_width / float(img.size[0]))\n",
    "hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "img = img.resize((base_width, hsize), Image.Resampling.LANCZOS)\n",
    "img.save(target+image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the paths for the source and target directories\n",
    "train_dir = \"Test/color/\"\n",
    "target_dir = \"rtest/color\"\n",
    "target_size = 512\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Retrieve list of image files with supported extensions\n",
    "image_files = [f for f in os.listdir(train_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "\n",
    "# Loop over all image files with a progress bar\n",
    "for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "    img_path = os.path.join(train_dir, image_file)\n",
    "    img = Image.open(img_path)\n",
    "    width, height = img.size\n",
    "\n",
    "    # Calculate the new dimensions while maintaining aspect ratio\n",
    "    if width > height:\n",
    "        new_width = target_size\n",
    "        new_height = int(height * (target_size / width))\n",
    "    else:\n",
    "        new_height = target_size\n",
    "        new_width = int(width * (target_size / height))\n",
    "\n",
    "    # Resize the image using high-quality resampling\n",
    "    resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Create a new blank image with a black background\n",
    "    new_img = Image.new(\"RGB\", (target_size, target_size), (0, 0, 0))\n",
    "    \n",
    "    # Calculate the position to paste the resized image\n",
    "    paste_x = (target_size - new_width) // 2\n",
    "    paste_y = (target_size - new_height) // 2\n",
    "\n",
    "    # Paste the resized image onto the blank canvas\n",
    "    new_img.paste(resized_img, (paste_x, paste_y))\n",
    "\n",
    "    # Save the final image\n",
    "    new_img.save(os.path.join(target_dir, image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the paths for the source and target directories\n",
    "train_dir = \"Test/label/\"\n",
    "target_dir = \"rtest/label\"\n",
    "target_size = 512\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "# Retrieve list of image files with supported extensions\n",
    "image_files = [f for f in os.listdir(train_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "\n",
    "# Loop over all image files with a progress bar\n",
    "for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "    img_path = os.path.join(train_dir, image_file)\n",
    "    \n",
    "    # Open the image and convert to numpy array to ensure we can control the values\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Verify we have the expected values\n",
    "    unique_values = np.unique(img_array)\n",
    "    # print(f\"Image {image_file} has unique values: {unique_values}\")\n",
    "    \n",
    "    # Get dimensions\n",
    "    height, width = img_array.shape[:2]\n",
    "    \n",
    "    # Calculate the new dimensions while maintaining aspect ratio\n",
    "    if width > height:\n",
    "        new_width = target_size\n",
    "        new_height = int(height * (target_size / width))\n",
    "    else:\n",
    "        new_height = target_size\n",
    "        new_width = int(width * (target_size / height))\n",
    "    \n",
    "    # Create a new blank array with zeros (black background)\n",
    "    new_array = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "    \n",
    "    # Resize the original image using PIL with NEAREST resampling\n",
    "    # This preserves the exact pixel values\n",
    "    resized_img = img.resize((new_width, new_height), Image.Resampling.NEAREST)\n",
    "    resized_array = np.array(resized_img)\n",
    "    \n",
    "    # Calculate the position to paste the resized image\n",
    "    paste_y = (target_size - new_height) // 2\n",
    "    paste_x = (target_size - new_width) // 2\n",
    "    \n",
    "    # Paste the resized image into the new array\n",
    "    new_array[paste_y:paste_y+new_height, paste_x:paste_x+new_width] = resized_array\n",
    "    \n",
    "    # Verify the final array has only the expected values\n",
    "    final_unique_values = np.unique(new_array)\n",
    "    # print(f\"Final image has unique values: {final_unique_values}\")\n",
    "    \n",
    "    # Convert back to PIL Image and save\n",
    "    new_img = Image.fromarray(new_array)\n",
    "    new_img.save(os.path.join(target_dir, image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Set the folder path containing the images (update this with your folder path)\n",
    "folder_path = 'tmp/color'\n",
    "\n",
    "# Compile the regex pattern:\n",
    "# This matches file names ending with an underscore, one or more digits, and \".jpg\".\n",
    "pattern = re.compile(r'^.*_\\d+\\.jpg$', re.IGNORECASE)\n",
    "\n",
    "# List to collect files that do NOT match the naming convention\n",
    "non_matching_files = []\n",
    "\n",
    "# Loop over all files in the specified folder\n",
    "for file in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    # Ensure we only check files\n",
    "    if os.path.isfile(file_path):\n",
    "        if not pattern.match(file):\n",
    "            non_matching_files.append(file)\n",
    "\n",
    "print(\"Files that do not follow the naming convention '<name>_<number>.jpg':\")\n",
    "print(non_matching_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_jpg_files(source_folder, destination_folder):\n",
    "    \"\"\"\n",
    "    Moves all JPG files from the source folder to the destination folder.\n",
    "\n",
    "    Args:\n",
    "        source_folder (str): The path to the source folder.\n",
    "        destination_folder (str): The path to the destination folder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Iterate through all files in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        # Check if the file ends with .jpg (case-insensitive)\n",
    "        if filename.lower().endswith(\".jpg\"):\n",
    "            # Create the full paths for the source and destination files\n",
    "            source_path = os.path.join(source_folder, filename)\n",
    "            destination_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "            try:\n",
    "                # Move the file\n",
    "                shutil.move(source_path, destination_path)\n",
    "                print(f\"Moved '{filename}' from '{source_folder}' to '{destination_folder}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error moving '{filename}': {e}\")\n",
    "\n",
    "# Example usage (replace with your actual folder paths)\n",
    "source_folder = \"tmp\"  # Replace with your source folder path\n",
    "destination_folder = \"tmp/color\"  # Replace with your destination folder path\n",
    "\n",
    "move_jpg_files(source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dout, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "    \n",
    "class up(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dout, dout, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(dout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scale = 3 # maybe using the original channels might be better\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.down1 = down(3, self.scale * 64)\n",
    "        self.down2 = down(self.scale * 64, self.scale * 128)\n",
    "        self.down3 = down(self.scale * 128, self.scale * 256)\n",
    "        self.down4 = down(self.scale * 256, self.scale * 512)\n",
    "        self.down5 = down(self.scale * 512, self.scale * 1024)\n",
    "        self.up1 = up(self.scale * 1024, self.scale * 512)\n",
    "        self.up2 = up(self.scale * 512, self.scale * 256)\n",
    "        self.up3 = up(self.scale * 256, self.scale * 128)\n",
    "        self.up4 = up(self.scale * 128, self.scale * 64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.upconv1 = nn.Conv2d(self.scale * 1024, self.scale * 512, kernel_size=3, padding=1)\n",
    "        self.upconv2 = nn.Conv2d(self.scale * 512, self.scale * 256, kernel_size=3, padding=1)\n",
    "        self.upconv3 = nn.Conv2d(self.scale * 256, self.scale * 128, kernel_size=3, padding=1)\n",
    "        self.upconv4 = nn.Conv2d(self.scale * 128, self.scale * 64, kernel_size=3, padding=1)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(self.scale * 64, 4, kernel_size=1),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(self.maxpool(x1))\n",
    "        x3 = self.down3(self.maxpool(x2))\n",
    "        x4 = self.down4(self.maxpool(x3))\n",
    "        x5 = self.down5(self.maxpool(x4))\n",
    "        x6 = self.up1(torch.cat([x4, self.upconv1(self.upsample(x5))], dim=1))\n",
    "        x7 = self.up2(torch.cat([x3, self.upconv2(self.upsample(x6))], dim=1))\n",
    "        x8 = self.up3(torch.cat([x2, self.upconv3(self.upsample(x7))], dim=1))\n",
    "        x9 = self.up4(torch.cat([x1, self.upconv4(self.upsample(x8))], dim=1))\n",
    "        pre_output = self.output(x9)\n",
    "        return torch.argmax(pre_output, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\"tmp/color/Abyssinian_1.jpg\")\n",
    "# t = transforms.ToTensor()(img)\n",
    "# t = t.unsqueeze(0)\n",
    "\n",
    "# print(t.size())\n",
    "\n",
    "model = unet()\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    res = model(t)\n",
    "    print(\"Output tensor shape:\", res.shape)\n",
    "    print(\"Output tensor:\\n\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, PILToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_names = sorted([os.path.splitext(filename)[0] for filename in os.listdir(img_dir)])\n",
    "        self.len = len(self.img_names)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(os.path.join(self.img_dir, self.img_names[idx] + \".jpg\"))\n",
    "        label = read_image(os.path.join(self.label_dir, self.img_names[idx] + \".png\"))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "def display_img_label(data, idx):\n",
    "    img, label = data[idx]\n",
    "    figure = plt.figure(figsize=(10,20))\n",
    "    figure.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    figure.add_subplot(1, 2, 2)\n",
    "    plt.imshow(label.permute(1, 2, 0), cmap='grey')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class target_remap(object):\n",
    "    def __call__(self, img):\n",
    "        img[img == 255] = 3\n",
    "        return img\n",
    "\n",
    "\n",
    "training_data = dataset(\"rtrain/color\", \"rtrain/label\", target_transform=target_remap())\n",
    "\n",
    "test_data = dataset(\"rtest/color\", \"rtest/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0].permute(1, 2, 0)\n",
    "# label = train_labels[0].permute(1, 2, 0)\n",
    "# print(torch.unique(label))\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(label, cmap='grey')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(\"tmp/color/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(1,4,512, 512, requires_grad=True)\n",
    "target = torch.empty(1,512, 512, dtype=torch.long).random_(3)\n",
    "print(input.size())\n",
    "print(target.size())\n",
    "output = loss(input, target)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0,1] = 255\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.log(torch.exp(input[0,0,0])/torch.sum(torch.exp(input[0,:,0])))\n",
    "b = torch.log(torch.exp(input[0,1,1])/torch.sum(torch.exp(input[0,:,1])))\n",
    "c = torch.log(torch.exp(input[0,0,2])/torch.sum(torch.exp(input[0,:,2])))\n",
    "d = torch.log(torch.exp(input[0,0,3])/torch.sum(torch.exp(input[0,:,3])))\n",
    "e = torch.log(torch.exp(input[0,1,4])/torch.sum(torch.exp(input[0,:,4])))\n",
    "(a+b+c+d+e)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_names = sorted([os.path.splitext(filename)[0] for filename in os.listdir(img_dir)])\n",
    "        self.len = len(self.img_names)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = decode_image(os.path.join(self.img_dir, self.img_names[idx] + \".jpg\")).float()/255\n",
    "        label = decode_image(os.path.join(self.label_dir, self.img_names[idx] + \".png\"))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "def display_img_label(data, idx):\n",
    "    img, label = data[idx]\n",
    "    figure = plt.figure(figsize=(10,20))\n",
    "    figure.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    figure.add_subplot(1, 2, 2)\n",
    "    plt.imshow(label.permute(1, 2, 0), cmap='grey')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class target_remap(object):\n",
    "    def __call__(self, img):\n",
    "        img[img == 255] = 3\n",
    "        return img\n",
    "\n",
    "def diff_size_collate(batch):\n",
    "    imgs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    return imgs, labels\n",
    "\n",
    "target_batch_size = 64\n",
    "batch_size = 1\n",
    "\n",
    "training_data = dataset(\"rtrain/color\", \"rtrain/label\", target_transform=target_remap())\n",
    "val_data = dataset(\"Val/color\", \"Val/label\", target_transform=target_remap())\n",
    "test_data = dataset(\"Test/color\", \"Test/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=diff_size_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModel\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\" # Hugging Face model identifier\n",
    "\n",
    "# --- 2. Setup Device and Load Model/Processor (Hugging Face) ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load the specific vision model and the processor\n",
    "    vision_model = CLIPVisionModel.from_pretrained(MODEL_NAME).to(device)\n",
    "    processor = CLIPImageProcessor.from_pretrained(MODEL_NAME)\n",
    "    vision_model.eval()\n",
    "    print(f\"Hugging Face CLIP Vision Model '{MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading HF CLIP model: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 4. Get one image from the DataLoader and Perform Forward Pass ---\n",
    "print(\"\\nFetching one image batch from DataLoader...\")\n",
    "\n",
    "# Get the first batch (which contains one image in this case)\n",
    "\n",
    "first_batch_pixel_values, _ = next(iter(train_dataloader))\n",
    "# The DataLoader might return a list or tuple depending on the Dataset's __getitem__\n",
    "# If __getitem__ returns only the tensor, the batch will be just the tensor batch\n",
    "# Move the batch to the correct device\n",
    "pixel_values_batch = first_batch_pixel_values.to(device)\n",
    "print(f\"Image batch loaded. Tensor shape: {pixel_values_batch.shape}\")\n",
    "\n",
    "pixel_values_batch = processor(images=pixel_values_batch, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "print(pixel_values_batch.shape)\n",
    "\n",
    "print(\"\\nPerforming forward pass using output_hidden_states=True...\")\n",
    "with torch.no_grad():\n",
    "    outputs = vision_model(\n",
    "        pixel_values=pixel_values_batch, # Use the batch from DataLoader\n",
    "        output_hidden_states=True       # Request intermediate layer outputs\n",
    "    )\n",
    "print(\"Forward pass completed.\")\n",
    "\n",
    "# --- 5. Access and Print Hidden States ---\n",
    "# outputs.hidden_states is a tuple.\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(f\"\\nCaptured {len(hidden_states)} hidden states for the image from the dataset:\")\n",
    "for i, state_tensor in enumerate(hidden_states):\n",
    "    layer_desc = \"Initial Embeddings\" if i == 0 else f\"Transformer Layer {i} Output\"\n",
    "    # Shape is (batch_size, sequence_length, hidden_size)\n",
    "    print(f\"  State {i} ({layer_desc}) Shape: {state_tensor.shape} (Device: {state_tensor.device})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPVisionModel, CLIPVisionConfig, CLIPImageProcessor\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Choose your CLIP model (adjust if using a different variant)\n",
    "PRETRAINED_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "# Example image size (must be compatible with CLIP's expected input size)\n",
    "# Standard CLIP ViT-B/16 takes 224x224\n",
    "IMG_SIZE = 224\n",
    "# Specify which transformer layers to use for skip connections\n",
    "# Layer indices count from 0 (embeddings) up to num_hidden_layers\n",
    "# Example: For ViT-Base (12 layers), use layers 3, 6, 9, and 12 (last)\n",
    "# Note: hidden_states output includes initial embeddings as layer 0\n",
    "# So, hidden_states[3] is output of layer 2, hidden_states[6] -> layer 5, etc.\n",
    "# Let's target outputs *after* layers 2, 5, 8, 11 (indices 3, 6, 9, 12)\n",
    "SKIP_LAYER_INDICES = [3, 6, 9, 12] # Example for ViT-B/16\n",
    "\n",
    "# --- CLIP ViT Encoder ---\n",
    "class ClipViTEncoder(nn.Module):\n",
    "    def __init__(self, model_name=PRETRAINED_MODEL_NAME, freeze_encoder=True, skip_indices=SKIP_LAYER_INDICES):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.skip_indices = sorted(skip_indices, reverse=True) # Get skips from deeper layers first\n",
    "\n",
    "        # Load CLIP Vision Model configuration and weights\n",
    "        self.config = CLIPVisionConfig.from_pretrained(model_name)\n",
    "        self.clip_vit = CLIPVisionModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for param in self.clip_vit.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Calculate grid size (assuming square patches and square image)\n",
    "        # Note: CLIP uses rectangular patches if image is not square, but\n",
    "        # reshaping assumes square grid. Input should ideally be square.\n",
    "        self.patch_size = self.config.patch_size\n",
    "        self.grid_size = self.config.image_size // self.patch_size\n",
    "        self.hidden_dim = self.config.hidden_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "\n",
    "        print(f\"CLIP ViT Encoder initialized:\")\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        print(f\"  Image Size: {self.config.image_size}x{self.config.image_size}\")\n",
    "        print(f\"  Patch Size: {self.patch_size}x{self.patch_size}\")\n",
    "        print(f\"  Grid Size: {self.grid_size}x{self.grid_size}\")\n",
    "        print(f\"  Hidden Dim: {self.hidden_dim}\")\n",
    "        print(f\"  Using skip connections from layer indices: {self.skip_indices}\")\n",
    "        print(f\"  Encoder frozen: {freeze_encoder}\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (B, C, H, W).\n",
    "                               Make sure H, W match CLIP's expected size (e.g., 224x224)\n",
    "                               and C=3. Preprocessing (normalization) should match CLIP's.\n",
    "        Returns:\n",
    "            tuple: (bottleneck_features, skip_features_list)\n",
    "                   - bottleneck_features (torch.Tensor): Reshaped features from the final layer.\n",
    "                                                        Shape: (B, hidden_dim, grid_size, grid_size)\n",
    "                   - skip_features_list (list[torch.Tensor]): List of reshaped features\n",
    "                                                             from specified intermediate layers.\n",
    "                                                             Ordered from deepest to shallowest skip.\n",
    "                                                             Each shape: (B, hidden_dim, grid_size, grid_size)\n",
    "        \"\"\"\n",
    "        if x.shape[2] != self.config.image_size or x.shape[3] != self.config.image_size:\n",
    "             warnings.warn(\n",
    "                 f\"Input image size ({x.shape[2]}x{x.shape[3]}) doesn't match \"\n",
    "                 f\"CLIP expected size ({self.config.image_size}x{self.config.image_size}). \"\n",
    "                 f\"Behavior may be unexpected. Consider resizing input.\"\n",
    "             )\n",
    "\n",
    "        # Pass image through CLIP ViT\n",
    "        # output_hidden_states=True is crucial\n",
    "        outputs = self.clip_vit(pixel_values=x, output_hidden_states=True)\n",
    "\n",
    "        # `hidden_states` is a tuple: (embeddings, hidden_state_layer_1, ..., hidden_state_layer_N)\n",
    "        # Length is num_hidden_layers + 1\n",
    "        all_hidden_states = outputs.hidden_states\n",
    "\n",
    "        # --- Extract and Reshape Features ---\n",
    "        skip_features_list = []\n",
    "        bottleneck_features = None\n",
    "\n",
    "        # We iterate through specified layers to extract skips\n",
    "        # The deepest specified layer serves as the bottleneck input for the decoder\n",
    "        is_bottleneck_set = False\n",
    "        for i in self.skip_indices:\n",
    "            hidden_state = all_hidden_states[i] # Shape: (B, num_patches + 1, hidden_dim)\n",
    "\n",
    "            # Remove the CLS token embedding (first token)\n",
    "            # Shape -> (B, num_patches, hidden_dim)\n",
    "            patch_embeddings = hidden_state[:, 1:, :]\n",
    "\n",
    "            # Check if num_patches matches calculated grid size squared\n",
    "            if patch_embeddings.shape[1] != self.num_patches:\n",
    "                 # This might happen if image size is not what model expected,\n",
    "                 # or if non-square patches/images lead to different seq lengths.\n",
    "                 current_num_patches = patch_embeddings.shape[1]\n",
    "                 current_grid_size_h = int(sqrt(current_num_patches)) # Approx calc\n",
    "                 current_grid_size_w = current_num_patches // current_grid_size_h\n",
    "                 if current_grid_size_h * current_grid_size_w != current_num_patches:\n",
    "                     raise ValueError(f\"Cannot reliably reshape patch embeddings. \"\n",
    "                                      f\"Expected {self.num_patches} patches, got {current_num_patches}.\")\n",
    "                 # print(f\"Warning: Actual patch count {current_num_patches} differs from expected {self.num_patches}. Trying to reshape to {current_grid_size_h}x{current_grid_size_w}.\")\n",
    "                 # Reshape to (B, grid_h, grid_w, C)\n",
    "                 reshaped_features = patch_embeddings.reshape(\n",
    "                     x.shape[0], current_grid_size_h, current_grid_size_w, self.hidden_dim\n",
    "                 )\n",
    "                 # Permute to (B, C, grid_h, grid_w) - PyTorch format\n",
    "                 reshaped_features = reshaped_features.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            else:\n",
    "                 # Reshape to (B, grid_size, grid_size, C)\n",
    "                 reshaped_features = patch_embeddings.reshape(\n",
    "                     x.shape[0], self.grid_size, self.grid_size, self.hidden_dim\n",
    "                 )\n",
    "                 # Permute to (B, C, grid_size, grid_size) - PyTorch format\n",
    "                 reshaped_features = reshaped_features.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "\n",
    "            if not is_bottleneck_set:\n",
    "                bottleneck_features = reshaped_features\n",
    "                is_bottleneck_set = True\n",
    "            else:\n",
    "                 # Add to skips (these come from shallower layers than bottleneck)\n",
    "                skip_features_list.append(reshaped_features)\n",
    "\n",
    "        # The skip list is currently deepest -> shallowest. Reverse it?\n",
    "        # No, typically the decoder expects skips from shallow -> deep encoder layers\n",
    "        # But our indices were sorted reverse, so bottleneck = deepest, list = next deeper -> shallowest\n",
    "        # Standard U-Net decoder takes skips shallowest first. Let's keep list order as is (deep->shallow)\n",
    "        # and the decoder can pop() or index accordingly. Or reverse here.\n",
    "        # Let's reverse to match typical U-Net expectation (shallowest first in list)\n",
    "        skip_features_list.reverse()\n",
    "\n",
    "        if bottleneck_features is None:\n",
    "             raise ValueError(\"Could not extract bottleneck features. Check skip_indices.\")\n",
    "\n",
    "        return bottleneck_features, skip_features_list\n",
    "\n",
    "# --- Example U-Net Decoder Block ---\n",
    "# This needs to be adapted based on the SKIP connection properties\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels_upsample, in_channels_skip, out_channels, skip_spatial_size, target_spatial_size):\n",
    "        super().__init__()\n",
    "        self.skip_spatial_size = skip_spatial_size\n",
    "        self.target_spatial_size = target_spatial_size\n",
    "\n",
    "        # Upsample the input from the previous decoder layer\n",
    "        # Using ConvTranspose2d, factor should align target_spatial_size / prev_size\n",
    "        # Or use nn.Upsample + Conv2d\n",
    "        # Assuming stride 2 for doubling resolution typically\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels_upsample, in_channels_upsample // 2, kernel_size=2, stride=2)\n",
    "        # Adjust number of channels in the skip connection if needed\n",
    "        # All our ViT skips have hidden_dim channels and grid_size x grid_size spatial dim\n",
    "        self.skip_conv = nn.Conv2d(in_channels_skip, in_channels_upsample // 2, kernel_size=1) # 1x1 conv\n",
    "        # Double convolution block after concatenation\n",
    "        conv_in_channels = in_channels_upsample // 2 + in_channels_upsample // 2 # After concat\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(conv_in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_upsample, x_skip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_upsample: Feature map from the previous (deeper) decoder layer\n",
    "            x_skip: Feature map (skip connection) from the corresponding encoder layer\n",
    "        \"\"\"\n",
    "        x_upsample = self.upsample(x_upsample) # Upsample (e.g., H/2, W/2 -> H, W)\n",
    "\n",
    "        # Adapt skip connection channels\n",
    "        x_skip = self.skip_conv(x_skip) # Projects skip channels\n",
    "\n",
    "        # --- Spatial Dimension Handling (CRITICAL) ---\n",
    "        # The ViT skips might all be grid_size x grid_size.\n",
    "        # The upsampled features x_upsample will have increasing resolution.\n",
    "        # We need to ensure they match before concatenation.\n",
    "\n",
    "        # Option 1: Resize skip connection to match x_upsample (may lose info)\n",
    "        if x_skip.shape[2:] != x_upsample.shape[2:]:\n",
    "              x_skip = F.interpolate(x_skip, size=x_upsample.shape[2:], mode='bilinear', align_corners=False)\n",
    "              # print(f\"Resized skip connection from {self.skip_spatial_size}x{self.skip_spatial_size} to {x_upsample.shape[2:]}\")\n",
    "\n",
    "\n",
    "        # Option 2: Design decoder so x_upsample always matches skip size (less standard U-Net)\n",
    "\n",
    "\n",
    "        # Concatenate along the channel dimension\n",
    "        x = torch.cat([x_upsample, x_skip], dim=1)\n",
    "\n",
    "        # Apply convolutions\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Example U-Net Decoder ---\n",
    "# This needs careful design based on the number of skips and their properties\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, encoder_grid_size, decoder_channels):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim  # Channels in ViT features\n",
    "        self.encoder_grid_size = encoder_grid_size   # Spatial size of ViT features (H/P, W/P)\n",
    "        self.decoder_channels = decoder_channels     # e.g., [512, 256, 128, 64] - channels *after* each block\n",
    "\n",
    "        # Example: 4 decoder blocks corresponding to 4 skip connections (+ bottleneck)\n",
    "        num_blocks = len(decoder_channels)\n",
    "        if num_blocks != len(SKIP_LAYER_INDICES) -1: # Need one less block than total skips extracted if one is bottleneck\n",
    "            warnings.warn(f\"Number of decoder blocks ({num_blocks}) \"\n",
    "                           f\"doesn't match number of skip connection layers ({len(SKIP_LAYER_INDICES) -1}). Adjust config.\")\n",
    "            # Adjusting num_blocks based on provided decoder_channels\n",
    "            num_blocks = len(decoder_channels)\n",
    "\n",
    "\n",
    "        # Determine input channels for each block\n",
    "        # Bottleneck has encoder_hidden_dim channels\n",
    "        # First block takes bottleneck (upsampled) + deepest skip (projected)\n",
    "        # Subsequent blocks take previous block output (upsampled) + next skip (projected)\n",
    "        in_channels_upsample = encoder_hidden_dim # From bottleneck initially\n",
    "        block_input_sizes = [] # To track expected spatial sizes\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        current_spatial_size = encoder_grid_size\n",
    "        for i in range(num_blocks):\n",
    "            out_ch = decoder_channels[i]\n",
    "            # All ViT skips have encoder_hidden_dim channels and encoder_grid_size spatial size\n",
    "            in_channels_skip = encoder_hidden_dim\n",
    "            skip_spatial_size = encoder_grid_size\n",
    "\n",
    "            # Target size after upsampling (typically doubles)\n",
    "            target_spatial_size = current_spatial_size * 2 # Assuming stride-2 upsampling\n",
    "\n",
    "            block = DecoderBlock(\n",
    "                in_channels_upsample=in_channels_upsample,\n",
    "                in_channels_skip=in_channels_skip,\n",
    "                out_channels=out_ch,\n",
    "                skip_spatial_size=skip_spatial_size,\n",
    "                target_spatial_size=target_spatial_size\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "            # Input channels for the *next* block's upsample path is the output of this block\n",
    "            in_channels_upsample = out_ch\n",
    "            current_spatial_size = target_spatial_size # Update expected spatial size\n",
    "\n",
    "    def forward(self, bottleneck_features, skip_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bottleneck_features (torch.Tensor): Output from the deepest ViT layer used.\n",
    "                                                Shape: (B, encoder_hidden_dim, G, G)\n",
    "            skip_features (list[torch.Tensor]): List of skip features from ViT,\n",
    "                                                ordered shallowest layer first.\n",
    "                                                Each: (B, encoder_hidden_dim, G, G)\n",
    "        Returns:\n",
    "            torch.Tensor: Output feature map from the final decoder block.\n",
    "        \"\"\"\n",
    "        x = bottleneck_features\n",
    "        # skip_features list is ordered shallowest -> deepest\n",
    "        # We need to pair decoder blocks (deep -> shallow) with skips (deep -> shallow)\n",
    "        # Let's assume skip_features were reversed by encoder: shallowest first in list\n",
    "        # Decoder blocks run from deep -> shallow output\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            # Get the corresponding skip connection\n",
    "            # If list is shallow->deep, i=0 block (deepest) uses skip_features[-1] (deepest skip)\n",
    "            # Let's reverse the list index access\n",
    "            skip = skip_features[len(skip_features) - 1 - i]\n",
    "            x = block(x, skip) # Pass current features and corresponding skip\n",
    "        return x\n",
    "\n",
    "# --- Combined CLIP-U-Net Model ---\n",
    "class ClipUNet(nn.Module):\n",
    "    def __init__(self, num_classes=4, decoder_channels=[512, 256, 128, 64], freeze_encoder=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = ClipViTEncoder(freeze_encoder=freeze_encoder)\n",
    "\n",
    "        self.decoder = UNetDecoder(\n",
    "            encoder_hidden_dim=self.encoder.hidden_dim,\n",
    "            encoder_grid_size=self.encoder.grid_size,\n",
    "            decoder_channels=decoder_channels\n",
    "        )\n",
    "\n",
    "        # Final 1x1 convolution to map to the number of classes\n",
    "        self.final_conv = nn.Conv2d(decoder_channels[-1], num_classes, kernel_size=1)\n",
    "\n",
    "        # Potentially add a final upsampling layer if the last decoder block's output\n",
    "        # resolution doesn't match the original image size.\n",
    "        # The last decoder block outputs at encoder_grid_size * 2^num_blocks\n",
    "        final_decoder_size = self.encoder.grid_size * (2**len(decoder_channels))\n",
    "        if final_decoder_size != IMG_SIZE:\n",
    "             print(f\"Final decoder size {final_decoder_size} != Input Image Size {IMG_SIZE}. Adding final upsample.\")\n",
    "             self.final_upsample = nn.Upsample(size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "             self.final_upsample = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is preprocessed correctly for CLIP!\n",
    "        # (Resize to 224x224, normalize with CLIP's mean/std)\n",
    "        \n",
    "        bottleneck, skips = self.encoder(x)\n",
    "        decoder_output = self.decoder(bottleneck, skips)\n",
    "        \n",
    "        # Final processing\n",
    "        output = self.final_conv(decoder_output)\n",
    "        output = self.final_upsample(output) # Upsample to original image size if needed\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = CLIPImageProcessor.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "batch = processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "# Instantiate the model\n",
    "model = ClipUNet(num_classes=4, decoder_channels=[512, 256, 128, 64]).to(device) # Example for 4 skips\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad(): # Use no_grad if just inferencing or checking shapes\n",
    "        output = model(batch)\n",
    "\n",
    "print(f\"\\nInput shape: {batch.shape}\")\n",
    "print(f\"Output shape: {output.shape}\") # Should be (B, num_classes, H, W) e.g., (2, 5, 224, 224)\n",
    "\n",
    "# --- Example: Manually check encoder outputs ---\n",
    "print(\"\\n--- Manual Encoder Check ---\")\n",
    "encoder = ClipViTEncoder().to(device)\n",
    "bottleneck, skips = encoder(batch)\n",
    "print(f\"Bottleneck shape: {bottleneck.shape}\") # e.g., (2, 768, 14, 14) for ViT-B/16\n",
    "print(f\"Number of skip features: {len(skips)}\")\n",
    "for i, skip in enumerate(skips):\n",
    "    print(f\"  Skip {i} (from shallowest layer used) shape: {skip.shape}\") # e.g., (2, 768, 14, 14)\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPVisionModel, CLIPVisionConfig, CLIPImageProcessor\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Choose your CLIP model (adjust if using a different variant)\n",
    "PRETRAINED_MODEL_NAME = \"openai/clip-vit-base-patch16\" # CHANGED to patch32\n",
    "# Example image size (must be compatible with CLIP's expected input size)\n",
    "IMG_SIZE = 224 # Standard for CLIP\n",
    "# Specify which transformer layers to use for skip connections\n",
    "# Using 4 layers: Deepest (12) is bottleneck, others (3, 6, 9) are skips\n",
    "SKIP_LAYER_INDICES = [3, 5, 7, 9, 12]\n",
    "\n",
    "# --- CLIP ViT Encoder ---\n",
    "class ClipViTEncoder(nn.Module):\n",
    "    def __init__(self, model_name=PRETRAINED_MODEL_NAME, freeze_encoder=True, skip_indices=SKIP_LAYER_INDICES):\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing ClipViTEncoder ---\")\n",
    "        self.model_name = model_name\n",
    "        self.skip_indices = sorted(skip_indices, reverse=True) # Process deeper layers first for bottleneck logic\n",
    "\n",
    "        # Load CLIP Vision Model configuration and weights\n",
    "        self.config = CLIPVisionConfig.from_pretrained(model_name)\n",
    "        self.clip_vit = CLIPVisionModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for param in self.clip_vit.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Calculate grid size\n",
    "        self.patch_size = self.config.patch_size\n",
    "        # Ensure image size in config matches expected if not default\n",
    "        if self.config.image_size != IMG_SIZE:\n",
    "             warnings.warn(f\"Model config image size {self.config.image_size} differs from specified IMG_SIZE {IMG_SIZE}. Using model config size for grid calculation.\")\n",
    "        self.grid_size = self.config.image_size // self.patch_size\n",
    "        self.hidden_dim = self.config.hidden_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "\n",
    "        # print(f\"  Model: {model_name}\")\n",
    "        # print(f\"  Expected Input Image Size (from config): {self.config.image_size}x{self.config.image_size}\")\n",
    "        # print(f\"  Patch Size: {self.patch_size}x{self.patch_size}\")\n",
    "        # print(f\"  Calculated Grid Size (G): {self.grid_size}x{self.grid_size}\")\n",
    "        # print(f\"  Calculated Num Patches (N_p = G*G): {self.num_patches}\")\n",
    "        # print(f\"  Hidden Dim (D_vit): {self.hidden_dim}\")\n",
    "        # print(f\"  Targeting hidden states at indices: {self.skip_indices}\")\n",
    "        # print(f\"  Encoder frozen: {freeze_encoder}\")\n",
    "        # print(\"--- ClipViTEncoder Initialized ---\\n\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"\\n--- ClipViTEncoder Forward ---\")\n",
    "        # print(f\"  Input shape: {x.shape}\")\n",
    "        if x.shape[2] != self.config.image_size or x.shape[3] != self.config.image_size:\n",
    "             warnings.warn(\n",
    "                 f\"Input image size ({x.shape[2]}x{x.shape[3]}) doesn't match \"\n",
    "                 f\"CLIP expected size ({self.config.image_size}x{self.config.image_size}). \"\n",
    "                 f\"Behavior may be unexpected. Consider resizing input.\"\n",
    "             )\n",
    "\n",
    "        # Pass image through CLIP ViT\n",
    "        outputs = self.clip_vit(pixel_values=x, output_hidden_states=True)\n",
    "        all_hidden_states = outputs.hidden_states\n",
    "        # print(f\"  Number of hidden states outputs (incl. embeddings): {len(all_hidden_states)}\")\n",
    "        # if all_hidden_states:\n",
    "            # print(f\"  Shape of one hidden state (e.g., index 0 - embeddings): {all_hidden_states[0].shape}\") # e.g., (B, N_p+1, D_vit)\n",
    "\n",
    "        # --- Extract and Reshape Features ---\n",
    "        skip_features_list = []\n",
    "        bottleneck_features = None\n",
    "        is_bottleneck_set = False\n",
    "\n",
    "        # print(\"  Extracting features from specified hidden state indices:\")\n",
    "        for i in self.skip_indices: # Iterates [12, 9, 6, 3]\n",
    "            # print(f\"    Processing hidden state index: {i}\")\n",
    "            hidden_state = all_hidden_states[i]\n",
    "            # print(f\"      Original hidden_state shape: {hidden_state.shape}\") # Should be (B, N_p+1, D_vit)\n",
    "\n",
    "            # Remove the CLS token embedding\n",
    "            patch_embeddings = hidden_state[:, 1:, :]\n",
    "            # print(f\"      Shape after removing CLS token: {patch_embeddings.shape}\") # Should be (B, N_p, D_vit)\n",
    "\n",
    "            # Check num_patches consistency\n",
    "            if patch_embeddings.shape[1] != self.num_patches:\n",
    "                 # Handle potential mismatch (e.g., if input size wasn't exactly config.image_size)\n",
    "                 current_num_patches = patch_embeddings.shape[1]\n",
    "                 current_grid_size = int(sqrt(current_num_patches))\n",
    "                 if current_grid_size * current_grid_size != current_num_patches:\n",
    "                     raise ValueError(f\"Cannot reliably reshape patch embeddings. Non-square grid? Expected {self.num_patches} patches, got {current_num_patches}.\")\n",
    "                 warnings.warn(f\"Actual patch count {current_num_patches} differs from expected {self.num_patches}. Reshaping to {current_grid_size}x{current_grid_size}.\")\n",
    "                 grid_h, grid_w = current_grid_size, current_grid_size\n",
    "            else:\n",
    "                 grid_h, grid_w = self.grid_size, self.grid_size\n",
    "\n",
    "            # Reshape to (B, grid_h, grid_w, D_vit) -> (B, D_vit, grid_h, grid_w)\n",
    "            reshaped_features = patch_embeddings.reshape(\n",
    "                x.shape[0], grid_h, grid_w, self.hidden_dim\n",
    "            )\n",
    "            reshaped_features = reshaped_features.permute(0, 3, 1, 2).contiguous()\n",
    "            # print(f\"      Shape after reshaping to grid (B, D_vit, G, G): {reshaped_features.shape}\")\n",
    "\n",
    "            if not is_bottleneck_set:\n",
    "                # print(f\"      --> Assigning to bottleneck_features\")\n",
    "                bottleneck_features = reshaped_features\n",
    "                is_bottleneck_set = True\n",
    "            else:\n",
    "                # print(f\"      --> Appending to intermediate skip list\")\n",
    "                skip_features_list.append(reshaped_features)\n",
    "\n",
    "        # Reverse the collected skips to be shallowest feature first\n",
    "        skip_features_list.reverse() # Now order matches [index_3_out, index_6_out, index_9_out]\n",
    "        # print(f\"  Finished extracting features.\")\n",
    "        # print(f\"    Bottleneck features final shape: {bottleneck_features.shape if bottleneck_features is not None else 'None'}\")\n",
    "        # print(f\"    Number of skip features collected: {len(skip_features_list)}\")\n",
    "        # for idx, skip in enumerate(skip_features_list):\n",
    "            # print(f\"      Skip {idx} (shallowest first) final shape: {skip.shape}\")\n",
    "\n",
    "        if bottleneck_features is None:\n",
    "             raise ValueError(\"Could not extract bottleneck features. Check skip_indices.\")\n",
    "\n",
    "        # print(\"--- ClipViTEncoder Forward End ---\")\n",
    "        return bottleneck_features, skip_features_list\n",
    "\n",
    "# --- Example U-Net Decoder Block ---\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, block_index, in_channels_upsample, in_channels_skip, out_channels, skip_spatial_size, target_spatial_size):\n",
    "        super().__init__()\n",
    "        self.block_index = block_index\n",
    "        # print(f\"    Initializing DecoderBlock {self.block_index}: Upsample_in={in_channels_upsample}, Skip_in={in_channels_skip}, Out={out_channels}\")\n",
    "        self.skip_spatial_size = skip_spatial_size\n",
    "        self.target_spatial_size = target_spatial_size\n",
    "        self.upsample_out_channels = in_channels_upsample // 2\n",
    "        self.skip_conv_out_channels = in_channels_upsample // 2 # Match for concatenation\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(in_channels_upsample, self.upsample_out_channels, kernel_size=2, stride=2)\n",
    "        self.skip_conv = nn.Conv2d(in_channels_skip, self.skip_conv_out_channels, kernel_size=1) # 1x1 conv\n",
    "\n",
    "        conv_in_channels = self.upsample_out_channels + self.skip_conv_out_channels\n",
    "        # print(f\"      Block {self.block_index}: Upsample out channels={self.upsample_out_channels}, SkipConv out channels={self.skip_conv_out_channels}, Concat channels={conv_in_channels}\")\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(conv_in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_upsample, x_skip):\n",
    "        # print(f\"\\n  --- DecoderBlock {self.block_index} Forward ---\")\n",
    "        # print(f\"    Input x_upsample shape: {x_upsample.shape}\")\n",
    "        # print(f\"    Input x_skip shape: {x_skip.shape}\")\n",
    "\n",
    "        x_upsample = self.upsample(x_upsample)\n",
    "        # print(f\"    Shape after upsample: {x_upsample.shape}\")\n",
    "\n",
    "        x_skip_proj = self.skip_conv(x_skip)\n",
    "        # print(f\"    Shape after skip_conv (channel adjust): {x_skip_proj.shape}\")\n",
    "\n",
    "        # Resize skip connection spatially if needed\n",
    "        if x_skip_proj.shape[2:] != x_upsample.shape[2:]:\n",
    "            #   print(f\"    Resizing skip connection from {x_skip_proj.shape[2:]} to {x_upsample.shape[2:]}\")\n",
    "              x_skip_resized = F.interpolate(x_skip_proj, size=x_upsample.shape[2:], mode='bilinear', align_corners=False)\n",
    "            #   print(f\"    Shape after skip resize: {x_skip_resized.shape}\")\n",
    "        else:\n",
    "              x_skip_resized = x_skip_proj\n",
    "            #   print(f\"    Skip connection spatial size matches upsample, no resize needed.\")\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x_upsample, x_skip_resized], dim=1)\n",
    "        # print(f\"    Shape after concatenation: {x.shape}\")\n",
    "\n",
    "        # Apply convolutions\n",
    "        x = self.conv_block(x)\n",
    "        # print(f\"    Shape after conv_block (Output): {x.shape}\")\n",
    "        # print(f\"  --- DecoderBlock {self.block_index} Forward End ---\")\n",
    "        return x\n",
    "\n",
    "# --- Example U-Net Decoder ---\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, encoder_grid_size, decoder_channels):\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing UNetDecoder ---\")\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.encoder_grid_size = encoder_grid_size\n",
    "        self.decoder_channels = decoder_channels\n",
    "        num_blocks = len(decoder_channels)\n",
    "\n",
    "        # *** Important Check ***\n",
    "        num_expected_skips = len(SKIP_LAYER_INDICES) - 1 # One layer is bottleneck\n",
    "        # print(f\"  Encoder provides {num_expected_skips} skip connections (indices besides bottleneck).\")\n",
    "        # print(f\"  Decoder configured with {num_blocks} blocks for channels: {decoder_channels}.\")\n",
    "        if num_blocks != num_expected_skips:\n",
    "            warnings.warn(\n",
    "               f\"*** POTENTIAL MISMATCH: Number of decoder blocks ({num_blocks}) \"\n",
    "               f\"does not match the number of available skip connections ({num_expected_skips}). \"\n",
    "               f\"This might lead to an IndexError during forward pass if not handled carefully. ***\"\n",
    "            )\n",
    "        # The code proceeds to create num_blocks based on decoder_channels length\n",
    "\n",
    "        in_channels_upsample = encoder_hidden_dim\n",
    "        self.blocks = nn.ModuleList()\n",
    "        current_spatial_size = encoder_grid_size\n",
    "        # print(f\"  Creating {num_blocks} DecoderBlock(s):\")\n",
    "        for i in range(num_blocks):\n",
    "            out_ch = decoder_channels[i]\n",
    "            in_channels_skip = encoder_hidden_dim\n",
    "            skip_spatial_size = encoder_grid_size\n",
    "            target_spatial_size = current_spatial_size * 2 # Assumes stride-2 upsampling\n",
    "\n",
    "            # Pass index for clearer prints inside block\n",
    "            block = DecoderBlock(\n",
    "                block_index=i,\n",
    "                in_channels_upsample=in_channels_upsample,\n",
    "                in_channels_skip=in_channels_skip,\n",
    "                out_channels=out_ch,\n",
    "                skip_spatial_size=skip_spatial_size,\n",
    "                target_spatial_size=target_spatial_size\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "            in_channels_upsample = out_ch # Next block's upsample input is this block's output\n",
    "            current_spatial_size = target_spatial_size\n",
    "\n",
    "        # print(\"--- UNetDecoder Initialized ---\\n\")\n",
    "\n",
    "\n",
    "    def forward(self, bottleneck_features, skip_features):\n",
    "        # print(\"\\n--- UNetDecoder Forward ---\")\n",
    "        # print(f\"  Input bottleneck_features shape: {bottleneck_features.shape}\")\n",
    "        # print(f\"  Input skip_features list length: {len(skip_features)}\")\n",
    "        x = bottleneck_features\n",
    "        num_available_skips = len(skip_features)\n",
    "\n",
    "        # Iterate through decoder blocks (indices 0 to num_blocks-1)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            # print(f\"  Processing Decoder Block {i}\")\n",
    "            # Calculate index for skip_features list (shallowest first)\n",
    "            # Block 0 (deepest decoder) uses last skip (deepest skip feature)\n",
    "            skip_idx = num_available_skips - 1 - i\n",
    "            # print(f\"    Attempting to use skip feature at index: {skip_idx} (from list of {num_available_skips})\")\n",
    "\n",
    "            if skip_idx < 0 or skip_idx >= num_available_skips:\n",
    "                 # This will happen if num_blocks > num_available_skips\n",
    "                 raise IndexError(f\"Attempted to access skip_features[{skip_idx}] but only {num_available_skips} skips are available. Mismatch between decoder blocks and encoder skips.\")\n",
    "\n",
    "            skip = skip_features[skip_idx]\n",
    "            # print(f\"    Using skip feature with shape: {skip.shape}\")\n",
    "            x = block(x, skip) # Pass current features and corresponding skip\n",
    "\n",
    "        # print(f\"  Final output shape from UNetDecoder: {x.shape}\")\n",
    "        # print(\"--- UNetDecoder Forward End ---\")\n",
    "        return x\n",
    "\n",
    "# --- Combined CLIP-U-Net Model ---\n",
    "class ClipUNet(nn.Module):\n",
    "    def __init__(self, num_classes=4, decoder_channels=[512, 256, 128, 64], freeze_encoder=True):\n",
    "        super().__init__()\n",
    "        # print(\"\\n--- Initializing ClipUNet ---\")\n",
    "        # print(f\"  Num output classes: {num_classes}\")\n",
    "        # print(f\"  Decoder channels specified: {decoder_channels}\")\n",
    "\n",
    "        self.encoder = ClipViTEncoder(freeze_encoder=freeze_encoder)\n",
    "\n",
    "        self.decoder = UNetDecoder(\n",
    "            encoder_hidden_dim=self.encoder.hidden_dim,\n",
    "            encoder_grid_size=self.encoder.grid_size,\n",
    "            decoder_channels=decoder_channels\n",
    "        )\n",
    "\n",
    "        # Final 1x1 convolution\n",
    "        last_decoder_channel = decoder_channels[-1] if decoder_channels else self.encoder.hidden_dim # Handle no decoder case?\n",
    "        # print(f\"  Final Conv: {last_decoder_channel} -> {num_classes} channels\")\n",
    "        self.final_conv = nn.Conv2d(last_decoder_channel, num_classes, kernel_size=1)\n",
    "\n",
    "        # Final upsampling layer check\n",
    "        final_decoder_size = self.encoder.grid_size * (2**len(decoder_channels))\n",
    "        # print(f\"  Calculated final decoder spatial size: {final_decoder_size}x{final_decoder_size}\")\n",
    "        if final_decoder_size != IMG_SIZE:\n",
    "            #  print(f\"  Final decoder size {final_decoder_size} != Target Image Size {IMG_SIZE}. Adding final Upsample layer.\")\n",
    "             self.final_upsample = nn.Upsample(size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            #  print(f\"  Final decoder size matches target image size. Final upsample is Identity.\")\n",
    "             self.final_upsample = nn.Identity()\n",
    "\n",
    "        # print(\"--- ClipUNet Initialized ---\\n\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"\\n\\n========== ClipUNet Forward Pass Start ==========\")\n",
    "        # print(f\"Overall Input shape: {x.shape}\")\n",
    "\n",
    "        # 1. Encoder\n",
    "        bottleneck, skips = self.encoder(x)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Encoder) ---\")\n",
    "        # print(f\"  Encoder returned bottleneck shape: {bottleneck.shape}\")\n",
    "        # print(f\"  Encoder returned {len(skips)} skip features.\")\n",
    "\n",
    "        # 2. Decoder\n",
    "        decoder_output = self.decoder(bottleneck, skips)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Decoder) ---\")\n",
    "        # print(f\"  Decoder returned output shape: {decoder_output.shape}\")\n",
    "\n",
    "        # 3. Final Convolution\n",
    "        output = self.final_conv(decoder_output)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Final Conv) ---\")\n",
    "        # print(f\"  Shape after final_conv: {output.shape}\")\n",
    "\n",
    "        # 4. Final Upsampling (if needed)\n",
    "        output = self.final_upsample(output)\n",
    "        # print(\"\\n--- Back in ClipUNet Forward (after Final Upsample) ---\")\n",
    "        # print(f\"  Shape after final_upsample: {output.shape}\")\n",
    "\n",
    "        # print(\"========== ClipUNet Forward Pass End ==========\\n\")\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration for the run\n",
    "BATCH_SIZE = 2 # Smaller batch size for testing\n",
    "NUM_CLASSES = 4\n",
    "# *** NOTE: This configuration will likely cause an error! ***\n",
    "# We request 4 decoder blocks ([512, 256, 128, 64])\n",
    "# But provide only 3 skips (from layers 3, 6, 9, with 12 as bottleneck)\n",
    "DECODER_CHANNELS_CONFIG = [512, 256, 128, 64]\n",
    "\n",
    "print(\"\\n--- Preparing Example Data ---\")\n",
    "# Use CLIPProcessor to get correct preprocessing\n",
    "try:\n",
    "    processor = CLIPImageProcessor.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    # Create dummy images (replace with your actual data loading)\n",
    "    # Generating random images often works, but using processor is better practice\n",
    "    images, labels = next(iter(train_dataloader))\n",
    "    # Process images\n",
    "    batch_processed = processor(images=images, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    print(f\"  Processed batch shape: {batch_processed.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error loading processor or processing dummy images: {e}\")\n",
    "    print(f\"  Falling back to simple random tensor.\")\n",
    "    # Fallback simple tensor\n",
    "    batch_processed = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "    print(f\"  Using dummy tensor input shape: {batch_processed.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Instantiating Model ---\")\n",
    "# Instantiate the model\n",
    "model = ClipUNet(num_classes=NUM_CLASSES,\n",
    "                    decoder_channels=DECODER_CHANNELS_CONFIG, # Using the potentially problematic config\n",
    "                    freeze_encoder=True).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "print(\"\\n--- Running Forward Pass ---\")\n",
    "try:\n",
    "    # Forward pass\n",
    "    model.eval() # Set to evaluation mode\n",
    "    with torch.no_grad(): # Use no_grad for inference example\n",
    "        output = model(batch_processed)\n",
    "\n",
    "    print(f\"\\n--- Forward Pass Successful ---\")\n",
    "    print(f\"Final Output shape: {output.shape}\") # Should be (B, num_classes, H, W)\n",
    "    # Expected: (2, 4, 224, 224) if no error occurs\n",
    "except IndexError as e:\n",
    "        print(f\"\\n--- EXPECTED INDEX ERROR OCCURRED ---\")\n",
    "        print(f\"  Error message: {e}\")\n",
    "        print(f\"  This likely happened because the number of decoder blocks ({len(DECODER_CHANNELS_CONFIG)})\")\n",
    "        print(f\"  is greater than the number of available skip connections ({len(SKIP_LAYER_INDICES) - 1}).\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- UNEXPECTED ERROR DURING FORWARD PASS ---\")\n",
    "    print(f\"  Error type: {type(e)}\")\n",
    "    print(f\"  Error message: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
