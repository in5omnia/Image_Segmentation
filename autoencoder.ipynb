{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.665275Z",
     "start_time": "2025-03-11T16:48:54.644297Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from dataset import *\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "training_data = dataset(\"../ResizedTrainVal/color\", \"../ResizedTrainVal/label\", target_transform=target_remap())\n",
    "#test_data = dataset(\"Test/color\", \"Test/label\", target_transform=target_remap())\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                             #collate_fn=diff_size_collate)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.672030Z",
     "start_time": "2025-03-11T16:48:54.669422Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np\n",
   "id": "32026ffa6859c3f4",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.687341Z",
     "start_time": "2025-03-11T16:48:54.681537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, din):\n",
    "        super().__init__()\n",
    "        self.encoderPart1 = EncoderPart(din, 64)\n",
    "        self.encoderPart2 = EncoderPart(64, 32)\n",
    "        self.encoderPart3 = EncoderPart(32, 16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoderPart1(x)\n",
    "        x = self.encoderPart2(x)\n",
    "        x = self.encoderPart3(x)\n",
    "        return x\n",
    "\n",
    "class DecoderPart(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(din, dout, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dout):\n",
    "        super().__init__()\n",
    "        self.decoderPart1 = DecoderPart(16, 16)\n",
    "        self.decoderPart2 = DecoderPart(16, 32)\n",
    "        self.decoderPart3 = DecoderPart(32, 64)\n",
    "        self.decoderOut = nn.Conv2d(64, dout, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoderPart1(x)\n",
    "        x = self.decoderPart2(x)\n",
    "        x = self.decoderPart3(x)\n",
    "        x = self.decoderOut(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(din)\n",
    "        self.decoder = Decoder(dout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        "
   ],
   "id": "e76fb997cd390592",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.886132Z",
     "start_time": "2025-03-11T16:48:54.695476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Autoencoder(3, 3)\n",
    "image, label = training_data[0]\n",
    "print(image)\n",
    "print(image.size())\n",
    "pred = model(image)\n",
    "print(pred)"
   ],
   "id": "17f918ffff43a85a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "torch.Size([3, 512, 512])\n",
      "tensor([[[-0.0512, -0.0411, -0.0421,  ..., -0.0427, -0.0423, -0.0391],\n",
      "         [-0.0534, -0.0469, -0.0478,  ..., -0.0496, -0.0473, -0.0481],\n",
      "         [-0.0534, -0.0475, -0.0489,  ..., -0.0506, -0.0475, -0.0488],\n",
      "         ...,\n",
      "         [-0.0539, -0.0482, -0.0497,  ..., -0.0511, -0.0482, -0.0492],\n",
      "         [-0.0555, -0.0524, -0.0525,  ..., -0.0532, -0.0496, -0.0486],\n",
      "         [-0.0391, -0.0361, -0.0370,  ..., -0.0387, -0.0366, -0.0397]],\n",
      "\n",
      "        [[ 0.0296,  0.0400,  0.0395,  ...,  0.0379,  0.0392,  0.0291],\n",
      "         [ 0.0340,  0.0455,  0.0431,  ...,  0.0447,  0.0474,  0.0326],\n",
      "         [ 0.0353,  0.0501,  0.0471,  ...,  0.0487,  0.0519,  0.0336],\n",
      "         ...,\n",
      "         [ 0.0345,  0.0486,  0.0451,  ...,  0.0487,  0.0523,  0.0339],\n",
      "         [ 0.0333,  0.0464,  0.0437,  ...,  0.0482,  0.0514,  0.0355],\n",
      "         [ 0.0327,  0.0383,  0.0367,  ...,  0.0399,  0.0413,  0.0285]],\n",
      "\n",
      "        [[-0.0510, -0.0518, -0.0473,  ..., -0.0469, -0.0458, -0.0317],\n",
      "         [-0.0473, -0.0428, -0.0418,  ..., -0.0443, -0.0442, -0.0355],\n",
      "         [-0.0473, -0.0407, -0.0395,  ..., -0.0391, -0.0387, -0.0327],\n",
      "         ...,\n",
      "         [-0.0475, -0.0410, -0.0400,  ..., -0.0404, -0.0399, -0.0335],\n",
      "         [-0.0456, -0.0396, -0.0402,  ..., -0.0442, -0.0426, -0.0372],\n",
      "         [-0.0396, -0.0292, -0.0312,  ..., -0.0361, -0.0379, -0.0383]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.897473Z",
     "start_time": "2025-03-11T16:48:54.894249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    target_batch_size = 32  #TODO before submission\n",
    "    batch_size = 4          #TODO before submission\n",
    "    \n",
    "    for batch, (X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "        X = X.to(device)\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, X)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if batch % (target_batch_size/batch_size) == 0:\n",
    "            # Ensure gradients are reset to 0 for new batch\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "        \n",
    "    return np.mean(losses)\n",
    "        \n",
    "    "
   ],
   "id": "c326529982cdc3fa",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:48:54.907981Z",
     "start_time": "2025-03-11T16:48:54.905263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval(dataloader, model, loss_fn):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    target_batch_size = 32  #TODO before submission\n",
    "    batch_size = 4          #TODO before submission\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, _) in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training\")):\n",
    "            X = X.to(device)\n",
    "            # Compute prediction\n",
    "            pred = model(X)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(pred, X)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "    return np.mean(losses)"
   ],
   "id": "2a079fe9e2df3c0",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:52:07.123524Z",
     "start_time": "2025-03-11T16:48:54.920460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "model = Autoencoder(3, 3).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_eval_loss = np.inf \n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f\"---------------------------------------- Epoch {epoch}\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    eval_loss = eval(train_dataloader, model, loss_fn)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Eval Loss: {eval_loss} for epoch {epoch}\")\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, f\"autoencoder/checkpoint_{epoch}.pytorch\")\n",
    "    \n",
    "\n"
   ],
   "id": "8b44f2b0ab103c59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m100\u001B[39m):\n\u001B[32m     17\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m---------------------------------------- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     train_loss = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m     eval_loss = \u001B[38;5;28meval\u001B[39m(train_dataloader, model, loss_fn)\n\u001B[32m     20\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m epoch % \u001B[32m5\u001B[39m == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(dataloader, model, loss_fn, optimizer)\u001B[39m\n\u001B[32m      5\u001B[39m batch_size = \u001B[32m4\u001B[39m          \u001B[38;5;66;03m#TODO before submission\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch, (X, _) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     X = \u001B[43mX\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     \u001B[38;5;66;03m# Compute prediction\u001B[39;00m\n\u001B[32m     10\u001B[39m     pred = model(X)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "496848c48c0dd5c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
